{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features CSV\n",
    "\n",
    "This Notebook concatenates all the generated feature files into a single CSV that can be trained. The columns to be included are:\n",
    "\n",
    "- Sentence Vector\n",
    "- Absolute Sentence Position\n",
    "- Sentence Length\n",
    "- Number of Special Terms\n",
    "- Sentiment Score\n",
    "- Mean TF-IDF\n",
    "- Normalized Mean TF-IDF\n",
    "- Mean TF-ISF\n",
    "- Normalized Mean TF-ISF\n",
    "\n",
    "All the files provided should contain equal number of line numbers ie. data records. As for the current GNUe chat dataset, that is `659165` records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_selected_chats(input_filename):\n",
    "    selected_chats = []\n",
    "    with open(input_filename) as input_file:\n",
    "        for chat_id in input_file:\n",
    "            log_id, is_summary = chat_id.strip().split(\",\")\n",
    "            selected_chats.append((int(log_id), int(is_summary)))\n",
    "    return selected_chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(85350, 0), (85351, 0), (85352, 0), (85353, 0), (85354, 0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_selected_chats(join(\"..\", \"data_files\", \"summarized_chat_log_ids.csv\"))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20715"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(load_selected_chats(join(\"..\", \"data_files\", \"summarized_chat_log_ids.csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_csv(\n",
    "    features_dir,\n",
    "    features_csv_filename,\n",
    "    absolute_sentence_positions_filename,\n",
    "    sentence_length_filename,\n",
    "    number_of_special_terms_filename,\n",
    "    sentiment_score_filename,\n",
    "    mean_tf_idf_filename,\n",
    "    normalized_mean_tf_idf_filename,\n",
    "    mean_tf_isf_filename,\n",
    "    normalized_mean_tf_isf_filename,\n",
    "    sentence_embeddings_filename=None,\n",
    "    selected_chat_logs=None\n",
    "):\n",
    "    current_log_id = None\n",
    "    file_line_number = 0\n",
    "    selected_chat_logs_index = 0\n",
    "    fake_embeddings_filename = \"fake_embeddings.txt\"\n",
    "    if selected_chat_logs:\n",
    "        current_log_id, is_summary = selected_chat_logs[selected_chat_logs_index]\n",
    "        print(\"Current Top: \", current_log_id)\n",
    "    \n",
    "    if not sentence_embeddings_filename:\n",
    "        sentence_embeddings_filename = fake_embeddings_filename\n",
    "        # Create fake file\n",
    "        with open(fake_embeddings_filename, \"w\"):\n",
    "            pass\n",
    "    else:\n",
    "        sentence_embeddings_filename = join(features_dir,sentence_embeddings_filename)\n",
    "    \n",
    "    with open(\n",
    "        join(features_dir, features_csv_filename), \"w\") as features_csv_file, open(\n",
    "        join(features_dir, absolute_sentence_positions_filename)) as absolute_sentence_positions_file, open(\n",
    "        join(features_dir, sentence_length_filename)) as sentence_length_file, open(\n",
    "        join(features_dir, number_of_special_terms_filename)) as number_of_special_terms_file, open(\n",
    "        join(features_dir, sentiment_score_filename)) as sentiment_score_file, open(\n",
    "        join(features_dir, mean_tf_idf_filename)) as mean_tf_idf_file, open(\n",
    "        join(features_dir, normalized_mean_tf_idf_filename)) as normalized_mean_tf_idf_file, open(\n",
    "        join(features_dir, mean_tf_isf_filename)) as mean_tf_isf_file, open(\n",
    "        join(features_dir, normalized_mean_tf_isf_filename)) as normalized_mean_tf_isf_file, gzip.open(\n",
    "        join(sentence_embeddings_filename)) as sentence_embeddings_file:\n",
    "        \n",
    "        file_line_number += 1\n",
    "        # Read first lines of each file\n",
    "        absolute_sentence_position = absolute_sentence_positions_file.readline().strip()\n",
    "        sentence_length = sentence_length_file.readline().strip()\n",
    "        number_of_special_terms = number_of_special_terms_file.readline().strip()\n",
    "        sentiment_score = sentiment_score_file.readline().strip().split()[0]\n",
    "        mean_tf_idf = mean_tf_idf_file.readline().strip()\n",
    "        normalized_mean_tf_idf = normalized_mean_tf_idf_file.readline().strip()\n",
    "        mean_tf_isf = mean_tf_isf_file.readline().strip()\n",
    "        normalized_mean_tf_isf = normalized_mean_tf_isf_file.readline().strip()\n",
    "        if sentence_embeddings_filename:\n",
    "            sentence_vector = sentence_embeddings_file.readline().strip()\n",
    "        else:\n",
    "            sentence_vector = 0.0\n",
    "            \n",
    "        features_csv = csv.writer(features_csv_file, delimiter=',')\n",
    "        features_csv.writerow([\n",
    "            \"log_id\",\n",
    "            # \"sentence_vector\",\n",
    "            \"absolute_sentence_position\",\n",
    "            \"sentence_length\",\n",
    "            \"number_of_special_terms\",\n",
    "            \"sentiment_score\",\n",
    "            \"mean_tf_idf\",\n",
    "            \"normalized_mean_tf_idf\",\n",
    "            \"mean_tf_isf\",\n",
    "            \"normalized_mean_tf_isf\",\n",
    "            \"is_summary\"\n",
    "        ])\n",
    "        while absolute_sentence_position:\n",
    "            \n",
    "            if selected_chat_logs:\n",
    "                if current_log_id != file_line_number:\n",
    "                    # Read next lines of each file\n",
    "                    absolute_sentence_position = absolute_sentence_positions_file.readline().strip()\n",
    "                    sentence_length = sentence_length_file.readline().strip()\n",
    "                    number_of_special_terms = number_of_special_terms_file.readline().strip()\n",
    "                    sentiment_score = sentiment_score_file.readline().strip().split()\n",
    "                    if sentiment_score:\n",
    "                        sentiment_score = sentiment_score[0]\n",
    "                    mean_tf_idf = mean_tf_idf_file.readline().strip()\n",
    "                    normalized_mean_tf_idf = normalized_mean_tf_idf_file.readline().strip()\n",
    "                    mean_tf_isf = mean_tf_isf_file.readline().strip()\n",
    "                    normalized_mean_tf_isf = normalized_mean_tf_isf_file.readline().strip()\n",
    "            \n",
    "                    if sentence_embeddings_filename:\n",
    "                        sentence_vector = sentence_embeddings_file.readline().strip() \n",
    "                    file_line_number += 1\n",
    "                    continue \n",
    "                    \n",
    "            if not selected_chat_logs:\n",
    "                current_log_id = file_line_number\n",
    "                is_summary=\"NULL\"\n",
    "            \n",
    "            features_csv.writerow([\n",
    "                current_log_id,\n",
    "                #sentence_vector,\n",
    "                absolute_sentence_position,\n",
    "                sentence_length,\n",
    "                number_of_special_terms,\n",
    "                sentiment_score,\n",
    "                mean_tf_idf,\n",
    "                normalized_mean_tf_idf,\n",
    "                mean_tf_isf,\n",
    "                normalized_mean_tf_isf,\n",
    "                is_summary\n",
    "            ])\n",
    "            \n",
    "            # Read next lines of each file\n",
    "            absolute_sentence_position = absolute_sentence_positions_file.readline().strip()\n",
    "            sentence_length = sentence_length_file.readline().strip()\n",
    "            number_of_special_terms = number_of_special_terms_file.readline().strip()\n",
    "            sentiment_score = sentiment_score_file.readline().strip().split()\n",
    "            if sentiment_score:\n",
    "                sentiment_score = sentiment_score[0]\n",
    "            mean_tf_idf = mean_tf_idf_file.readline().strip()\n",
    "            normalized_mean_tf_idf = normalized_mean_tf_idf_file.readline().strip()\n",
    "            mean_tf_isf = mean_tf_isf_file.readline().strip()\n",
    "            normalized_mean_tf_isf = normalized_mean_tf_isf_file.readline().strip()\n",
    "            \n",
    "            # if sentence_embeddings_filename:\n",
    "            #   sentence_vector = sentence_embeddings_file.readline().strip()  \n",
    "                \n",
    "            file_line_number += 1\n",
    "            if selected_chat_logs:\n",
    "                selected_chat_logs_index += 1\n",
    "                if selected_chat_logs_index >= len(selected_chat_logs):\n",
    "                    break\n",
    "                current_log_id, is_summary = selected_chat_logs[selected_chat_logs_index]\n",
    "                \n",
    "        if selected_chat_logs:\n",
    "            assert selected_chat_logs_index == len(selected_chat_logs)\n",
    "            print(\"Final index: \", selected_chat_logs_index)\n",
    "        print(\"Last File Number: \", file_line_number)\n",
    "        \n",
    "        if not sentence_embeddings_filename:\n",
    "           os.remove(fake_embeddings_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Top:  1\n",
      "Final index:  659165\n",
      "Last File Number:  659166\n"
     ]
    }
   ],
   "source": [
    "FEATURES_DIR = join(\"..\", \"feature_outputs\")\n",
    "features_csv_filename = \"all_chat_features.csv\" \n",
    "summarized_chats_features_csv_filename = \"summarized_chats_features.csv\" \n",
    "absolute_sentence_positions_filename = \"absolute_sentence_positions.txt\" \n",
    "sentence_length_filename = \"sentence_word_counts.txt\" \n",
    "number_of_special_terms_filename = \"special_terms_count.txt\" \n",
    "sentiment_score_filename = \"sentence_sentiments.txt\" \n",
    "mean_tf_idf_filename = \"chats_mean_tf_idf.txt\" \n",
    "normalized_mean_tf_idf_filename = \"normalized_chats_mean_tf_idf.txt\" \n",
    "mean_tf_isf_filename = \"chats_mean_tf_isf.txt\" \n",
    "normalized_mean_tf_isf_filename = \"normalized_chats_mean_tf_isf.txt\" \n",
    "sentence_embeddings_filename = \"sentence_embeddings_2.csv.gz\"\n",
    "selected_chat_logs = load_selected_chats(join(\"..\", \"data_files\", \"summarized_chat_log_ids.csv\"))\n",
    "chat_logs = load_selected_chats(join(\"..\", \"data_files\", \"chat_log_ids.csv\"))\n",
    " \n",
    "\n",
    "\n",
    "#selected_chat_logs[0]\n",
    "# All chats\n",
    "create_features_csv(\n",
    "    FEATURES_DIR,\n",
    "    features_csv_filename,\n",
    "    absolute_sentence_positions_filename,\n",
    "    sentence_length_filename,\n",
    "    number_of_special_terms_filename,\n",
    "    sentiment_score_filename,\n",
    "    mean_tf_idf_filename,\n",
    "    normalized_mean_tf_idf_filename,\n",
    "    mean_tf_isf_filename,\n",
    "    normalized_mean_tf_isf_filename,\n",
    "    None,\n",
    "    chat_logs\n",
    ")\n",
    "\n",
    "# Summary features\n",
    "# create_features_csv(\n",
    "#     FEATURES_DIR,\n",
    "#     \"summarized_chats_features_with.csv\",\n",
    "#     absolute_sentence_positions_filename,\n",
    "#     sentence_length_filename,\n",
    "#     number_of_special_terms_filename,\n",
    "#     sentiment_score_filename,\n",
    "#     mean_tf_idf_filename,\n",
    "#     normalized_mean_tf_idf_filename,\n",
    "#     mean_tf_isf_filename,\n",
    "#     normalized_mean_tf_isf_filename,\n",
    "#     None,\n",
    "#     selected_chat_logs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lines_in_file(input_filename):\n",
    "    line_count = 0\n",
    "    with open(input_filename) as input_file:\n",
    "        for line in input_file:\n",
    "            line_count += 1\n",
    "    return line_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'1.0222757701660766e-10,-1.9559458929091927e-09,1.3279695939731687e-09,1.2324553052248554e-09,2.482606608290032e-09,-2.74391371849788e-10,-1.0863636265334856e-09,-2.4886456205472817e-10,1.2621625688900456e-09,9.18070533198011e-10,2.4075938601442944e-09,-5.326582242199325e-10,7.117011443837444e-10,2.579528216134441e-09,-7.93907053032071e-10,-1.7025355585994092e-09,5.637443285719513e-10,4.4665024924786264e-10,-2.3499701611548076e-09,-1.4646918318630677e-09,-1.4871635485560617e-09,8.606370103764562e-11,3.736129755012441e-10,-2.514077149090871e-09,-9.642871501648177e-10,-1.0885982765332219e-09,9.025190258043666e-10,1.24999058770125e-09,-6.623168452549785e-10,1.1354552824116734e-09,3.353764061842507e-10,1.9264491487097168e-09,-2.778983400399219e-09,1.9516112582558669e-10,-5.482126810054205e-09,5.117940143867821e-10,1.3941000366499076e-10,5.096144667975348e-10,-2.226912349709594e-09,-4.0765575687121e-10,2.690162376727568e-10,-1.8730766360115173e-10,4.043583102986519e-10,-1.1536109085317949e-10,1.0386995999347466e-09,-4.088251819066367e-10,5.850644578688114e-10,1.8206670026145553e-09,2.6595937846779066e-10,-1.881586426226272e-09,1.2637043767219984e-09,-2.2014181245130444e-09,1.1883105035703785e-09,2.480386263723788e-09,-5.27921271325921e-11,-1.016932293383533e-09,1.1844312806261789e-10,1.9990808369252837e-09,2.7419068957722807e-09,-3.846584126330319e-09,-4.256529635611754e-10,-1.6945418236521187e-09,1.1974869976299928e-09,3.676278956113117e-10,-1.5114592963244745e-09,-7.269912243955735e-10,1.035438049403637e-09,-1.8987299870793827e-09,6.909088149386358e-10,3.5613351953752755e-09,2.7881916843110385e-10,-7.801956482502382e-10,-9.361493338419698e-10,1.6166388481100575e-09,7.396394793840045e-12,-3.19543117072865e-09,-2.9643196516379225e-09,-1.8141363073098647e-09,-3.9930103049110033e-10,-3.414744062783704e-09,-7.09066869450245e-10,5.977270624433073e-10,-1.9075836375288242e-09,3.0405421802703583e-09,-3.2156961418477326e-09,2.1197137890884e-10,1.2229149939677814e-10,-2.886748526830826e-09,1.7479863933486502e-09,-1.0777342711523808e-09,1.011313172090827e-09,1.3080201371372232e-09,-4.089232889227478e-09,-5.0705873929052585e-11,-1.8385717538176905e-09,-5.508489781267407e-09,2.74781768895897e-11,1.2345420441066926e-09,-1.985308766508826e-09,-5.6956231305123e-10,-1.1401931183571351e-09,-5.121670153193233e-10,5.630179304490622e-10,4.076326562452742e-09,2.967439295800994e-09,-1.5829749841173486e-09,-1.5643614958203271e-09,-1.8705717722680365e-09,1.1687398758065451e-09,-2.001278575374404e-09,1.667801259638835e-09,4.471899261416186e-10,1.9389068853477794e-09,2.021980303740728e-11,5.568482707297632e-10,-4.941354137753748e-10,1.1810058136696358e-09,-3.546280581032628e-09,1.6779634157271007e-09,-4.588027149996099e-10,-5.738517354703082e-10,1.1107435291050973e-09,-2.7282278989786685e-09,-7.197123195971752e-10,-9.850216397399417e-10,-5.069424855671143e-10,-1.270229067276828e-09,-2.663329268922309e-09,3.6894048536309312e-09,1.5660633125750463e-09,2.6942378799328487e-10,1.5445371673748672e-09,-4.4883827413103135e-10,2.5345272077939036e-09,-1.2354312769169883e-09,7.175133007234522e-11,1.484585038321639e-09,1.2817332849590239e-09,1.1187911301908466e-09,1.0881156006105728e-09,-6.610668859272986e-10,-1.5202071572101887e-09,8.983655050034656e-10,8.465219075860465e-10,3.878688698072362e-11,2.7973040688353727e-09,1.0892287311160527e-10,1.7919556443748384e-09,7.814260038357248e-10,-5.340538870368863e-10'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open(join(FEATURES_DIR, \"sentence_vectors.txt.gz\")) as features_e:\n",
    "    emb = features_e.readline().strip()\n",
    "    emb2 = features_e.readline().strip()\n",
    "emb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20716"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATURES_DIR = join(\"..\", \"feature_outputs\")\n",
    "sentence_embeddings_filename = join(FEATURES_DIR, \"summarized_chats_features_with_embeddings.csv\")\n",
    "count_lines_in_file(sentence_embeddings_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "os.stat_result(st_mode=33188, st_ino=19478805, st_dev=16777221, st_nlink=1, st_uid=501, st_gid=20, st_size=21151757222, st_atime=1550506379, st_mtime=1550493216, st_ctime=1550494480)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = os.stat(sentence_embeddings_filename)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
