{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features CSV\n",
    "\n",
    "This Notebook concatenates all the generated feature files into a single CSV that can be trained. The columns to be included are:\n",
    "\n",
    "- Sentence Vector\n",
    "- Absolute Sentence Position\n",
    "- Sentence Length\n",
    "- Number of Special Terms\n",
    "- Sentiment Score\n",
    "- Mean TF-IDF\n",
    "- Normalized Mean TF-IDF\n",
    "- Mean TF-ISF\n",
    "- Normalized Mean TF-ISF\n",
    "\n",
    "All the files provided should contain equal number of line numbers ie. data records. As for the current GNUe chat dataset, that is `659165` records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "from os.path import join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_selected_chats(input_filename):\n",
    "    selected_chats = []\n",
    "    with open(input_filename) as input_file:\n",
    "        for chat_id in input_file:\n",
    "            selected_chats.append(int(chat_id.strip()))\n",
    "    return selected_chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20715"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(load_selected_chats(join(\"..\", \"data_files\", \"summarized_chat_log_ids.csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_csv(\n",
    "    features_dir,\n",
    "    features_csv_filename,\n",
    "    absolute_sentence_positions_filename,\n",
    "    sentence_length_filename,\n",
    "    number_of_special_terms_filename,\n",
    "    sentiment_score_filename,\n",
    "    mean_tf_idf_filename,\n",
    "    normalized_mean_tf_idf_filename,\n",
    "    mean_tf_isf_filename,\n",
    "    normalized_mean_tf_isf_filename,\n",
    "    sentence_vector_filename=None,\n",
    "    selected_chat_logs=None\n",
    "):\n",
    "    current_log_id = None\n",
    "    file_line_number = 0\n",
    "    selected_chat_logs_index = 0\n",
    "    if selected_chat_logs:\n",
    "        current_log_id = selected_chat_logs[selected_chat_logs_index]\n",
    "        print(\"Current Top: \", current_log_id)\n",
    "        \n",
    "    with open(\n",
    "        join(features_dir, features_csv_filename), \"w\") as features_csv_file, open(\n",
    "        join(features_dir, absolute_sentence_positions_filename)) as absolute_sentence_positions_file, open(\n",
    "        join(features_dir, sentence_length_filename)) as sentence_length_file, open(\n",
    "        join(features_dir, number_of_special_terms_filename)) as number_of_special_terms_file, open(\n",
    "        join(features_dir, sentiment_score_filename)) as sentiment_score_file, open(\n",
    "        join(features_dir, mean_tf_idf_filename)) as mean_tf_idf_file, open(\n",
    "        join(features_dir, normalized_mean_tf_idf_filename)) as normalized_mean_tf_idf_file, open(\n",
    "        join(features_dir, mean_tf_isf_filename)) as mean_tf_isf_file, open(\n",
    "        join(features_dir, normalized_mean_tf_isf_filename)) as normalized_mean_tf_isf_file:\n",
    "        \n",
    "        file_line_number += 1\n",
    "        # Read first lines of each file\n",
    "        absolute_sentence_position = absolute_sentence_positions_file.readline().strip()\n",
    "        sentence_length = sentence_length_file.readline().strip()\n",
    "        number_of_special_terms = number_of_special_terms_file.readline().strip()\n",
    "        sentiment_score = sentiment_score_file.readline().strip().split()[0]\n",
    "        mean_tf_idf = mean_tf_idf_file.readline().strip()\n",
    "        normalized_mean_tf_idf = normalized_mean_tf_idf_file.readline().strip()\n",
    "        mean_tf_isf = mean_tf_isf_file.readline().strip()\n",
    "        normalized_mean_tf_isf = normalized_mean_tf_isf_file.readline().strip()\n",
    "        # sentence_vector = sentence_vector_file.readline()      \n",
    "            \n",
    "        features_csv = csv.writer(features_csv_file, delimiter=',')\n",
    "        features_csv.writerow([\n",
    "            \"absolute_sentence_position\",\n",
    "            \"sentence_length\",\n",
    "            \"number_of_special_terms\",\n",
    "            \"sentiment_score\",\n",
    "            \"mean_tf_idf\",\n",
    "            \"normalized_mean_tf_idf\",\n",
    "            \"mean_tf_isf\",\n",
    "            \"normalized_mean_tf_isf\"\n",
    "            #\"sentence_vector\"\n",
    "        ])\n",
    "        while absolute_sentence_position:\n",
    "            \n",
    "            if current_log_id:\n",
    "                if current_log_id != file_line_number:\n",
    "                    # Read next lines of each file\n",
    "                    absolute_sentence_position = absolute_sentence_positions_file.readline().strip()\n",
    "                    sentence_length = sentence_length_file.readline().strip()\n",
    "                    number_of_special_terms = number_of_special_terms_file.readline().strip()\n",
    "                    sentiment_score = sentiment_score_file.readline().strip().split()\n",
    "                    if sentiment_score:\n",
    "                        sentiment_score = sentiment_score[0]\n",
    "                    mean_tf_idf = mean_tf_idf_file.readline().strip()\n",
    "                    normalized_mean_tf_idf = normalized_mean_tf_idf_file.readline().strip()\n",
    "                    mean_tf_isf = mean_tf_isf_file.readline().strip()\n",
    "                    normalized_mean_tf_isf = normalized_mean_tf_isf_file.readline().strip()\n",
    "                    # sentence_vector = sentence_vector_file.readline() \n",
    "                    file_line_number += 1\n",
    "                    continue          \n",
    "            \n",
    "            features_csv.writerow([\n",
    "                absolute_sentence_position,\n",
    "                sentence_length,\n",
    "                number_of_special_terms,\n",
    "                sentiment_score,\n",
    "                mean_tf_idf,\n",
    "                normalized_mean_tf_idf,\n",
    "                mean_tf_isf,\n",
    "                normalized_mean_tf_isf,\n",
    "                #sentence_vector,\n",
    "            ])\n",
    "            # Read next lines of each file\n",
    "            absolute_sentence_position = absolute_sentence_positions_file.readline().strip()\n",
    "            sentence_length = sentence_length_file.readline().strip()\n",
    "            number_of_special_terms = number_of_special_terms_file.readline().strip()\n",
    "            sentiment_score = sentiment_score_file.readline().strip().split()\n",
    "            if sentiment_score:\n",
    "                sentiment_score = sentiment_score[0]\n",
    "            mean_tf_idf = mean_tf_idf_file.readline().strip()\n",
    "            normalized_mean_tf_idf = normalized_mean_tf_idf_file.readline().strip()\n",
    "            mean_tf_isf = mean_tf_isf_file.readline().strip()\n",
    "            normalized_mean_tf_isf = normalized_mean_tf_isf_file.readline().strip()\n",
    "            # sentence_vector = sentence_vector_file.readline()   \n",
    "            file_line_number += 1\n",
    "            if selected_chat_logs:\n",
    "                selected_chat_logs_index += 1\n",
    "                if selected_chat_logs_index >= len(selected_chat_logs):\n",
    "                    break\n",
    "                current_log_id = selected_chat_logs[selected_chat_logs_index]\n",
    "        if selected_chat_logs:\n",
    "            assert selected_chat_logs_index == len(selected_chat_logs)\n",
    "            print(\"Final index: \", selected_chat_logs_index)\n",
    "        print(\"Last File Number: \", file_line_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_DIR = join(\"..\", \"feature_outputs\")\n",
    "features_csv_filename = \"features.csv\" \n",
    "summarized_chats_features_csv_filename = \"summarized_chats_features.csv\" \n",
    "absolute_sentence_positions_filename = \"absolute_sentence_positions.txt\" \n",
    "sentence_length_filename = \"sentence_word_counts.txt\" \n",
    "number_of_special_terms_filename = \"special_terms_count.txt\" \n",
    "sentiment_score_filename = \"sentence_sentiments.txt\" \n",
    "mean_tf_idf_filename = \"chats_mean_tf_idf.txt\" \n",
    "normalized_mean_tf_idf_filename = \"normalized_chats_mean_tf_idf.txt\" \n",
    "mean_tf_isf_filename = \"chats_mean_tf_isf.txt\" \n",
    "normalized_mean_tf_isf_filename = \"normalized_chats_mean_tf_isf.txt\" \n",
    "selected_chat_logs = load_selected_chats(join(\"..\", \"data_files\", \"summarized_chat_log_ids.csv\"))\n",
    "# sentence_vector_filename = \"sentence_vector.txt\" \n",
    "\n",
    "# create_features_csv(\n",
    "#     FEATURES_DIR,\n",
    "#     features_csv_filename,\n",
    "#     absolute_sentence_positions_filename,\n",
    "#     sentence_length_filename,\n",
    "#     number_of_special_terms_filename,\n",
    "#     sentiment_score_filename,\n",
    "#     mean_tf_idf_filename,\n",
    "#     normalized_mean_tf_idf_filename,\n",
    "#     mean_tf_isf_filename,\n",
    "#     normalized_mean_tf_isf_filename,\n",
    "#     None,\n",
    "#     None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lines_in_file(input_filename):\n",
    "    line_count = 0\n",
    "    with open(input_filename) as input_file:\n",
    "        for line in input_file:\n",
    "            line_count += 1\n",
    "    return line_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_DIR = join(\"..\", \"feature_outputs\")\n",
    "sentence_embeddings_filename = join(FEATURES_DIR, \"sentence_embeddings_2.csv\")\n",
    "count_lines_in_file(sentence_embeddings_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "os.stat_result(st_mode=33188, st_ino=19478805, st_dev=16777221, st_nlink=1, st_uid=501, st_gid=20, st_size=21151757222, st_atime=1550506379, st_mtime=1550493216, st_ctime=1550494480)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = os.stat(sentence_embeddings_filename)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
