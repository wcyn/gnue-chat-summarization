{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import gensim\n",
    "import logging\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from os.path import join\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_DIR = join(\"..\", \"feature_outputs\")\n",
    "DATA_DIR = join(\"..\", \"data_files\")\n",
    "PUNCTUATION_SET = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_INPUT_FILE = join(DATA_DIR, \"gnue_irc_chat_logs_preprocessed_words_only.txt.gz\")\n",
    "\n",
    "# with gzip.open (data_file, 'rb') as f:\n",
    "#     for i,line in enumerate (f):\n",
    "#         print(line)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "\n",
    "    logging.info(\"reading file {0}...this may take a while\".format(input_file))\n",
    "\n",
    "    with gzip.open(input_file, 'rb') as f:\n",
    "        for i, line in enumerate(f):\n",
    "\n",
    "            if (i % 100000 == 0):\n",
    "                logging.info(\"read {0} chat lines\".format(i))\n",
    "            # do some pre-processing and return a list of words for each review text\n",
    "            yield gensim.utils.simple_preprocess(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 08:45:55,934 : INFO : reading file ../data_files/gnue_irc_chat_logs_preprocessed_words_only.txt.gz...this may take a while\n",
      "2019-03-14 08:45:55,937 : INFO : read 0 chat lines\n",
      "2019-03-14 08:45:57,245 : INFO : read 100000 chat lines\n",
      "2019-03-14 08:45:58,532 : INFO : read 200000 chat lines\n",
      "2019-03-14 08:45:59,895 : INFO : read 300000 chat lines\n",
      "2019-03-14 08:46:01,285 : INFO : read 400000 chat lines\n",
      "2019-03-14 08:46:02,603 : INFO : read 500000 chat lines\n",
      "2019-03-14 08:46:03,924 : INFO : read 600000 chat lines\n",
      "2019-03-14 08:46:04,616 : INFO : Done reading data file\n"
     ]
    }
   ],
   "source": [
    "# read the tokenized reviews into a list\n",
    "# each review item becomes a serries of words\n",
    "# so this becomes a list of lists\n",
    "documents = list(read_input(CHAT_INPUT_FILE))\n",
    "logging.info(\"Done reading data file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 08:46:06,501 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-03-14 08:46:06,502 : INFO : collecting all words and their counts\n",
      "2019-03-14 08:46:06,503 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-03-14 08:46:06,522 : INFO : PROGRESS: at sentence #10000, processed 64153 words, keeping 6873 word types\n",
      "2019-03-14 08:46:06,540 : INFO : PROGRESS: at sentence #20000, processed 126677 words, keeping 10038 word types\n",
      "2019-03-14 08:46:06,558 : INFO : PROGRESS: at sentence #30000, processed 188326 words, keeping 12592 word types\n",
      "2019-03-14 08:46:06,578 : INFO : PROGRESS: at sentence #40000, processed 251292 words, keeping 15081 word types\n",
      "2019-03-14 08:46:06,596 : INFO : PROGRESS: at sentence #50000, processed 313590 words, keeping 17212 word types\n",
      "2019-03-14 08:46:06,614 : INFO : PROGRESS: at sentence #60000, processed 379341 words, keeping 18964 word types\n",
      "2019-03-14 08:46:06,637 : INFO : PROGRESS: at sentence #70000, processed 440245 words, keeping 20799 word types\n",
      "2019-03-14 08:46:06,658 : INFO : PROGRESS: at sentence #80000, processed 504481 words, keeping 22253 word types\n",
      "2019-03-14 08:46:06,679 : INFO : PROGRESS: at sentence #90000, processed 568210 words, keeping 23798 word types\n",
      "2019-03-14 08:46:06,697 : INFO : PROGRESS: at sentence #100000, processed 632911 words, keeping 25305 word types\n",
      "2019-03-14 08:46:06,717 : INFO : PROGRESS: at sentence #110000, processed 697752 words, keeping 26843 word types\n",
      "2019-03-14 08:46:06,738 : INFO : PROGRESS: at sentence #120000, processed 761144 words, keeping 28393 word types\n",
      "2019-03-14 08:46:06,756 : INFO : PROGRESS: at sentence #130000, processed 821728 words, keeping 29635 word types\n",
      "2019-03-14 08:46:06,775 : INFO : PROGRESS: at sentence #140000, processed 883332 words, keeping 31066 word types\n",
      "2019-03-14 08:46:06,792 : INFO : PROGRESS: at sentence #150000, processed 945536 words, keeping 32212 word types\n",
      "2019-03-14 08:46:06,810 : INFO : PROGRESS: at sentence #160000, processed 1006463 words, keeping 33485 word types\n",
      "2019-03-14 08:46:06,831 : INFO : PROGRESS: at sentence #170000, processed 1071464 words, keeping 34804 word types\n",
      "2019-03-14 08:46:06,852 : INFO : PROGRESS: at sentence #180000, processed 1137469 words, keeping 36012 word types\n",
      "2019-03-14 08:46:06,871 : INFO : PROGRESS: at sentence #190000, processed 1202826 words, keeping 37025 word types\n",
      "2019-03-14 08:46:06,890 : INFO : PROGRESS: at sentence #200000, processed 1264028 words, keeping 38188 word types\n",
      "2019-03-14 08:46:06,909 : INFO : PROGRESS: at sentence #210000, processed 1324748 words, keeping 39375 word types\n",
      "2019-03-14 08:46:06,930 : INFO : PROGRESS: at sentence #220000, processed 1388448 words, keeping 40664 word types\n",
      "2019-03-14 08:46:06,951 : INFO : PROGRESS: at sentence #230000, processed 1455223 words, keeping 41613 word types\n",
      "2019-03-14 08:46:06,971 : INFO : PROGRESS: at sentence #240000, processed 1520148 words, keeping 42550 word types\n",
      "2019-03-14 08:46:06,991 : INFO : PROGRESS: at sentence #250000, processed 1582385 words, keeping 43567 word types\n",
      "2019-03-14 08:46:07,014 : INFO : PROGRESS: at sentence #260000, processed 1645034 words, keeping 44589 word types\n",
      "2019-03-14 08:46:07,034 : INFO : PROGRESS: at sentence #270000, processed 1709224 words, keeping 45534 word types\n",
      "2019-03-14 08:46:07,054 : INFO : PROGRESS: at sentence #280000, processed 1771384 words, keeping 46513 word types\n",
      "2019-03-14 08:46:07,075 : INFO : PROGRESS: at sentence #290000, processed 1835985 words, keeping 47438 word types\n",
      "2019-03-14 08:46:07,097 : INFO : PROGRESS: at sentence #300000, processed 1896434 words, keeping 48401 word types\n",
      "2019-03-14 08:46:07,120 : INFO : PROGRESS: at sentence #310000, processed 1962535 words, keeping 49532 word types\n",
      "2019-03-14 08:46:07,141 : INFO : PROGRESS: at sentence #320000, processed 2027977 words, keeping 50647 word types\n",
      "2019-03-14 08:46:07,163 : INFO : PROGRESS: at sentence #330000, processed 2091869 words, keeping 51606 word types\n",
      "2019-03-14 08:46:07,185 : INFO : PROGRESS: at sentence #340000, processed 2156215 words, keeping 52417 word types\n",
      "2019-03-14 08:46:07,207 : INFO : PROGRESS: at sentence #350000, processed 2217442 words, keeping 53190 word types\n",
      "2019-03-14 08:46:07,228 : INFO : PROGRESS: at sentence #360000, processed 2281576 words, keeping 53969 word types\n",
      "2019-03-14 08:46:07,249 : INFO : PROGRESS: at sentence #370000, processed 2343644 words, keeping 54784 word types\n",
      "2019-03-14 08:46:07,270 : INFO : PROGRESS: at sentence #380000, processed 2409443 words, keeping 55706 word types\n",
      "2019-03-14 08:46:07,291 : INFO : PROGRESS: at sentence #390000, processed 2473506 words, keeping 56417 word types\n",
      "2019-03-14 08:46:07,311 : INFO : PROGRESS: at sentence #400000, processed 2537358 words, keeping 57231 word types\n",
      "2019-03-14 08:46:07,332 : INFO : PROGRESS: at sentence #410000, processed 2601030 words, keeping 58021 word types\n",
      "2019-03-14 08:46:07,352 : INFO : PROGRESS: at sentence #420000, processed 2664046 words, keeping 58811 word types\n",
      "2019-03-14 08:46:07,375 : INFO : PROGRESS: at sentence #430000, processed 2727583 words, keeping 59517 word types\n",
      "2019-03-14 08:46:07,396 : INFO : PROGRESS: at sentence #440000, processed 2788323 words, keeping 60269 word types\n",
      "2019-03-14 08:46:07,417 : INFO : PROGRESS: at sentence #450000, processed 2848728 words, keeping 61093 word types\n",
      "2019-03-14 08:46:07,437 : INFO : PROGRESS: at sentence #460000, processed 2912451 words, keeping 61799 word types\n",
      "2019-03-14 08:46:07,460 : INFO : PROGRESS: at sentence #470000, processed 2977432 words, keeping 62553 word types\n",
      "2019-03-14 08:46:07,480 : INFO : PROGRESS: at sentence #480000, processed 3041870 words, keeping 63258 word types\n",
      "2019-03-14 08:46:07,499 : INFO : PROGRESS: at sentence #490000, processed 3106709 words, keeping 64115 word types\n",
      "2019-03-14 08:46:07,519 : INFO : PROGRESS: at sentence #500000, processed 3166435 words, keeping 64799 word types\n",
      "2019-03-14 08:46:07,541 : INFO : PROGRESS: at sentence #510000, processed 3229364 words, keeping 65703 word types\n",
      "2019-03-14 08:46:07,561 : INFO : PROGRESS: at sentence #520000, processed 3292932 words, keeping 66547 word types\n",
      "2019-03-14 08:46:07,580 : INFO : PROGRESS: at sentence #530000, processed 3354078 words, keeping 67186 word types\n",
      "2019-03-14 08:46:07,600 : INFO : PROGRESS: at sentence #540000, processed 3414186 words, keeping 67869 word types\n",
      "2019-03-14 08:46:07,622 : INFO : PROGRESS: at sentence #550000, processed 3477793 words, keeping 68556 word types\n",
      "2019-03-14 08:46:07,647 : INFO : PROGRESS: at sentence #560000, processed 3537879 words, keeping 69663 word types\n",
      "2019-03-14 08:46:07,670 : INFO : PROGRESS: at sentence #570000, processed 3601795 words, keeping 70423 word types\n",
      "2019-03-14 08:46:07,693 : INFO : PROGRESS: at sentence #580000, processed 3663790 words, keeping 71088 word types\n",
      "2019-03-14 08:46:07,717 : INFO : PROGRESS: at sentence #590000, processed 3729051 words, keeping 71784 word types\n",
      "2019-03-14 08:46:07,740 : INFO : PROGRESS: at sentence #600000, processed 3793319 words, keeping 72549 word types\n",
      "2019-03-14 08:46:07,762 : INFO : PROGRESS: at sentence #610000, processed 3857248 words, keeping 73147 word types\n",
      "2019-03-14 08:46:07,786 : INFO : PROGRESS: at sentence #620000, processed 3922081 words, keeping 73864 word types\n",
      "2019-03-14 08:46:07,810 : INFO : PROGRESS: at sentence #630000, processed 3986181 words, keeping 74479 word types\n",
      "2019-03-14 08:46:07,832 : INFO : PROGRESS: at sentence #640000, processed 4050962 words, keeping 75147 word types\n",
      "2019-03-14 08:46:07,858 : INFO : PROGRESS: at sentence #650000, processed 4113148 words, keeping 75756 word types\n",
      "2019-03-14 08:46:07,879 : INFO : collected 76348 word types from a corpus of 4169816 raw words and 659165 sentences\n",
      "2019-03-14 08:46:07,880 : INFO : Loading a fresh vocabulary\n",
      "2019-03-14 08:46:07,986 : INFO : min_count=2 retains 37992 unique words (49% of original 76348, drops 38356)\n",
      "2019-03-14 08:46:07,987 : INFO : min_count=2 leaves 4131460 word corpus (99% of original 4169816, drops 38356)\n",
      "2019-03-14 08:46:08,090 : INFO : deleting the raw counts dictionary of 76348 items\n",
      "2019-03-14 08:46:08,093 : INFO : sample=0.001 downsamples 54 most-common words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 08:46:08,094 : INFO : downsampling leaves estimated 3321113 word corpus (80.4% of prior 4131460)\n",
      "2019-03-14 08:46:08,231 : INFO : estimated required memory for 37992 words and 150 dimensions: 64586400 bytes\n",
      "2019-03-14 08:46:08,232 : INFO : resetting layer weights\n",
      "2019-03-14 08:46:08,653 : INFO : training model with 10 workers on 37992 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-03-14 08:46:09,680 : INFO : EPOCH 1 - PROGRESS: at 35.01% examples, 1158912 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-14 08:46:10,685 : INFO : EPOCH 1 - PROGRESS: at 69.92% examples, 1158041 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-14 08:46:11,565 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-14 08:46:11,582 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-14 08:46:11,588 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-14 08:46:11,590 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-14 08:46:11,593 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-14 08:46:11,597 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-14 08:46:11,600 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-14 08:46:11,601 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-14 08:46:11,609 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-14 08:46:11,613 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-14 08:46:11,614 : INFO : EPOCH - 1 : training on 4169816 raw words (3321156 effective words) took 2.9s, 1130997 effective words/s\n",
      "2019-03-14 08:46:12,647 : INFO : EPOCH 2 - PROGRESS: at 30.73% examples, 999276 words/s, in_qsize 20, out_qsize 3\n",
      "2019-03-14 08:46:13,651 : INFO : EPOCH 2 - PROGRESS: at 66.28% examples, 1089874 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-14 08:46:14,592 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-14 08:46:14,600 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-14 08:46:14,608 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-14 08:46:14,612 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-14 08:46:14,614 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-14 08:46:14,615 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-14 08:46:14,623 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-14 08:46:14,629 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-14 08:46:14,632 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-14 08:46:14,633 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-14 08:46:14,634 : INFO : EPOCH - 2 : training on 4169816 raw words (3320230 effective words) took 3.0s, 1104500 effective words/s\n",
      "2019-03-14 08:46:15,653 : INFO : EPOCH 3 - PROGRESS: at 32.91% examples, 1083941 words/s, in_qsize 16, out_qsize 3\n",
      "2019-03-14 08:46:16,654 : INFO : EPOCH 3 - PROGRESS: at 63.56% examples, 1054741 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-14 08:46:17,622 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-14 08:46:17,632 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-14 08:46:17,645 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-14 08:46:17,647 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-14 08:46:17,650 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-14 08:46:17,653 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-14 08:46:17,655 : INFO : EPOCH 3 - PROGRESS: at 99.29% examples, 1095887 words/s, in_qsize 3, out_qsize 1\n",
      "2019-03-14 08:46:17,656 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-14 08:46:17,658 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-14 08:46:17,667 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-14 08:46:17,669 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-14 08:46:17,670 : INFO : EPOCH - 3 : training on 4169816 raw words (3320773 effective words) took 3.0s, 1098392 effective words/s\n",
      "2019-03-14 08:46:18,698 : INFO : EPOCH 4 - PROGRESS: at 33.61% examples, 1098821 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-14 08:46:19,701 : INFO : EPOCH 4 - PROGRESS: at 66.02% examples, 1089293 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-14 08:46:20,599 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-14 08:46:20,606 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-14 08:46:20,617 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-14 08:46:20,620 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-14 08:46:20,624 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-14 08:46:20,626 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-14 08:46:20,628 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-14 08:46:20,629 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-14 08:46:20,640 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-14 08:46:20,641 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-14 08:46:20,642 : INFO : EPOCH - 4 : training on 4169816 raw words (3321225 effective words) took 3.0s, 1122520 effective words/s\n",
      "2019-03-14 08:46:21,672 : INFO : EPOCH 5 - PROGRESS: at 36.85% examples, 1207719 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-14 08:46:22,681 : INFO : EPOCH 5 - PROGRESS: at 72.05% examples, 1184558 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-14 08:46:23,328 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-14 08:46:23,330 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-14 08:46:23,356 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-14 08:46:23,357 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-14 08:46:23,358 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-14 08:46:23,361 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-14 08:46:23,363 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-14 08:46:23,369 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-14 08:46:23,372 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-14 08:46:23,379 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-14 08:46:23,380 : INFO : EPOCH - 5 : training on 4169816 raw words (3322099 effective words) took 2.7s, 1220345 effective words/s\n",
      "2019-03-14 08:46:23,381 : INFO : training on a 20849080 raw words (16605483 effective words) took 14.7s, 1127541 effective words/s\n",
      "2019-03-14 08:46:23,382 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-03-14 08:46:23,383 : INFO : training model with 10 workers on 37992 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-03-14 08:46:24,407 : INFO : EPOCH 1 - PROGRESS: at 37.08% examples, 1222936 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-14 08:46:25,420 : INFO : EPOCH 1 - PROGRESS: at 76.14% examples, 1252466 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-14 08:46:25,988 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-14 08:46:25,999 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-14 08:46:26,008 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-14 08:46:26,011 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-14 08:46:26,016 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-14 08:46:26,018 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-14 08:46:26,020 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-14 08:46:26,021 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 08:46:26,025 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-14 08:46:26,032 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-14 08:46:26,033 : INFO : EPOCH - 1 : training on 4169816 raw words (3321118 effective words) took 2.6s, 1260442 effective words/s\n",
      "2019-03-14 08:46:27,061 : INFO : EPOCH 2 - PROGRESS: at 38.07% examples, 1259058 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-14 08:46:28,072 : INFO : EPOCH 2 - PROGRESS: at 76.14% examples, 1255866 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-14 08:46:28,614 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-14 08:46:28,629 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-14 08:46:28,633 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-14 08:46:28,637 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-14 08:46:28,639 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-14 08:46:28,639 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-14 08:46:28,640 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-14 08:46:28,653 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-14 08:46:28,655 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-14 08:46:28,656 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-14 08:46:28,657 : INFO : EPOCH - 2 : training on 4169816 raw words (3321222 effective words) took 2.6s, 1276890 effective words/s\n",
      "2019-03-14 08:46:29,677 : INFO : EPOCH 3 - PROGRESS: at 34.74% examples, 1148047 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-14 08:46:30,681 : INFO : EPOCH 3 - PROGRESS: at 71.09% examples, 1176243 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-14 08:46:31,406 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-14 08:46:31,410 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-14 08:46:31,425 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-14 08:46:31,429 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-14 08:46:31,431 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-14 08:46:31,433 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-14 08:46:31,434 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-14 08:46:31,435 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-14 08:46:31,447 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-14 08:46:31,448 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-14 08:46:31,449 : INFO : EPOCH - 3 : training on 4169816 raw words (3320252 effective words) took 2.8s, 1195314 effective words/s\n",
      "2019-03-14 08:46:32,465 : INFO : EPOCH 4 - PROGRESS: at 36.87% examples, 1224496 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-14 08:46:33,468 : INFO : EPOCH 4 - PROGRESS: at 73.90% examples, 1227281 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-14 08:46:34,082 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-14 08:46:34,099 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-14 08:46:34,110 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-14 08:46:34,111 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-14 08:46:34,112 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-14 08:46:34,113 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-14 08:46:34,115 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-14 08:46:34,119 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-14 08:46:34,122 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-14 08:46:34,130 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-14 08:46:34,131 : INFO : EPOCH - 4 : training on 4169816 raw words (3321835 effective words) took 2.7s, 1245373 effective words/s\n",
      "2019-03-14 08:46:35,170 : INFO : EPOCH 5 - PROGRESS: at 36.85% examples, 1207156 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-14 08:46:36,179 : INFO : EPOCH 5 - PROGRESS: at 76.65% examples, 1258747 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-14 08:46:36,734 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-14 08:46:36,742 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-14 08:46:36,748 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-14 08:46:36,752 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-14 08:46:36,754 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-14 08:46:36,756 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-14 08:46:36,757 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-14 08:46:36,767 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-14 08:46:36,769 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-14 08:46:36,770 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-14 08:46:36,771 : INFO : EPOCH - 5 : training on 4169816 raw words (3320833 effective words) took 2.6s, 1269573 effective words/s\n",
      "2019-03-14 08:46:37,787 : INFO : EPOCH 6 - PROGRESS: at 37.12% examples, 1230820 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-14 08:46:38,798 : INFO : EPOCH 6 - PROGRESS: at 75.36% examples, 1245479 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-14 08:46:39,361 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-14 08:46:39,371 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-14 08:46:39,377 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-14 08:46:39,381 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-14 08:46:39,382 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-14 08:46:39,383 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-14 08:46:39,384 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-14 08:46:39,388 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-14 08:46:39,397 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-14 08:46:39,401 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-14 08:46:39,402 : INFO : EPOCH - 6 : training on 4169816 raw words (3321108 effective words) took 2.6s, 1268785 effective words/s\n",
      "2019-03-14 08:46:40,441 : INFO : EPOCH 7 - PROGRESS: at 32.21% examples, 1039593 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-14 08:46:41,449 : INFO : EPOCH 7 - PROGRESS: at 63.82% examples, 1045143 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-14 08:46:42,456 : INFO : EPOCH 7 - PROGRESS: at 92.07% examples, 1004857 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-14 08:46:42,620 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-14 08:46:42,627 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-14 08:46:42,639 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-14 08:46:42,652 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-14 08:46:42,654 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-14 08:46:42,655 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-14 08:46:42,657 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-14 08:46:42,658 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-14 08:46:42,671 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-14 08:46:42,672 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-14 08:46:42,673 : INFO : EPOCH - 7 : training on 4169816 raw words (3320996 effective words) took 3.3s, 1019178 effective words/s\n",
      "2019-03-14 08:46:43,699 : INFO : EPOCH 8 - PROGRESS: at 30.47% examples, 999377 words/s, in_qsize 18, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 08:46:44,712 : INFO : EPOCH 8 - PROGRESS: at 63.85% examples, 1049849 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-14 08:46:45,649 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-14 08:46:45,655 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-14 08:46:45,668 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-14 08:46:45,677 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-14 08:46:45,679 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-14 08:46:45,680 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-14 08:46:45,681 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-14 08:46:45,683 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-14 08:46:45,694 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-14 08:46:45,697 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-14 08:46:45,698 : INFO : EPOCH - 8 : training on 4169816 raw words (3320815 effective words) took 3.0s, 1103133 effective words/s\n",
      "2019-03-14 08:46:46,732 : INFO : EPOCH 9 - PROGRESS: at 35.95% examples, 1171238 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-14 08:46:47,733 : INFO : EPOCH 9 - PROGRESS: at 69.92% examples, 1150669 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-14 08:46:48,451 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-14 08:46:48,461 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-14 08:46:48,464 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-14 08:46:48,467 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-14 08:46:48,470 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-14 08:46:48,471 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-14 08:46:48,474 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-14 08:46:48,485 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-14 08:46:48,488 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-14 08:46:48,490 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-14 08:46:48,491 : INFO : EPOCH - 9 : training on 4169816 raw words (3321120 effective words) took 2.8s, 1195387 effective words/s\n",
      "2019-03-14 08:46:49,527 : INFO : EPOCH 10 - PROGRESS: at 33.38% examples, 1081896 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-14 08:46:50,552 : INFO : EPOCH 10 - PROGRESS: at 67.02% examples, 1088929 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-14 08:46:51,484 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-14 08:46:51,490 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-14 08:46:51,506 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-14 08:46:51,508 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-14 08:46:51,512 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-14 08:46:51,517 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-14 08:46:51,518 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-14 08:46:51,519 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-14 08:46:51,526 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-14 08:46:51,530 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-14 08:46:51,531 : INFO : EPOCH - 10 : training on 4169816 raw words (3320523 effective words) took 3.0s, 1097399 effective words/s\n",
      "2019-03-14 08:46:51,532 : INFO : training on a 41698160 raw words (33209822 effective words) took 28.1s, 1179815 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(33209822, 41698160)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 0.7157601714134216),\n",
       " ('howdy', 0.6666513085365295),\n",
       " ('mornin', 0.6130080223083496),\n",
       " ('wb', 0.607377290725708),\n",
       " ('hallo', 0.6039972305297852),\n",
       " ('hiya', 0.5790766477584839),\n",
       " ('greetings', 0.5756561160087585),\n",
       " ('heya', 0.5622656941413879),\n",
       " ('ello', 0.5578193664550781),\n",
       " ('heyas', 0.5369154810905457)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"hi\"\n",
    "model.wv.most_similar(positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5279345 , -0.13940096,  0.14848003, -0.20386428,  0.30318138,\n",
       "       -0.7518825 ,  0.32862315, -0.0710255 , -0.30316716,  0.42825776,\n",
       "        0.41443545,  0.8094691 ,  1.0279076 ,  0.09358946,  0.5970718 ,\n",
       "       -0.51281583,  0.1515062 , -0.800897  , -0.4606536 , -0.28351092,\n",
       "       -0.2884971 , -0.8592874 , -0.17111757,  1.5243422 ,  0.13311459,\n",
       "       -1.017981  ,  0.308114  , -0.6696086 , -0.14961235, -0.5324405 ,\n",
       "        0.39775124,  0.48026583, -0.7153367 ,  0.3510015 ,  0.06037151,\n",
       "        0.39693207, -0.25557142,  0.14970575, -0.05517276, -0.16058485,\n",
       "       -0.54178226,  0.41233024, -0.31504527, -1.1565965 , -0.88247764,\n",
       "        0.45110017, -0.336391  , -0.9309303 , -0.14017169,  0.94689447,\n",
       "       -0.30711138,  0.8684238 , -0.40074196, -0.12314773,  0.09314933,\n",
       "        0.67560005,  0.46069032, -0.28590992, -0.508878  , -0.08849116,\n",
       "        0.02737017,  0.09804839,  0.54818404, -0.2637814 , -0.6895593 ,\n",
       "       -1.0969024 , -0.08252746, -0.8242667 ,  1.0529389 ,  0.32799673,\n",
       "        0.08334362,  0.70864403,  0.00854665,  0.6314138 , -0.5764023 ,\n",
       "       -0.45767355,  0.2895736 ,  0.2875674 , -1.3217204 , -0.07163829,\n",
       "        0.4143162 , -1.5265738 , -0.9303992 ,  0.0652837 , -0.07798912,\n",
       "        0.6829595 , -0.3790291 , -0.9242792 , -0.36027136, -0.7187858 ,\n",
       "       -1.0055206 ,  0.27539957, -0.41865504, -0.04729822,  0.77154136,\n",
       "        0.41502744, -0.29963586, -0.12062345,  0.80958366,  0.8296192 ,\n",
       "       -0.8125551 , -1.1219753 , -0.4206626 ,  0.2078395 , -0.17958874,\n",
       "        0.2043385 ,  0.9850568 , -0.12774275,  0.12028933,  0.78558236,\n",
       "        0.03002846,  0.83231556, -0.2637299 , -0.20461662, -0.08793671,\n",
       "        0.4695923 ,  0.03883104,  0.3572267 , -0.42751685,  0.6114776 ,\n",
       "        0.2559533 ,  1.0831643 ,  0.6055657 ,  0.32116815, -0.45837897,\n",
       "       -0.8674931 ,  0.13680466,  0.67733663,  0.11661343,  0.5671863 ,\n",
       "       -1.155024  ,  0.10741711, -0.91388404,  0.9823512 ,  0.8499823 ,\n",
       "       -1.1501591 , -0.00684459,  0.6120391 , -1.3265384 ,  0.07521719,\n",
       "        0.25629124,  1.0116129 ,  0.30716628, -0.49220976, -0.27134144,\n",
       "        0.66873246,  0.6829645 , -0.449277  , -0.6534644 , -0.67819357],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derek_vector = model.wv.word_vec(\"derek\")\n",
    "derek_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.word_vec(\"cvs\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.75430268e-05,  3.20636493e-04,  1.42087520e-03,  1.68726873e-03,\n",
       "        1.34648243e-03, -6.37582038e-04, -6.66913227e-04, -5.27164491e-04,\n",
       "        3.07432172e-04,  6.05612528e-04, -2.35400908e-03,  1.04977039e-03,\n",
       "       -6.52331917e-04,  1.87362335e-03, -8.56540806e-04,  1.97623111e-03,\n",
       "       -2.30358331e-03,  1.54692063e-03, -2.52765487e-03, -1.96043798e-03,\n",
       "       -2.12545809e-03, -8.89024464e-04, -1.21306453e-03, -7.72741972e-04,\n",
       "       -1.16806792e-03, -1.31328934e-05, -6.17633850e-05, -5.72148303e-04,\n",
       "       -2.74863269e-04, -3.50224669e-03,  5.04993717e-04, -9.83446487e-04,\n",
       "        8.95250356e-04, -9.80200246e-04, -1.67520915e-03,  1.39368454e-03,\n",
       "       -1.92016573e-03, -1.82855176e-03, -9.05466950e-05,  3.05284688e-04,\n",
       "        7.33390800e-04,  3.51534982e-04, -1.20753444e-04, -9.90741610e-05,\n",
       "       -6.91084075e-04, -2.10401035e-04,  9.06368135e-04,  6.84935600e-04,\n",
       "       -1.28763320e-03, -2.87177623e-04, -1.25641900e-03, -1.66943087e-03,\n",
       "       -9.07944632e-04,  1.80903426e-05,  7.47367856e-04,  6.77598000e-04,\n",
       "        2.18759436e-04,  7.14097187e-05,  3.95832263e-04,  9.42674058e-04,\n",
       "        1.06951536e-03, -1.68501725e-03, -2.59853550e-04, -3.38952901e-04,\n",
       "        1.58308621e-03, -6.33764255e-04,  6.33129384e-05,  1.99016556e-03,\n",
       "       -4.88161109e-04, -6.32786774e-04,  4.90075734e-04,  1.03755458e-03,\n",
       "       -2.92085082e-04,  1.65113225e-03, -4.48106206e-04, -1.99117046e-03,\n",
       "        1.58757844e-03, -2.44730362e-03,  3.44028464e-04,  1.54860225e-03,\n",
       "        7.26308557e-04,  7.03585974e-04, -7.66912999e-05,  7.46452948e-04,\n",
       "       -4.89642960e-04,  5.03161689e-04, -1.18328957e-04, -1.34206074e-03,\n",
       "       -1.16904092e-03,  1.00453710e-03,  2.06496939e-03,  4.89430968e-04,\n",
       "        9.07712791e-04, -9.25157627e-04,  1.41282286e-03, -4.28248255e-04,\n",
       "       -1.00845750e-03,  9.04060609e-04,  4.28478525e-04,  1.50159711e-03,\n",
       "       -3.23773100e-04, -1.42298883e-03, -2.10075203e-04,  1.66285632e-03,\n",
       "       -6.43575622e-05, -1.17424305e-03,  6.06184069e-04, -6.81742153e-04,\n",
       "       -1.54217111e-03, -6.69734203e-04,  4.93023312e-04,  1.32517924e-03,\n",
       "        6.47573790e-04,  3.46433022e-03,  5.58340224e-04, -1.68956700e-04,\n",
       "       -8.87927948e-04,  1.85825606e-03,  2.83803628e-03,  9.50692687e-04,\n",
       "        1.99450456e-04,  8.45497882e-04,  1.81591429e-03, -1.03730895e-03,\n",
       "       -4.01707657e-04,  6.63494342e-04, -1.81786926e-03,  5.87727875e-04,\n",
       "        4.87602520e-04, -3.27550486e-04,  4.46081831e-04,  1.04231527e-04,\n",
       "       -1.03733355e-05, -5.37791173e-04,  1.54717732e-03, -2.37736618e-04,\n",
       "       -3.96819087e-04, -6.76410622e-04, -1.29524502e-03, -4.73117223e-04,\n",
       "        9.27727378e-05,  1.92099356e-03, -1.81116149e-04,  3.14805278e-04,\n",
       "       -1.38570336e-04,  1.38818345e-03,  6.73375267e-04,  1.48490319e-04,\n",
       "        1.22803671e-04,  5.71640208e-04], dtype=float32)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(model.wv.word_vec(\"derek\"), 0.0015415098078320612)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-19 14:54:34,437 : INFO : saving Word2Vec object under ../data_files/gnue_irc_word2vec_model, separately None\n",
      "2019-02-19 14:54:34,438 : INFO : not storing attribute vectors_norm\n",
      "2019-02-19 14:54:34,439 : INFO : not storing attribute cum_table\n",
      "2019-02-19 14:54:34,984 : INFO : saved ../data_files/gnue_irc_word2vec_model\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "filename = join(DATA_DIR, \"gnue_irc_word2vec_model\")\n",
    "model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Word TF-IDFS\n",
    "def get_word_tf_idfs(word_tf_idfs_filename):\n",
    "    word_tf_idfs = []\n",
    "    with open(word_tf_idfs_filename) as word_tf_idfs_file:\n",
    "        for values in word_tf_idfs_file:\n",
    "            values = [float(value) for value in values.strip().split()]\n",
    "            word_tf_idfs.append(values)\n",
    "    return word_tf_idfs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tf_idfs_filename = join(DATA_DIR, \"word_tf_idfs.txt\")\n",
    "word_tf_idfs = get_word_tf_idfs(word_tf_idfs_filename)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0005048845017409812, 0.0006060496352296479]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tf_idfs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Sentence Vectors using the **Average of Word2Vec vectors with TF-IDF** method\n",
    "\n",
    "\"This is one of the best approach which I will recommend. Just take the word vectors and multiply it with their TF-IDF scores. Just take the average and it will represent your sentence vector\"\n",
    "\n",
    "https://stackoverflow.com/questions/29760935/how-to-get-vector-for-a-sentence-from-the-word2vec-of-tokens-in-sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_leading_and_trailing_punctuation(word):\n",
    "    return word.strip(string.punctuation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_sentence(sentence):\n",
    "    if not sentence:\n",
    "        print(\"No words here\")\n",
    "    if type(sentence) is not str:\n",
    "        try:\n",
    "            sentence = sentence.decode('utf-8')\n",
    "        except Exception:\n",
    "            raise ValueError(\"Input must be a String or ByteString\")\n",
    "        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_chars_in_word_are_punctuation(word):\n",
    "    return all(char in PUNCTUATION_SET for char in word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_sentence(sentence):\n",
    "    words = []\n",
    "    for word in sentence.split():\n",
    "        if not all_chars_in_word_are_punctuation(word):\n",
    "            word = strip_leading_and_trailing_punctuation(word)\n",
    "            if ',' in word:\n",
    "                comma_split_words = word.split(',')\n",
    "                for word in comma_split_words:\n",
    "                    if not all_chars_in_word_are_punctuation(word):\n",
    "                        words.append(word)\n",
    "            else:\n",
    "                words.append(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_word_vectors(sentence, model):\n",
    "    sentence = pre_process_sentence(sentence)\n",
    "    vectors = []\n",
    "    words = get_words_in_sentence(sentence)\n",
    "    if not words:\n",
    "        return [np.zeros((150,))]\n",
    "    for word in words:\n",
    "        try:\n",
    "            vectors.append(model.wv.word_vec(word.lower()))\n",
    "        except KeyError:\n",
    "            vectors.append(np.zeros((150,)))\n",
    "    \n",
    "    return vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([vec for vec in get_sentence_word_vectors(\"cvs diff\", model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_word_vectors(\"cvs ddghsdg\", model)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(sentence_word_tf_idfs, word_vectors):\n",
    "    products = []\n",
    "    for word_tf_idf, word_vector in zip(sentence_word_tf_idfs, word_vectors):\n",
    "        product = np.multiply(word_vector, word_tf_idf)\n",
    "        products.append(product)\n",
    "    vector_sum = np.sum(products, axis=0)\n",
    "    return vector_sum / len(word_tf_idfs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0005048845017409812, 0.0006060496352296479]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tf_idfs_2, word_vectors = word_tf_idfs[1], get_sentence_word_vectors(\"cVS diff\", model)\n",
    "sentence_vector = get_sentence_vector(word_tf_idfs_2, word_vectors)\n",
    "assert sentence_vector.shape == derek_vector.shape\n",
    "sentence_vector.shape\n",
    "len(word_vectors)\n",
    "word_tf_idfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_vectors(\n",
    "    chat_input_filename, \n",
    "    word_tf_idfs, \n",
    "    word_2_vec_model, \n",
    "    sentence_vectors_output_filename\n",
    "):\n",
    "    with gzip.open(chat_input_filename) as chat_input_file, open(\n",
    "        sentence_vectors_output_filename, \"w\") as sentence_vectors_output_file:\n",
    "        line_count = 0\n",
    "        # Add header ro csv. Numbers from 1 to 150\n",
    "        sentence_vectors_output_file.write(\"{}\\n\".format(\",\".join([str(num) for num in range(1,151)])))\n",
    "        for sentence in chat_input_file:\n",
    "            sentence_word_vectors = get_sentence_word_vectors(sentence, word_2_vec_model)\n",
    "            sentence_vector = get_sentence_vector(word_tf_idfs[line_count], sentence_word_vectors)\n",
    "            csv_vector = ','.join([str(num) for num in sentence_vector])\n",
    "            sentence_vectors_output_file.write(\"{} \\n\".format(csv_vector))\n",
    "            line_count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_word_tf_idfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2c868cfe9148>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword_tf_idfs_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"word_tf_idfs.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword_tf_idfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_word_tf_idfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tf_idfs_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mword_2_vec_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msentence_vectors_output_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFEATURES_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sentence_vectors.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_word_tf_idfs' is not defined"
     ]
    }
   ],
   "source": [
    "word_tf_idfs_filename = join(DATA_DIR, \"word_tf_idfs.txt\")\n",
    "word_tf_idfs = get_word_tf_idfs(word_tf_idfs_filename)\n",
    "word_2_vec_model = model\n",
    "sentence_vectors_output_filename = join(FEATURES_DIR, \"sentence_vectors.csv\")\n",
    "\n",
    "generate_sentence_vectors(\n",
    "    CHAT_INPUT_FILE, \n",
    "    word_tf_idfs, \n",
    "    word_2_vec_model, \n",
    "    sentence_vectors_output_filename\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  4,  8],\n",
       "       [12, 16, 20],\n",
       "       [24, 28, 32]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.arange(9.0).reshape((3, 3))\n",
    "x2 = np.arange(9).reshape((3, 3)) * 2\n",
    "# np.multiply(x1, x2)\n",
    "np.sum([x2, x2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sentence_vectors_output_filename) as sentence_vectors:\n",
    "    sentence_vector = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
