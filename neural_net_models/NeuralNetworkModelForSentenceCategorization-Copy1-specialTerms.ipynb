{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Model for Sentence Categorization\n",
    "\n",
    "A Neural Network model to help classify sentences as important, or not important enough to be part of a summary\n",
    "\n",
    "Built using Keras\n",
    "\n",
    "https://towardsdatascience.com/building-a-deep-learning-model-using-keras-1548ca149d37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_DIR = join(\"..\", \"feature_extraction\", \"feature_outputs\")\n",
    "DATA_FILES_DIR = join(\"..\", \"feature_extraction\", \"data_files\")\n",
    "sentence_vectors_filename = join(FEATURES_DIR, \"sentence_vectors.csv\")\n",
    "summarized_chat_log_ids_filename = join(DATA_FILES_DIR, \"summarized_chat_log_ids.csv\")\n",
    "summarized_chat_date_partitions_filename = join(\n",
    "    DATA_FILES_DIR, \"summarized_chat_date_partitions_cumulative_count.csv\"\n",
    ")\n",
    "concatenated_vectors_filename = join(DATA_FILES_DIR, \"summarized_concatenated_vectors_window_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data using pandas. This takes a couple of seconds\n",
    "# sentence_vectors_df = pd.read_csv(sentence_vectors_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sentence_vectors_df.shape)\n",
    "# sentence_vectors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_chat_log_ids = pd.read_csv(\n",
    "    summarized_chat_log_ids_filename,\n",
    "    names = [\"log_id\", \"is_summary\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20715, 2)\n",
      "   log_id  is_summary\n",
      "0   85350           0\n",
      "1   85351           0\n",
      "2   85352           0\n",
      "3   85353           0\n",
      "4   85354           0\n",
      "       log_id  is_summary\n",
      "20710  624000           0\n",
      "20711  624001           0\n",
      "20712  624002           0\n",
      "20713  624003           0\n",
      "20714  624004           0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 85350,  85351,  85352, ..., 624002, 624003, 624004])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(summarized_chat_log_ids.shape)\n",
    "print(summarized_chat_log_ids.head())\n",
    "print(summarized_chat_log_ids.tail())\n",
    "summarized_chat_log_ids_array = np.array(summarized_chat_log_ids.log_id)\n",
    "summarized_chat_log_ids_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarized_sentence_vectors_df = sentence_vectors_df.filter(summarized_chat_log_ids_array, axis=0)\n",
    "# summarized_num_of_columns = summarized_sentence_vectors_df.shape[0]\n",
    "# # summarized_sentence_vectors_df[\"index\"] = [num for num in range(summarized_num_of_columns)]\n",
    "# # summarized_sentence_vectors_df.set_index([\"index\"])\n",
    "# # summarized_sentence_vectors_df.reset_index\n",
    "# # summarized_sentence_vectors_df.insert(0, \"index\", range(summarized_num_of_columns))\n",
    "# print(summarized_sentence_vectors_df.shape)\n",
    "# summarized_sentence_vectors_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_chat_date_partitions = pd.read_csv(summarized_chat_date_partitions_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_log_id</th>\n",
       "      <th>date_of_log</th>\n",
       "      <th>chat_line_count</th>\n",
       "      <th>cumulative_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>473197</td>\n",
       "      <td>2001-11-07</td>\n",
       "      <td>1990</td>\n",
       "      <td>16079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>495175</td>\n",
       "      <td>2001-11-13</td>\n",
       "      <td>1051</td>\n",
       "      <td>17130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>497259</td>\n",
       "      <td>2001-11-14</td>\n",
       "      <td>536</td>\n",
       "      <td>17666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>526307</td>\n",
       "      <td>2001-11-15</td>\n",
       "      <td>1053</td>\n",
       "      <td>18719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>531627</td>\n",
       "      <td>2001-11-12</td>\n",
       "      <td>1348</td>\n",
       "      <td>20067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>579591</td>\n",
       "      <td>2001-10-24</td>\n",
       "      <td>162</td>\n",
       "      <td>20229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>586206</td>\n",
       "      <td>2001-10-23</td>\n",
       "      <td>165</td>\n",
       "      <td>20394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>623684</td>\n",
       "      <td>2001-10-25</td>\n",
       "      <td>321</td>\n",
       "      <td>20715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    min_log_id date_of_log  chat_line_count  cumulative_count\n",
       "19      473197  2001-11-07             1990             16079\n",
       "20      495175  2001-11-13             1051             17130\n",
       "21      497259  2001-11-14              536             17666\n",
       "22      526307  2001-11-15             1053             18719\n",
       "23      531627  2001-11-12             1348             20067\n",
       "24      579591  2001-10-24              162             20229\n",
       "25      586206  2001-10-23              165             20394\n",
       "26      623684  2001-10-25              321             20715"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_chat_date_partitions.tail(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the first chat log id for the third last chat date. Use the last three chat log dates for testing. The rest to be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20067"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_for_validation_test_split = summarized_chat_date_partitions.tail(4)[\"cumulative_count\"]\n",
    "index_for_validation_test_split = index_for_validation_test_split.values[0]\n",
    "index_for_train_validation_split = summarized_chat_date_partitions.tail(8)[\"cumulative_count\"]\n",
    "index_for_train_validation_split = index_for_train_validation_split.values[0]\n",
    "print(index_for_train_validation_split)\n",
    "index_for_validation_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate sentence vectors for context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_sentence_vectors(data_values, window):\n",
    "    number_of_rows, number_of_columns = data_values.shape\n",
    "    eventual_number_of_columns = number_of_columns +(number_of_columns * window * 2)\n",
    "    new_np_data = np.array([]).reshape(0, eventual_number_of_columns)\n",
    "    \n",
    "    for index in range(data_values.shape[0]):\n",
    "        first_index = index - window\n",
    "        last_index = index + window\n",
    "        start_padding = []\n",
    "        end_padding = []\n",
    "        \n",
    "        if first_index < 0:\n",
    "            start_padding = np.zeros((abs(first_index), number_of_columns))\n",
    "            first_index = 0\n",
    "        if last_index >= number_of_rows:\n",
    "            end_padding = np.zeros((((last_index - number_of_rows) + 1), number_of_columns))\n",
    "            last_index = number_of_rows - 1\n",
    "            \n",
    "        concatenated_row = data_values[first_index:last_index+1]\n",
    "        # print(\"\\n\", index, \"\\n\", concatenated_row, start_padding, \"\\n\\n\")\n",
    "        if len(start_padding) > 0:\n",
    "            # print(start_padding.shape, data_values[first_index: first_index+1].shape)\n",
    "            concatenated_row = np.concatenate((start_padding, concatenated_row))\n",
    "        if len(end_padding) > 0:\n",
    "            # print(end_padding.shape, data_values[first_index: first_index+1].shape)\n",
    "            concatenated_row = np.concatenate((concatenated_row, end_padding))\n",
    "        concatenated_row = concatenated_row.ravel()\n",
    "        # print(index, concatenated_row, \"\\n\\n\")\n",
    "        # print(concatenated_row.shape, new_np_data.shape)\n",
    "        new_np_data = np.concatenate((new_np_data, concatenated_row))\n",
    "    return pd.DataFrame(new_np_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.392333984375e-05\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# summarized_sentence_vectors_array = summarized_sentence_vectors_df.values\n",
    "context_window = 5\n",
    "\n",
    "# This takes about 40 minutes, or 2386.799 seconds\n",
    "# concatenated_sentence_vectors_df = concatenate_sentence_vectors(summarized_sentence_vectors_array, context_window)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated_sentence_vectors_df = pd.read_csv(concatenated_vectors_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated_sentence_vectors_df = concatenated_sentence_vectors_df.drop(columns=[0])\n",
    "# concatenated_sentence_vectors_df.to_csv(concatenated_vectors_filename)\n",
    "# concatenated_sentence_vectors_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated_sentence_vectors_df.tail(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated_sentence_vectors_df = concatenated_sentence_vectors_df.drop(columns=[\"Unnamed: 0\"])\n",
    "# concatenated_sentence_vectors_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(summarized_sentence_vectors_df.shape)\n",
    "# summarized_sentence_vectors_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_DATA_FILE = join(FEATURES_DIR, \"summarized_chats_features.csv\")\n",
    "# read in data using pandas\n",
    "unnormalized_chat_log_df = pd.read_csv(FEATURES_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_id</th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>mean_tf_isf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "      <th>is_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85350</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.006050</td>\n",
       "      <td>2.758723</td>\n",
       "      <td>0.788992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85351</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.003486</td>\n",
       "      <td>0.026734</td>\n",
       "      <td>0.149401</td>\n",
       "      <td>0.042729</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85352</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.004054</td>\n",
       "      <td>0.031086</td>\n",
       "      <td>0.180867</td>\n",
       "      <td>0.051728</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85353</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.003645</td>\n",
       "      <td>0.027952</td>\n",
       "      <td>0.499669</td>\n",
       "      <td>0.142905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85354</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>0.002310</td>\n",
       "      <td>0.017715</td>\n",
       "      <td>0.295432</td>\n",
       "      <td>0.084493</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   log_id  absolute_sentence_position  sentence_length  \\\n",
       "0   85350                    0.000436                1   \n",
       "1   85351                    0.000871               13   \n",
       "2   85352                    0.001307               10   \n",
       "3   85353                    0.001743                5   \n",
       "4   85354                    0.002179                8   \n",
       "\n",
       "   number_of_special_terms  sentiment_score  mean_tf_idf  \\\n",
       "0                        0           0.6249     0.000789   \n",
       "1                        0           0.0000     0.003486   \n",
       "2                        0           0.0000     0.004054   \n",
       "3                        0           0.0000     0.003645   \n",
       "4                        0           0.2263     0.002310   \n",
       "\n",
       "   normalized_mean_tf_idf  mean_tf_isf  normalized_mean_tf_isf  is_summary  \n",
       "0                0.006050     2.758723                0.788992           0  \n",
       "1                0.026734     0.149401                0.042729           0  \n",
       "2                0.031086     0.180867                0.051728           0  \n",
       "3                0.027952     0.499669                0.142905           0  \n",
       "4                0.017715     0.295432                0.084493           0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unnormalized_chat_log_df.tail()\n",
    "# unnormalized_chat_log_df.insert(0, \"index\", range(unnormalized_chat_log_df.shape[0]))\n",
    "# print(unnormalized_chat_log_df.loc[0:5])\n",
    "unnormalized_chat_log_df.sentiment_score = abs(unnormalized_chat_log_df.sentiment_score)\n",
    "unnormalized_chat_log_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnormalized_merged_log_data_df = pd.merge(\n",
    "#     unnormalized_chat_log_df, \n",
    "#     summarized_sentence_vectors_df, \n",
    "#     left_on=\"log_id\",\n",
    "#     right_index=True\n",
    "# )\n",
    "# print(unnormalized_merged_log_data_df.shape)\n",
    "# unnormalized_merged_log_data_df.head()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing The Data\n",
    "Formula to normalize data\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.9707\n",
      "0.00043572984749455336\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.04073392898973263\n",
      "3.36078268987328\n",
      "0.3123448778466237\n",
      "0.9611808193277522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_data_max_values = unnormalized_chat_log_df.max()\n",
    "chat_data_min_values = unnormalized_chat_log_df.min()\n",
    "\n",
    "max_number_of_special_terms = chat_data_max_values.number_of_special_terms\n",
    "max_sentence_length = chat_data_max_values.sentence_length\n",
    "max_sentiment_score = chat_data_max_values.sentiment_score\n",
    "max_absolute_sentence_position = chat_data_max_values.absolute_sentence_position\n",
    "max_mean_tf_idf = chat_data_max_values.mean_tf_idf\n",
    "max_mean_tf_isf = chat_data_max_values.mean_tf_isf\n",
    "max_normalized_mean_tf_idf = chat_data_max_values.normalized_mean_tf_idf\n",
    "max_normalized_mean_tf_isf = chat_data_max_values.normalized_mean_tf_isf\n",
    "\n",
    "min_number_of_special_terms = chat_data_min_values.number_of_special_terms\n",
    "min_sentence_length = chat_data_min_values.sentence_length\n",
    "min_sentiment_score = chat_data_min_values.sentiment_score\n",
    "min_absolute_sentence_position = chat_data_min_values.absolute_sentence_position\n",
    "min_mean_tf_idf = chat_data_min_values.mean_tf_idf\n",
    "min_mean_tf_isf = chat_data_min_values.mean_tf_isf\n",
    "min_normalized_mean_tf_idf = chat_data_min_values.normalized_mean_tf_idf\n",
    "min_normalized_mean_tf_isf = chat_data_min_values.normalized_mean_tf_isf\n",
    "\n",
    "print(min_sentiment_score)\n",
    "print(max_sentiment_score)\n",
    "print(min_absolute_sentence_position)\n",
    "print(max_absolute_sentence_position)\n",
    "print(min_mean_tf_idf)\n",
    "print(min_mean_tf_isf)\n",
    "print(min_normalized_mean_tf_idf)\n",
    "print(min_normalized_mean_tf_isf)\n",
    "print(max_mean_tf_idf)\n",
    "print(max_mean_tf_isf)\n",
    "print(max_normalized_mean_tf_idf)\n",
    "print(max_normalized_mean_tf_isf)\n",
    "max_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.013699\n",
      "1    0.178082\n",
      "2    0.136986\n",
      "3    0.068493\n",
      "4    0.109589\n",
      "Name: sentence_length, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_id</th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>mean_tf_isf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "      <th>is_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.643762</td>\n",
       "      <td>0.019369</td>\n",
       "      <td>0.019369</td>\n",
       "      <td>0.820857</td>\n",
       "      <td>0.820857</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85351</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085591</td>\n",
       "      <td>0.085591</td>\n",
       "      <td>0.044454</td>\n",
       "      <td>0.044454</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85352</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099525</td>\n",
       "      <td>0.099525</td>\n",
       "      <td>0.053817</td>\n",
       "      <td>0.053817</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85353</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089492</td>\n",
       "      <td>0.089492</td>\n",
       "      <td>0.148677</td>\n",
       "      <td>0.148677</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85354</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.233131</td>\n",
       "      <td>0.056715</td>\n",
       "      <td>0.056715</td>\n",
       "      <td>0.087906</td>\n",
       "      <td>0.087906</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   log_id  absolute_sentence_position  sentence_length  \\\n",
       "0   85350                    0.000000         0.013699   \n",
       "1   85351                    0.000436         0.178082   \n",
       "2   85352                    0.000872         0.136986   \n",
       "3   85353                    0.001308         0.068493   \n",
       "4   85354                    0.001744         0.109589   \n",
       "\n",
       "   number_of_special_terms  sentiment_score  mean_tf_idf  \\\n",
       "0                      0.0         0.643762     0.019369   \n",
       "1                      0.0         0.000000     0.085591   \n",
       "2                      0.0         0.000000     0.099525   \n",
       "3                      0.0         0.000000     0.089492   \n",
       "4                      0.0         0.233131     0.056715   \n",
       "\n",
       "   normalized_mean_tf_idf  mean_tf_isf  normalized_mean_tf_isf  is_summary  \n",
       "0                0.019369     0.820857                0.820857           0  \n",
       "1                0.085591     0.044454                0.044454           0  \n",
       "2                0.099525     0.053817                0.053817           0  \n",
       "3                0.089492     0.148677                0.148677           0  \n",
       "4                0.056715     0.087906                0.087906           0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_log_df = unnormalized_chat_log_df.copy()\n",
    "chat_log_df.sentence_length = (\n",
    "    chat_log_df.sentence_length - min_sentence_length) / (\n",
    "    max_sentence_length - min_sentence_length)\n",
    "chat_log_df.number_of_special_terms = (\n",
    "    chat_log_df.number_of_special_terms - min_number_of_special_terms) / (\n",
    "    max_number_of_special_terms - min_number_of_special_terms)\n",
    "chat_log_df.sentiment_score = (\n",
    "    chat_log_df.sentiment_score - min_sentiment_score) / (\n",
    "    max_sentiment_score - min_sentiment_score)\n",
    "chat_log_df.absolute_sentence_position = (\n",
    "    chat_log_df.absolute_sentence_position - min_absolute_sentence_position) / (\n",
    "    max_absolute_sentence_position - min_absolute_sentence_position)\n",
    "chat_log_df.mean_tf_idf = (\n",
    "    chat_log_df.mean_tf_idf - min_mean_tf_idf) / (\n",
    "    max_mean_tf_idf - min_mean_tf_idf)\n",
    "chat_log_df.mean_tf_isf = (\n",
    "    chat_log_df.mean_tf_isf - min_mean_tf_isf) / (\n",
    "    max_mean_tf_isf - min_mean_tf_isf)\n",
    "chat_log_df.normalized_mean_tf_idf = (\n",
    "    chat_log_df.normalized_mean_tf_idf - min_normalized_mean_tf_idf) / (\n",
    "    max_normalized_mean_tf_idf - min_normalized_mean_tf_idf)\n",
    "chat_log_df.normalized_mean_tf_isf = (\n",
    "    chat_log_df.normalized_mean_tf_isf - min_normalized_mean_tf_isf) / (\n",
    "    max_normalized_mean_tf_isf - min_normalized_mean_tf_isf)\n",
    "\n",
    "print(chat_log_df.sentence_length.head())\n",
    "chat_log_df.iloc[0:5]\n",
    "# chat_log_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_id</th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>mean_tf_isf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "      <th>is_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16074</th>\n",
       "      <td>475182</td>\n",
       "      <td>0.997989</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.208406</td>\n",
       "      <td>0.081768</td>\n",
       "      <td>0.081768</td>\n",
       "      <td>0.059544</td>\n",
       "      <td>0.059544</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16075</th>\n",
       "      <td>475183</td>\n",
       "      <td>0.998492</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052944</td>\n",
       "      <td>0.052944</td>\n",
       "      <td>0.712859</td>\n",
       "      <td>0.712859</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16076</th>\n",
       "      <td>475184</td>\n",
       "      <td>0.998995</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.233131</td>\n",
       "      <td>0.050432</td>\n",
       "      <td>0.050432</td>\n",
       "      <td>0.269761</td>\n",
       "      <td>0.269761</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16077</th>\n",
       "      <td>475185</td>\n",
       "      <td>0.999497</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.656124</td>\n",
       "      <td>0.110526</td>\n",
       "      <td>0.110526</td>\n",
       "      <td>0.522136</td>\n",
       "      <td>0.522136</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16078</th>\n",
       "      <td>475186</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.230761</td>\n",
       "      <td>0.096316</td>\n",
       "      <td>0.096316</td>\n",
       "      <td>0.080129</td>\n",
       "      <td>0.080129</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       log_id  absolute_sentence_position  sentence_length  \\\n",
       "16074  475182                    0.997989         0.136986   \n",
       "16075  475183                    0.998492         0.013699   \n",
       "16076  475184                    0.998995         0.027397   \n",
       "16077  475185                    0.999497         0.013699   \n",
       "16078  475186                    1.000000         0.082192   \n",
       "\n",
       "       number_of_special_terms  sentiment_score  mean_tf_idf  \\\n",
       "16074                      0.0         0.208406     0.081768   \n",
       "16075                      0.0         0.000000     0.052944   \n",
       "16076                      0.0         0.233131     0.050432   \n",
       "16077                      0.0         0.656124     0.110526   \n",
       "16078                      0.0         0.230761     0.096316   \n",
       "\n",
       "       normalized_mean_tf_idf  mean_tf_isf  normalized_mean_tf_isf  is_summary  \n",
       "16074                0.081768     0.059544                0.059544           0  \n",
       "16075                0.052944     0.712859                0.712859           0  \n",
       "16076                0.050432     0.269761                0.269761           0  \n",
       "16077                0.110526     0.522136                0.522136           0  \n",
       "16078                0.096316     0.080129                0.080129           0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = chat_log_df.iloc[:index_for_train_validation_split]\n",
    "validation_df = chat_log_df.iloc[index_for_train_validation_split:index_for_validation_test_split]\n",
    "test_df = chat_log_df.iloc[index_for_validation_test_split:]\n",
    "train_df.tail()\n",
    "\n",
    "# Sentence Vectors only\n",
    "# train_vectors_df = summarized_sentence_vectors_df.iloc[:index_for_train_validation_split]\n",
    "# validation_vectors_df = summarized_sentence_vectors_df.iloc[index_for_train_validation_split:index_for_validation_test_split]\n",
    "# test_vectors_df = summarized_sentence_vectors_df.iloc[index_for_validation_test_split:]\n",
    "# train_vectors_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_id</th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>mean_tf_isf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "      <th>is_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16079</th>\n",
       "      <td>495175</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472649</td>\n",
       "      <td>0.050304</td>\n",
       "      <td>0.050304</td>\n",
       "      <td>0.051808</td>\n",
       "      <td>0.051808</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16080</th>\n",
       "      <td>495176</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>0.899077</td>\n",
       "      <td>0.899077</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16081</th>\n",
       "      <td>495177</td>\n",
       "      <td>0.002420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16082</th>\n",
       "      <td>495178</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040161</td>\n",
       "      <td>0.040161</td>\n",
       "      <td>0.063344</td>\n",
       "      <td>0.063344</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16083</th>\n",
       "      <td>495179</td>\n",
       "      <td>0.004324</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472649</td>\n",
       "      <td>0.038080</td>\n",
       "      <td>0.038080</td>\n",
       "      <td>0.131802</td>\n",
       "      <td>0.131802</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       log_id  absolute_sentence_position  sentence_length  \\\n",
       "16079  495175                    0.000516         0.150685   \n",
       "16080  495176                    0.001468         0.013699   \n",
       "16081  495177                    0.002420         0.000000   \n",
       "16082  495178                    0.003372         0.136986   \n",
       "16083  495179                    0.004324         0.054795   \n",
       "\n",
       "       number_of_special_terms  sentiment_score  mean_tf_idf  \\\n",
       "16079                 0.000000         0.472649     0.050304   \n",
       "16080                 0.000000         0.000000     0.020250   \n",
       "16081                 0.000000         0.472649     0.000000   \n",
       "16082                 0.054054         0.000000     0.040161   \n",
       "16083                 0.000000         0.472649     0.038080   \n",
       "\n",
       "       normalized_mean_tf_idf  mean_tf_isf  normalized_mean_tf_isf  is_summary  \n",
       "16079                0.050304     0.051808                0.051808           0  \n",
       "16080                0.020250     0.899077                0.899077           0  \n",
       "16081                0.000000     0.000000                0.000000           0  \n",
       "16082                0.040161     0.063344                0.063344           0  \n",
       "16083                0.038080     0.131802                0.131802           0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_id</th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>mean_tf_isf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "      <th>is_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20062</th>\n",
       "      <td>532970</td>\n",
       "      <td>0.997031</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043630</td>\n",
       "      <td>0.043630</td>\n",
       "      <td>0.931238</td>\n",
       "      <td>0.931238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20063</th>\n",
       "      <td>532971</td>\n",
       "      <td>0.997774</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099908</td>\n",
       "      <td>0.099908</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20064</th>\n",
       "      <td>532972</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.542598</td>\n",
       "      <td>0.057936</td>\n",
       "      <td>0.057936</td>\n",
       "      <td>0.752096</td>\n",
       "      <td>0.752096</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20065</th>\n",
       "      <td>532973</td>\n",
       "      <td>0.999258</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257546</td>\n",
       "      <td>0.065927</td>\n",
       "      <td>0.065927</td>\n",
       "      <td>0.105421</td>\n",
       "      <td>0.105421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20066</th>\n",
       "      <td>532974</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022086</td>\n",
       "      <td>0.022086</td>\n",
       "      <td>0.340556</td>\n",
       "      <td>0.340556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       log_id  absolute_sentence_position  sentence_length  \\\n",
       "20062  532970                    0.997031         0.013699   \n",
       "20063  532971                    0.997774         0.041096   \n",
       "20064  532972                    0.998516         0.013699   \n",
       "20065  532973                    0.999258         0.068493   \n",
       "20066  532974                    1.000000         0.027397   \n",
       "\n",
       "       number_of_special_terms  sentiment_score  mean_tf_idf  \\\n",
       "20062                      0.0         0.000000     0.043630   \n",
       "20063                      0.0         0.000000     0.099908   \n",
       "20064                      0.0         0.542598     0.057936   \n",
       "20065                      0.0         0.257546     0.065927   \n",
       "20066                      0.0         0.000000     0.022086   \n",
       "\n",
       "       normalized_mean_tf_idf  mean_tf_isf  normalized_mean_tf_isf  is_summary  \n",
       "20062                0.043630     0.931238                0.931238           0  \n",
       "20063                0.099908     0.230029                0.230029           0  \n",
       "20064                0.057936     0.752096                0.752096           0  \n",
       "20065                0.065927     0.105421                0.105421           0  \n",
       "20066                0.022086     0.340556                0.340556           0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_id</th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>mean_tf_isf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "      <th>is_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20067</th>\n",
       "      <td>579591</td>\n",
       "      <td>0.005740</td>\n",
       "      <td>0.232877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.617493</td>\n",
       "      <td>0.014957</td>\n",
       "      <td>0.014957</td>\n",
       "      <td>0.029834</td>\n",
       "      <td>0.029834</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20068</th>\n",
       "      <td>579592</td>\n",
       "      <td>0.011915</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304935</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.328720</td>\n",
       "      <td>0.328720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20069</th>\n",
       "      <td>579593</td>\n",
       "      <td>0.018091</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304935</td>\n",
       "      <td>0.010781</td>\n",
       "      <td>0.010781</td>\n",
       "      <td>0.191207</td>\n",
       "      <td>0.191207</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20070</th>\n",
       "      <td>579594</td>\n",
       "      <td>0.024266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.453693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071</th>\n",
       "      <td>579595</td>\n",
       "      <td>0.030442</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013585</td>\n",
       "      <td>0.013585</td>\n",
       "      <td>0.283935</td>\n",
       "      <td>0.283935</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       log_id  absolute_sentence_position  sentence_length  \\\n",
       "20067  579591                    0.005740         0.232877   \n",
       "20068  579592                    0.011915         0.027397   \n",
       "20069  579593                    0.018091         0.041096   \n",
       "20070  579594                    0.024266         0.000000   \n",
       "20071  579595                    0.030442         0.027397   \n",
       "\n",
       "       number_of_special_terms  sentiment_score  mean_tf_idf  \\\n",
       "20067                      0.0         0.617493     0.014957   \n",
       "20068                      0.0         0.304935     0.002195   \n",
       "20069                      0.0         0.304935     0.010781   \n",
       "20070                      0.0         0.453693     0.000000   \n",
       "20071                      0.0         0.000000     0.013585   \n",
       "\n",
       "       normalized_mean_tf_idf  mean_tf_isf  normalized_mean_tf_isf  is_summary  \n",
       "20067                0.014957     0.029834                0.029834           0  \n",
       "20068                0.002195     0.328720                0.328720           0  \n",
       "20069                0.010781     0.191207                0.191207           0  \n",
       "20070                0.000000     0.000000                0.000000           0  \n",
       "20071                0.013585     0.283935                0.283935           0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_id</th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>mean_tf_isf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "      <th>is_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20710</th>\n",
       "      <td>624000</td>\n",
       "      <td>0.987534</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010594</td>\n",
       "      <td>0.010594</td>\n",
       "      <td>0.292627</td>\n",
       "      <td>0.292627</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20711</th>\n",
       "      <td>624001</td>\n",
       "      <td>0.990650</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019673</td>\n",
       "      <td>0.019673</td>\n",
       "      <td>0.132364</td>\n",
       "      <td>0.132364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20712</th>\n",
       "      <td>624002</td>\n",
       "      <td>0.993767</td>\n",
       "      <td>0.095890</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.542598</td>\n",
       "      <td>0.027545</td>\n",
       "      <td>0.027545</td>\n",
       "      <td>0.091081</td>\n",
       "      <td>0.091081</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20713</th>\n",
       "      <td>624003</td>\n",
       "      <td>0.996883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20714</th>\n",
       "      <td>624004</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019725</td>\n",
       "      <td>0.019725</td>\n",
       "      <td>0.656239</td>\n",
       "      <td>0.656239</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       log_id  absolute_sentence_position  sentence_length  \\\n",
       "20710  624000                    0.987534         0.027397   \n",
       "20711  624001                    0.990650         0.054795   \n",
       "20712  624002                    0.993767         0.095890   \n",
       "20713  624003                    0.996883         0.000000   \n",
       "20714  624004                    1.000000         0.013699   \n",
       "\n",
       "       number_of_special_terms  sentiment_score  mean_tf_idf  \\\n",
       "20710                      0.0         0.000000     0.010594   \n",
       "20711                      0.0         0.000000     0.019673   \n",
       "20712                      0.0         0.542598     0.027545   \n",
       "20713                      0.0         0.472649     0.000000   \n",
       "20714                      0.0         0.000000     0.019725   \n",
       "\n",
       "       normalized_mean_tf_idf  mean_tf_isf  normalized_mean_tf_isf  is_summary  \n",
       "20710                0.010594     0.292627                0.292627           0  \n",
       "20711                0.019673     0.132364                0.132364           0  \n",
       "20712                0.027545     0.091081                0.091081           0  \n",
       "20713                0.000000     0.000000                0.000000           0  \n",
       "20714                0.019725     0.656239                0.656239           0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data has been read in properly\n",
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16079, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.643762</td>\n",
       "      <td>0.019369</td>\n",
       "      <td>0.820857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085591</td>\n",
       "      <td>0.044454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099525</td>\n",
       "      <td>0.053817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089492</td>\n",
       "      <td>0.148677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>0.233131</td>\n",
       "      <td>0.056715</td>\n",
       "      <td>0.087906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   absolute_sentence_position  sentence_length  sentiment_score  \\\n",
       "0                    0.000000         0.013699         0.643762   \n",
       "1                    0.000436         0.178082         0.000000   \n",
       "2                    0.000872         0.136986         0.000000   \n",
       "3                    0.001308         0.068493         0.000000   \n",
       "4                    0.001744         0.109589         0.233131   \n",
       "\n",
       "   normalized_mean_tf_idf  normalized_mean_tf_isf  \n",
       "0                0.019369                0.820857  \n",
       "1                0.085591                0.044454  \n",
       "2                0.099525                0.053817  \n",
       "3                0.089492                0.148677  \n",
       "4                0.056715                0.087906  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a dataframe with all training data except the target column\n",
    "columns_to_drop = [\"log_id\", \"is_summary\", \"mean_tf_idf\", \"mean_tf_isf\", \"number_of_special_terms\"]\n",
    "# Keep only normalized columns\n",
    "train_X = train_df.drop(columns=columns_to_drop)\n",
    "validation_X = validation_df.drop(columns=columns_to_drop)\n",
    "test_X = test_df.drop(columns=columns_to_drop)\n",
    "\n",
    "assert train_X.shape[1] == test_X.shape[1] and test_X.shape[1] == validation_X.shape[1] \n",
    "#check that the target variable has been removed\n",
    "print(train_X.shape)\n",
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_columns = [str(num) for num in range(1, 151)]\n",
    "# train_X_no_vectors = train_X.drop(columns=vector_columns)\n",
    "# validation_X_no_vectors = validation_X.drop(columns=vector_columns)\n",
    "# test_X_no_vectors = test_X.drop(columns=vector_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_X_no_vectors.shape)\n",
    "# assert train_X_no_vectors.shape[1] == validation_X_no_vectors.shape[1] \n",
    "# assert validation_X_no_vectors.shape[1] == test_X_no_vectors.shape[1]\n",
    "# train_X_no_vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3988, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16079</th>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.472649</td>\n",
       "      <td>0.050304</td>\n",
       "      <td>0.051808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16080</th>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>0.899077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16081</th>\n",
       "      <td>0.002420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16082</th>\n",
       "      <td>0.003372</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040161</td>\n",
       "      <td>0.063344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16083</th>\n",
       "      <td>0.004324</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>0.472649</td>\n",
       "      <td>0.038080</td>\n",
       "      <td>0.131802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       absolute_sentence_position  sentence_length  sentiment_score  \\\n",
       "16079                    0.000516         0.150685         0.472649   \n",
       "16080                    0.001468         0.013699         0.000000   \n",
       "16081                    0.002420         0.000000         0.472649   \n",
       "16082                    0.003372         0.136986         0.000000   \n",
       "16083                    0.004324         0.054795         0.472649   \n",
       "\n",
       "       normalized_mean_tf_idf  normalized_mean_tf_isf  \n",
       "16079                0.050304                0.051808  \n",
       "16080                0.020250                0.899077  \n",
       "16081                0.000000                0.000000  \n",
       "16082                0.040161                0.063344  \n",
       "16083                0.038080                0.131802  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(validation_X.shape)\n",
    "validation_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(648, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20067</th>\n",
       "      <td>0.005740</td>\n",
       "      <td>0.232877</td>\n",
       "      <td>0.617493</td>\n",
       "      <td>0.014957</td>\n",
       "      <td>0.029834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20068</th>\n",
       "      <td>0.011915</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.304935</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.328720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20069</th>\n",
       "      <td>0.018091</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.304935</td>\n",
       "      <td>0.010781</td>\n",
       "      <td>0.191207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20070</th>\n",
       "      <td>0.024266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071</th>\n",
       "      <td>0.030442</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013585</td>\n",
       "      <td>0.283935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       absolute_sentence_position  sentence_length  sentiment_score  \\\n",
       "20067                    0.005740         0.232877         0.617493   \n",
       "20068                    0.011915         0.027397         0.304935   \n",
       "20069                    0.018091         0.041096         0.304935   \n",
       "20070                    0.024266         0.000000         0.453693   \n",
       "20071                    0.030442         0.027397         0.000000   \n",
       "\n",
       "       normalized_mean_tf_idf  normalized_mean_tf_isf  \n",
       "20067                0.014957                0.029834  \n",
       "20068                0.002195                0.328720  \n",
       "20069                0.010781                0.191207  \n",
       "20070                0.000000                0.000000  \n",
       "20071                0.013585                0.283935  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_X.shape)\n",
    "test_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.is_summary.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When separating the target column, we need to call the `to_categorical()` function so that column will be `one-hot encoded`. Currently, a chat line that is not a summary is represented with a `0` in the `is_summary` column and a chat line that is a summary is represented with a `1`. With one-hot encoding, the integer will be removed and a binary variable is inputted for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a one hot encoding for the target column\n",
    "train_y = to_categorical(train_df.is_summary)\n",
    "train_y_nums = train_df.is_summary.values\n",
    "validation_y = to_categorical(validation_df.is_summary)\n",
    "validation_y_nums = validation_df.is_summary.values\n",
    "test_y = to_categorical(test_df.is_summary)\n",
    "test_y_nums = test_df.is_summary.values\n",
    "# view one hot encoding numpy array\n",
    "print(train_y_nums)\n",
    "train_y[415: 425]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16079, 2)\n",
      "(3988, 2)\n",
      "(648, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_y.shape)\n",
    "print(validation_y.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation is `softmax`. Softmax makes the output sum up to `1` so the output can be interpreted as probabilities. The model will then make its prediction based on which option has a higher probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/wcyn/anaconda3/envs/gnue-irc/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#create model\n",
    "model = Sequential()\n",
    "\n",
    "#get number of columns in training data\n",
    "# n_cols = train_vectors_df.shape[1]\n",
    "n_cols = train_X.shape[1]\n",
    "\n",
    "#add model layers\n",
    "model.add(Dense(n_cols, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(300, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                300       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 5,532\n",
      "Trainable params: 5,532\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `categorical_crossentropy` for our loss function. This is the most common choice for classification. A lower score indicates that the model is performing better.\n",
    "\n",
    "We will use the ‘mean absolute error’ metric to see the score on the validation set at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model using mse as a measure of model performance\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set early stopping monitor so the model stops training when it won't improve anymore\n",
    "# early_stopping_monitor = EarlyStopping(patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/wcyn/anaconda3/envs/gnue-irc/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 16079 samples, validate on 3988 samples\n",
      "Epoch 1/1000\n",
      "16079/16079 [==============================] - 2s 119us/step - loss: 0.1579 - acc: 0.9676 - val_loss: 0.1290 - val_acc: 0.9709\n",
      "Epoch 2/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1332 - acc: 0.9684 - val_loss: 0.1333 - val_acc: 0.9709\n",
      "Epoch 3/1000\n",
      "16079/16079 [==============================] - 1s 65us/step - loss: 0.1333 - acc: 0.9684 - val_loss: 0.1323 - val_acc: 0.9709\n",
      "Epoch 4/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1334 - acc: 0.9684 - val_loss: 0.1281 - val_acc: 0.9709\n",
      "Epoch 5/1000\n",
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1327 - acc: 0.9684 - val_loss: 0.1283 - val_acc: 0.9709\n",
      "Epoch 6/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1327 - acc: 0.9684 - val_loss: 0.1290 - val_acc: 0.9709\n",
      "Epoch 7/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1322 - acc: 0.9684 - val_loss: 0.1286 - val_acc: 0.9709\n",
      "Epoch 8/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1318 - acc: 0.9684 - val_loss: 0.1286 - val_acc: 0.9709\n",
      "Epoch 9/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1323 - acc: 0.9684 - val_loss: 0.1313 - val_acc: 0.9709\n",
      "Epoch 10/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1318 - acc: 0.9684 - val_loss: 0.1292 - val_acc: 0.9709\n",
      "Epoch 11/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1316 - acc: 0.9684 - val_loss: 0.1276 - val_acc: 0.9709\n",
      "Epoch 12/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1313 - acc: 0.9684 - val_loss: 0.1313 - val_acc: 0.9709\n",
      "Epoch 13/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1310 - acc: 0.9684 - val_loss: 0.1310 - val_acc: 0.9709\n",
      "Epoch 14/1000\n",
      "16079/16079 [==============================] - 1s 62us/step - loss: 0.1314 - acc: 0.9684 - val_loss: 0.1295 - val_acc: 0.9709\n",
      "Epoch 15/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1312 - acc: 0.9684 - val_loss: 0.1294 - val_acc: 0.9709\n",
      "Epoch 16/1000\n",
      "16079/16079 [==============================] - 1s 64us/step - loss: 0.1308 - acc: 0.9684 - val_loss: 0.1300 - val_acc: 0.9709\n",
      "Epoch 17/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1309 - acc: 0.9684 - val_loss: 0.1298 - val_acc: 0.9709\n",
      "Epoch 18/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1306 - acc: 0.9684 - val_loss: 0.1297 - val_acc: 0.9709\n",
      "Epoch 19/1000\n",
      "16079/16079 [==============================] - 1s 62us/step - loss: 0.1308 - acc: 0.9684 - val_loss: 0.1298 - val_acc: 0.9709\n",
      "Epoch 20/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1308 - acc: 0.9684 - val_loss: 0.1295 - val_acc: 0.9709\n",
      "Epoch 21/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1306 - acc: 0.9684 - val_loss: 0.1312 - val_acc: 0.9709\n",
      "Epoch 22/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1303 - acc: 0.9684 - val_loss: 0.1310 - val_acc: 0.9709\n",
      "Epoch 23/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1303 - acc: 0.9684 - val_loss: 0.1297 - val_acc: 0.9709\n",
      "Epoch 24/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1307 - acc: 0.9684 - val_loss: 0.1296 - val_acc: 0.9709\n",
      "Epoch 25/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1306 - acc: 0.9684 - val_loss: 0.1297 - val_acc: 0.9709\n",
      "Epoch 26/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1305 - acc: 0.9684 - val_loss: 0.1295 - val_acc: 0.9709\n",
      "Epoch 27/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1302 - acc: 0.9684 - val_loss: 0.1294 - val_acc: 0.9709\n",
      "Epoch 28/1000\n",
      "16079/16079 [==============================] - 1s 59us/step - loss: 0.1300 - acc: 0.9684 - val_loss: 0.1299 - val_acc: 0.9709\n",
      "Epoch 29/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1303 - acc: 0.9684 - val_loss: 0.1292 - val_acc: 0.9709\n",
      "Epoch 30/1000\n",
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1300 - acc: 0.9684 - val_loss: 0.1303 - val_acc: 0.9709\n",
      "Epoch 31/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1301 - acc: 0.9684 - val_loss: 0.1308 - val_acc: 0.9709\n",
      "Epoch 32/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1302 - acc: 0.9684 - val_loss: 0.1292 - val_acc: 0.9709\n",
      "Epoch 33/1000\n",
      "16079/16079 [==============================] - 1s 64us/step - loss: 0.1307 - acc: 0.9684 - val_loss: 0.1287 - val_acc: 0.9709\n",
      "Epoch 34/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1303 - acc: 0.9684 - val_loss: 0.1295 - val_acc: 0.9709\n",
      "Epoch 35/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1303 - acc: 0.9684 - val_loss: 0.1308 - val_acc: 0.9709\n",
      "Epoch 36/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1298 - acc: 0.9684 - val_loss: 0.1310 - val_acc: 0.9709\n",
      "Epoch 37/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1299 - acc: 0.9684 - val_loss: 0.1293 - val_acc: 0.9709\n",
      "Epoch 38/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1298 - acc: 0.9684 - val_loss: 0.1303 - val_acc: 0.9709\n",
      "Epoch 39/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1297 - acc: 0.9684 - val_loss: 0.1311 - val_acc: 0.9709\n",
      "Epoch 40/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1301 - acc: 0.9684 - val_loss: 0.1305 - val_acc: 0.9709\n",
      "Epoch 41/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1298 - acc: 0.9684 - val_loss: 0.1313 - val_acc: 0.9709\n",
      "Epoch 42/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1296 - acc: 0.9684 - val_loss: 0.1292 - val_acc: 0.9709\n",
      "Epoch 43/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1299 - acc: 0.9684 - val_loss: 0.1301 - val_acc: 0.9709\n",
      "Epoch 44/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1297 - acc: 0.9684 - val_loss: 0.1304 - val_acc: 0.9709\n",
      "Epoch 45/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1298 - acc: 0.9684 - val_loss: 0.1297 - val_acc: 0.9709\n",
      "Epoch 46/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1295 - acc: 0.9684 - val_loss: 0.1297 - val_acc: 0.9709\n",
      "Epoch 47/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1296 - acc: 0.9684 - val_loss: 0.1303 - val_acc: 0.9709\n",
      "Epoch 48/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1296 - acc: 0.9684 - val_loss: 0.1306 - val_acc: 0.9709\n",
      "Epoch 49/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1296 - acc: 0.9684 - val_loss: 0.1314 - val_acc: 0.9709\n",
      "Epoch 50/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1297 - acc: 0.9684 - val_loss: 0.1326 - val_acc: 0.9709\n",
      "Epoch 51/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1295 - acc: 0.9684 - val_loss: 0.1304 - val_acc: 0.9709\n",
      "Epoch 52/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1298 - acc: 0.9684 - val_loss: 0.1297 - val_acc: 0.9709\n",
      "Epoch 53/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1301 - acc: 0.9684 - val_loss: 0.1307 - val_acc: 0.9709\n",
      "Epoch 54/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1294 - acc: 0.9684 - val_loss: 0.1290 - val_acc: 0.9709\n",
      "Epoch 55/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1291 - acc: 0.9684 - val_loss: 0.1292 - val_acc: 0.9709\n",
      "Epoch 56/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1291 - acc: 0.9684 - val_loss: 0.1307 - val_acc: 0.9709\n",
      "Epoch 57/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1293 - acc: 0.9684 - val_loss: 0.1300 - val_acc: 0.9709\n",
      "Epoch 58/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1296 - acc: 0.9684 - val_loss: 0.1294 - val_acc: 0.9709\n",
      "Epoch 59/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1294 - acc: 0.9684 - val_loss: 0.1303 - val_acc: 0.9709\n",
      "Epoch 60/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1294 - acc: 0.9684 - val_loss: 0.1300 - val_acc: 0.9709\n",
      "Epoch 61/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1294 - acc: 0.9684 - val_loss: 0.1300 - val_acc: 0.9709\n",
      "Epoch 62/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1292 - acc: 0.9684 - val_loss: 0.1301 - val_acc: 0.9709\n",
      "Epoch 63/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1291 - acc: 0.9684 - val_loss: 0.1296 - val_acc: 0.9709\n",
      "Epoch 64/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1291 - acc: 0.9684 - val_loss: 0.1298 - val_acc: 0.9709\n",
      "Epoch 65/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1290 - acc: 0.9684 - val_loss: 0.1308 - val_acc: 0.9709\n",
      "Epoch 66/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1290 - acc: 0.9684 - val_loss: 0.1307 - val_acc: 0.9709\n",
      "Epoch 67/1000\n",
      "16079/16079 [==============================] - 1s 66us/step - loss: 0.1292 - acc: 0.9684 - val_loss: 0.1314 - val_acc: 0.9709\n",
      "Epoch 68/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1289 - acc: 0.9684 - val_loss: 0.1300 - val_acc: 0.9709\n",
      "Epoch 69/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1290 - acc: 0.9684 - val_loss: 0.1300 - val_acc: 0.9709\n",
      "Epoch 70/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1285 - acc: 0.9684 - val_loss: 0.1321 - val_acc: 0.9709\n",
      "Epoch 71/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1292 - acc: 0.9684 - val_loss: 0.1294 - val_acc: 0.9709\n",
      "Epoch 72/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1286 - acc: 0.9684 - val_loss: 0.1306 - val_acc: 0.9709\n",
      "Epoch 73/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1286 - acc: 0.9684 - val_loss: 0.1310 - val_acc: 0.9709\n",
      "Epoch 74/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1286 - acc: 0.9684 - val_loss: 0.1331 - val_acc: 0.9709\n",
      "Epoch 75/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1288 - acc: 0.9684 - val_loss: 0.1314 - val_acc: 0.9709\n",
      "Epoch 76/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1287 - acc: 0.9684 - val_loss: 0.1313 - val_acc: 0.9709\n",
      "Epoch 77/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1288 - acc: 0.9684 - val_loss: 0.1300 - val_acc: 0.9709\n",
      "Epoch 78/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1282 - acc: 0.9684 - val_loss: 0.1300 - val_acc: 0.9709\n",
      "Epoch 79/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1288 - acc: 0.9684 - val_loss: 0.1304 - val_acc: 0.9709\n",
      "Epoch 80/1000\n",
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1288 - acc: 0.9684 - val_loss: 0.1306 - val_acc: 0.9709\n",
      "Epoch 81/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1284 - acc: 0.9684 - val_loss: 0.1319 - val_acc: 0.9709\n",
      "Epoch 82/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1282 - acc: 0.9684 - val_loss: 0.1316 - val_acc: 0.9709\n",
      "Epoch 83/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1286 - acc: 0.9684 - val_loss: 0.1313 - val_acc: 0.9709\n",
      "Epoch 84/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1284 - acc: 0.9684 - val_loss: 0.1311 - val_acc: 0.9709\n",
      "Epoch 85/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1286 - acc: 0.9684 - val_loss: 0.1311 - val_acc: 0.9709\n",
      "Epoch 86/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1283 - acc: 0.9684 - val_loss: 0.1319 - val_acc: 0.9709\n",
      "Epoch 87/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1283 - acc: 0.9684 - val_loss: 0.1332 - val_acc: 0.9709\n",
      "Epoch 88/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1283 - acc: 0.9684 - val_loss: 0.1322 - val_acc: 0.9709\n",
      "Epoch 89/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1282 - acc: 0.9684 - val_loss: 0.1324 - val_acc: 0.9709\n",
      "Epoch 90/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1280 - acc: 0.9684 - val_loss: 0.1312 - val_acc: 0.9709\n",
      "Epoch 91/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1283 - acc: 0.9684 - val_loss: 0.1329 - val_acc: 0.9709\n",
      "Epoch 92/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1279 - acc: 0.9684 - val_loss: 0.1305 - val_acc: 0.9709\n",
      "Epoch 93/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1280 - acc: 0.9684 - val_loss: 0.1328 - val_acc: 0.9709\n",
      "Epoch 94/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1282 - acc: 0.9684 - val_loss: 0.1302 - val_acc: 0.9709\n",
      "Epoch 95/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1284 - acc: 0.9684 - val_loss: 0.1321 - val_acc: 0.9709\n",
      "Epoch 96/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1281 - acc: 0.9684 - val_loss: 0.1304 - val_acc: 0.9709\n",
      "Epoch 97/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1279 - acc: 0.9684 - val_loss: 0.1302 - val_acc: 0.9709\n",
      "Epoch 98/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1280 - acc: 0.9684 - val_loss: 0.1320 - val_acc: 0.9709\n",
      "Epoch 99/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1278 - acc: 0.9684 - val_loss: 0.1312 - val_acc: 0.9709\n",
      "Epoch 100/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1281 - acc: 0.9684 - val_loss: 0.1325 - val_acc: 0.9709\n",
      "Epoch 101/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1278 - acc: 0.9684 - val_loss: 0.1313 - val_acc: 0.9709\n",
      "Epoch 102/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1278 - acc: 0.9684 - val_loss: 0.1311 - val_acc: 0.9709\n",
      "Epoch 103/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1273 - acc: 0.9684 - val_loss: 0.1358 - val_acc: 0.9709\n",
      "Epoch 104/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1278 - acc: 0.9684 - val_loss: 0.1309 - val_acc: 0.9709\n",
      "Epoch 105/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1272 - acc: 0.9684 - val_loss: 0.1393 - val_acc: 0.9709\n",
      "Epoch 106/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1279 - acc: 0.9684 - val_loss: 0.1309 - val_acc: 0.9709\n",
      "Epoch 107/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1274 - acc: 0.9684 - val_loss: 0.1321 - val_acc: 0.9709\n",
      "Epoch 108/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1274 - acc: 0.9684 - val_loss: 0.1304 - val_acc: 0.9709\n",
      "Epoch 109/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1278 - acc: 0.9684 - val_loss: 0.1320 - val_acc: 0.9709\n",
      "Epoch 110/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1275 - acc: 0.9684 - val_loss: 0.1355 - val_acc: 0.9709\n",
      "Epoch 111/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1277 - acc: 0.9684 - val_loss: 0.1338 - val_acc: 0.9709\n",
      "Epoch 112/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1276 - acc: 0.9684 - val_loss: 0.1312 - val_acc: 0.9709\n",
      "Epoch 113/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1275 - acc: 0.9684 - val_loss: 0.1342 - val_acc: 0.9709\n",
      "Epoch 114/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1270 - acc: 0.9684 - val_loss: 0.1298 - val_acc: 0.9709\n",
      "Epoch 115/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1274 - acc: 0.9684 - val_loss: 0.1330 - val_acc: 0.9709\n",
      "Epoch 116/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1274 - acc: 0.9684 - val_loss: 0.1324 - val_acc: 0.9709\n",
      "Epoch 117/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1274 - acc: 0.9684 - val_loss: 0.1311 - val_acc: 0.9709\n",
      "Epoch 118/1000\n",
      "16079/16079 [==============================] - 1s 63us/step - loss: 0.1272 - acc: 0.9684 - val_loss: 0.1341 - val_acc: 0.9709\n",
      "Epoch 119/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1273 - acc: 0.9684 - val_loss: 0.1330 - val_acc: 0.9709\n",
      "Epoch 120/1000\n",
      "16079/16079 [==============================] - 1s 66us/step - loss: 0.1272 - acc: 0.9684 - val_loss: 0.1318 - val_acc: 0.9709\n",
      "Epoch 121/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1271 - acc: 0.9684 - val_loss: 0.1309 - val_acc: 0.9709\n",
      "Epoch 122/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1269 - acc: 0.9684 - val_loss: 0.1321 - val_acc: 0.9709\n",
      "Epoch 123/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1268 - acc: 0.9684 - val_loss: 0.1378 - val_acc: 0.9709\n",
      "Epoch 124/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1272 - acc: 0.9684 - val_loss: 0.1328 - val_acc: 0.9709\n",
      "Epoch 125/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1266 - acc: 0.9684 - val_loss: 0.1306 - val_acc: 0.9709\n",
      "Epoch 126/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1268 - acc: 0.9684 - val_loss: 0.1346 - val_acc: 0.9709\n",
      "Epoch 127/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1266 - acc: 0.9684 - val_loss: 0.1345 - val_acc: 0.9709\n",
      "Epoch 128/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1269 - acc: 0.9684 - val_loss: 0.1335 - val_acc: 0.9709\n",
      "Epoch 129/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1270 - acc: 0.9684 - val_loss: 0.1322 - val_acc: 0.9709\n",
      "Epoch 130/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1264 - acc: 0.9684 - val_loss: 0.1312 - val_acc: 0.9709\n",
      "Epoch 131/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1271 - acc: 0.9684 - val_loss: 0.1305 - val_acc: 0.9709\n",
      "Epoch 132/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1264 - acc: 0.9684 - val_loss: 0.1302 - val_acc: 0.9709\n",
      "Epoch 133/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1271 - acc: 0.9684 - val_loss: 0.1350 - val_acc: 0.9709\n",
      "Epoch 134/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1268 - acc: 0.9684 - val_loss: 0.1327 - val_acc: 0.9709\n",
      "Epoch 135/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1267 - acc: 0.9684 - val_loss: 0.1322 - val_acc: 0.9709\n",
      "Epoch 136/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1264 - acc: 0.9684 - val_loss: 0.1341 - val_acc: 0.9709\n",
      "Epoch 137/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1263 - acc: 0.9684 - val_loss: 0.1348 - val_acc: 0.9709\n",
      "Epoch 138/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1262 - acc: 0.9684 - val_loss: 0.1343 - val_acc: 0.9709\n",
      "Epoch 139/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1262 - acc: 0.9684 - val_loss: 0.1357 - val_acc: 0.9709\n",
      "Epoch 140/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1262 - acc: 0.9684 - val_loss: 0.1340 - val_acc: 0.9709\n",
      "Epoch 141/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1260 - acc: 0.9684 - val_loss: 0.1349 - val_acc: 0.9709\n",
      "Epoch 142/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1264 - acc: 0.9684 - val_loss: 0.1333 - val_acc: 0.9709\n",
      "Epoch 143/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1263 - acc: 0.9684 - val_loss: 0.1345 - val_acc: 0.9709\n",
      "Epoch 144/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1258 - acc: 0.9684 - val_loss: 0.1408 - val_acc: 0.9709\n",
      "Epoch 145/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1266 - acc: 0.9684 - val_loss: 0.1353 - val_acc: 0.9709\n",
      "Epoch 146/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1261 - acc: 0.9684 - val_loss: 0.1338 - val_acc: 0.9709\n",
      "Epoch 147/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1259 - acc: 0.9684 - val_loss: 0.1334 - val_acc: 0.9709\n",
      "Epoch 148/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1260 - acc: 0.9684 - val_loss: 0.1316 - val_acc: 0.9709\n",
      "Epoch 149/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1261 - acc: 0.9684 - val_loss: 0.1330 - val_acc: 0.9709\n",
      "Epoch 150/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1262 - acc: 0.9684 - val_loss: 0.1333 - val_acc: 0.9709\n",
      "Epoch 151/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1260 - acc: 0.9684 - val_loss: 0.1366 - val_acc: 0.9709\n",
      "Epoch 152/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1260 - acc: 0.9684 - val_loss: 0.1338 - val_acc: 0.9709\n",
      "Epoch 153/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1258 - acc: 0.9684 - val_loss: 0.1336 - val_acc: 0.9709\n",
      "Epoch 154/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1258 - acc: 0.9684 - val_loss: 0.1386 - val_acc: 0.9709\n",
      "Epoch 155/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1257 - acc: 0.9684 - val_loss: 0.1334 - val_acc: 0.9709\n",
      "Epoch 156/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1259 - acc: 0.9684 - val_loss: 0.1318 - val_acc: 0.9709\n",
      "Epoch 157/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1256 - acc: 0.9684 - val_loss: 0.1355 - val_acc: 0.9709\n",
      "Epoch 158/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1255 - acc: 0.9684 - val_loss: 0.1356 - val_acc: 0.9709\n",
      "Epoch 159/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1258 - acc: 0.9684 - val_loss: 0.1388 - val_acc: 0.9709\n",
      "Epoch 160/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1261 - acc: 0.9684 - val_loss: 0.1318 - val_acc: 0.9709\n",
      "Epoch 161/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1254 - acc: 0.9684 - val_loss: 0.1347 - val_acc: 0.9709\n",
      "Epoch 162/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1255 - acc: 0.9684 - val_loss: 0.1342 - val_acc: 0.9709\n",
      "Epoch 163/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1257 - acc: 0.9684 - val_loss: 0.1333 - val_acc: 0.9709\n",
      "Epoch 164/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1254 - acc: 0.9684 - val_loss: 0.1418 - val_acc: 0.9709\n",
      "Epoch 165/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1253 - acc: 0.9684 - val_loss: 0.1403 - val_acc: 0.9709\n",
      "Epoch 166/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1255 - acc: 0.9684 - val_loss: 0.1344 - val_acc: 0.9709\n",
      "Epoch 167/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1253 - acc: 0.9684 - val_loss: 0.1360 - val_acc: 0.9709\n",
      "Epoch 168/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1260 - acc: 0.9684 - val_loss: 0.1360 - val_acc: 0.9709\n",
      "Epoch 169/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1254 - acc: 0.9684 - val_loss: 0.1354 - val_acc: 0.9709\n",
      "Epoch 170/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1252 - acc: 0.9684 - val_loss: 0.1432 - val_acc: 0.9709\n",
      "Epoch 171/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1251 - acc: 0.9684 - val_loss: 0.1371 - val_acc: 0.9709\n",
      "Epoch 172/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1252 - acc: 0.9684 - val_loss: 0.1349 - val_acc: 0.9709\n",
      "Epoch 173/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1250 - acc: 0.9684 - val_loss: 0.1344 - val_acc: 0.9709\n",
      "Epoch 174/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1255 - acc: 0.9684 - val_loss: 0.1334 - val_acc: 0.9709\n",
      "Epoch 175/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1256 - acc: 0.9684 - val_loss: 0.1353 - val_acc: 0.9709\n",
      "Epoch 176/1000\n",
      "16079/16079 [==============================] - 1s 61us/step - loss: 0.1251 - acc: 0.9684 - val_loss: 0.1357 - val_acc: 0.9709\n",
      "Epoch 177/1000\n",
      "16079/16079 [==============================] - 1s 66us/step - loss: 0.1253 - acc: 0.9684 - val_loss: 0.1378 - val_acc: 0.9709\n",
      "Epoch 178/1000\n",
      "16079/16079 [==============================] - 1s 65us/step - loss: 0.1253 - acc: 0.9684 - val_loss: 0.1405 - val_acc: 0.9709\n",
      "Epoch 179/1000\n",
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1250 - acc: 0.9684 - val_loss: 0.1373 - val_acc: 0.9709\n",
      "Epoch 180/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1255 - acc: 0.9684 - val_loss: 0.1313 - val_acc: 0.9709\n",
      "Epoch 181/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1251 - acc: 0.9684 - val_loss: 0.1351 - val_acc: 0.9709\n",
      "Epoch 182/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1252 - acc: 0.9684 - val_loss: 0.1372 - val_acc: 0.9709\n",
      "Epoch 183/1000\n",
      "16079/16079 [==============================] - 1s 65us/step - loss: 0.1249 - acc: 0.9684 - val_loss: 0.1334 - val_acc: 0.9709\n",
      "Epoch 184/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1249 - acc: 0.9684 - val_loss: 0.1338 - val_acc: 0.9709\n",
      "Epoch 185/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1245 - acc: 0.9684 - val_loss: 0.1349 - val_acc: 0.9709\n",
      "Epoch 186/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1253 - acc: 0.9684 - val_loss: 0.1343 - val_acc: 0.9709\n",
      "Epoch 187/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1251 - acc: 0.9684 - val_loss: 0.1346 - val_acc: 0.9709\n",
      "Epoch 188/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1247 - acc: 0.9684 - val_loss: 0.1401 - val_acc: 0.9709\n",
      "Epoch 189/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1249 - acc: 0.9684 - val_loss: 0.1359 - val_acc: 0.9709\n",
      "Epoch 190/1000\n",
      "16079/16079 [==============================] - 1s 64us/step - loss: 0.1249 - acc: 0.9684 - val_loss: 0.1363 - val_acc: 0.9709\n",
      "Epoch 191/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1252 - acc: 0.9684 - val_loss: 0.1362 - val_acc: 0.9709\n",
      "Epoch 192/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1252 - acc: 0.9684 - val_loss: 0.1373 - val_acc: 0.9709\n",
      "Epoch 193/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1248 - acc: 0.9684 - val_loss: 0.1356 - val_acc: 0.9709\n",
      "Epoch 194/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1249 - acc: 0.9684 - val_loss: 0.1371 - val_acc: 0.9709\n",
      "Epoch 195/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1247 - acc: 0.9684 - val_loss: 0.1366 - val_acc: 0.9709\n",
      "Epoch 196/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1250 - acc: 0.9684 - val_loss: 0.1362 - val_acc: 0.9709\n",
      "Epoch 197/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1249 - acc: 0.9684 - val_loss: 0.1329 - val_acc: 0.9709\n",
      "Epoch 198/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1244 - acc: 0.9684 - val_loss: 0.1383 - val_acc: 0.9709\n",
      "Epoch 199/1000\n",
      "16079/16079 [==============================] - 1s 66us/step - loss: 0.1245 - acc: 0.9684 - val_loss: 0.1349 - val_acc: 0.9709\n",
      "Epoch 200/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1244 - acc: 0.9684 - val_loss: 0.1412 - val_acc: 0.9709\n",
      "Epoch 201/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1245 - acc: 0.9684 - val_loss: 0.1396 - val_acc: 0.9709\n",
      "Epoch 202/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1245 - acc: 0.9684 - val_loss: 0.1383 - val_acc: 0.9709\n",
      "Epoch 203/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1252 - acc: 0.9684 - val_loss: 0.1394 - val_acc: 0.9709\n",
      "Epoch 204/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1242 - acc: 0.9684 - val_loss: 0.1363 - val_acc: 0.9709\n",
      "Epoch 205/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1252 - acc: 0.9684 - val_loss: 0.1395 - val_acc: 0.9709\n",
      "Epoch 206/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1241 - acc: 0.9684 - val_loss: 0.1364 - val_acc: 0.9709\n",
      "Epoch 207/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1242 - acc: 0.9684 - val_loss: 0.1409 - val_acc: 0.9709\n",
      "Epoch 208/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1246 - acc: 0.9684 - val_loss: 0.1392 - val_acc: 0.9709\n",
      "Epoch 209/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1245 - acc: 0.9684 - val_loss: 0.1377 - val_acc: 0.9709\n",
      "Epoch 210/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1240 - acc: 0.9684 - val_loss: 0.1427 - val_acc: 0.9709\n",
      "Epoch 211/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1242 - acc: 0.9684 - val_loss: 0.1430 - val_acc: 0.9709\n",
      "Epoch 212/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1244 - acc: 0.9684 - val_loss: 0.1387 - val_acc: 0.9709\n",
      "Epoch 213/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1241 - acc: 0.9684 - val_loss: 0.1383 - val_acc: 0.9709\n",
      "Epoch 214/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1240 - acc: 0.9684 - val_loss: 0.1405 - val_acc: 0.9709\n",
      "Epoch 215/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1239 - acc: 0.9684 - val_loss: 0.1396 - val_acc: 0.9709\n",
      "Epoch 216/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1242 - acc: 0.9684 - val_loss: 0.1416 - val_acc: 0.9709\n",
      "Epoch 217/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1241 - acc: 0.9684 - val_loss: 0.1422 - val_acc: 0.9709\n",
      "Epoch 218/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1243 - acc: 0.9684 - val_loss: 0.1392 - val_acc: 0.9709\n",
      "Epoch 219/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1244 - acc: 0.9684 - val_loss: 0.1445 - val_acc: 0.9709\n",
      "Epoch 220/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1238 - acc: 0.9684 - val_loss: 0.1418 - val_acc: 0.9709\n",
      "Epoch 221/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1245 - acc: 0.9684 - val_loss: 0.1454 - val_acc: 0.9709\n",
      "Epoch 222/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1236 - acc: 0.9684 - val_loss: 0.1377 - val_acc: 0.9709\n",
      "Epoch 223/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1244 - acc: 0.9684 - val_loss: 0.1406 - val_acc: 0.9709\n",
      "Epoch 224/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1239 - acc: 0.9684 - val_loss: 0.1403 - val_acc: 0.9709\n",
      "Epoch 225/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1241 - acc: 0.9684 - val_loss: 0.1415 - val_acc: 0.9709\n",
      "Epoch 226/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1243 - acc: 0.9684 - val_loss: 0.1434 - val_acc: 0.9709\n",
      "Epoch 227/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1241 - acc: 0.9684 - val_loss: 0.1389 - val_acc: 0.9709\n",
      "Epoch 228/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1242 - acc: 0.9684 - val_loss: 0.1390 - val_acc: 0.9709\n",
      "Epoch 229/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1230 - acc: 0.9684 - val_loss: 0.1415 - val_acc: 0.9709\n",
      "Epoch 230/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1247 - acc: 0.9684 - val_loss: 0.1426 - val_acc: 0.9709\n",
      "Epoch 231/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1241 - acc: 0.9684 - val_loss: 0.1410 - val_acc: 0.9709\n",
      "Epoch 232/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1240 - acc: 0.9684 - val_loss: 0.1431 - val_acc: 0.9709\n",
      "Epoch 233/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1241 - acc: 0.9684 - val_loss: 0.1412 - val_acc: 0.9709\n",
      "Epoch 234/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1245 - acc: 0.9684 - val_loss: 0.1413 - val_acc: 0.9709\n",
      "Epoch 235/1000\n",
      "16079/16079 [==============================] - 1s 63us/step - loss: 0.1235 - acc: 0.9684 - val_loss: 0.1393 - val_acc: 0.9709\n",
      "Epoch 236/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1241 - acc: 0.9684 - val_loss: 0.1411 - val_acc: 0.9709\n",
      "Epoch 237/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1237 - acc: 0.9684 - val_loss: 0.1376 - val_acc: 0.9709\n",
      "Epoch 238/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1237 - acc: 0.9684 - val_loss: 0.1425 - val_acc: 0.9709\n",
      "Epoch 239/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1238 - acc: 0.9684 - val_loss: 0.1439 - val_acc: 0.9709\n",
      "Epoch 240/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1236 - acc: 0.9684 - val_loss: 0.1414 - val_acc: 0.9709\n",
      "Epoch 241/1000\n",
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1235 - acc: 0.9684 - val_loss: 0.1422 - val_acc: 0.9709\n",
      "Epoch 242/1000\n",
      "16079/16079 [==============================] - 1s 66us/step - loss: 0.1235 - acc: 0.9684 - val_loss: 0.1394 - val_acc: 0.9709\n",
      "Epoch 243/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1237 - acc: 0.9684 - val_loss: 0.1477 - val_acc: 0.9709\n",
      "Epoch 244/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1241 - acc: 0.9684 - val_loss: 0.1417 - val_acc: 0.9709\n",
      "Epoch 245/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1233 - acc: 0.9684 - val_loss: 0.1434 - val_acc: 0.9709\n",
      "Epoch 246/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1236 - acc: 0.9684 - val_loss: 0.1389 - val_acc: 0.9709\n",
      "Epoch 247/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1233 - acc: 0.9684 - val_loss: 0.1458 - val_acc: 0.9709\n",
      "Epoch 248/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1236 - acc: 0.9684 - val_loss: 0.1438 - val_acc: 0.9709\n",
      "Epoch 249/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1240 - acc: 0.9684 - val_loss: 0.1358 - val_acc: 0.9709\n",
      "Epoch 250/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1237 - acc: 0.9684 - val_loss: 0.1416 - val_acc: 0.9709\n",
      "Epoch 251/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1233 - acc: 0.9684 - val_loss: 0.1387 - val_acc: 0.9709\n",
      "Epoch 252/1000\n",
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1232 - acc: 0.9684 - val_loss: 0.1417 - val_acc: 0.9709\n",
      "Epoch 253/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1232 - acc: 0.9684 - val_loss: 0.1440 - val_acc: 0.9709\n",
      "Epoch 254/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1231 - acc: 0.9684 - val_loss: 0.1350 - val_acc: 0.9709\n",
      "Epoch 255/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1234 - acc: 0.9684 - val_loss: 0.1458 - val_acc: 0.9709\n",
      "Epoch 256/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1236 - acc: 0.9684 - val_loss: 0.1411 - val_acc: 0.9709\n",
      "Epoch 257/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1236 - acc: 0.9684 - val_loss: 0.1429 - val_acc: 0.9709\n",
      "Epoch 258/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1239 - acc: 0.9684 - val_loss: 0.1357 - val_acc: 0.9709\n",
      "Epoch 259/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1227 - acc: 0.9684 - val_loss: 0.1352 - val_acc: 0.9709\n",
      "Epoch 260/1000\n",
      "16079/16079 [==============================] - 1s 66us/step - loss: 0.1230 - acc: 0.9684 - val_loss: 0.1413 - val_acc: 0.9709\n",
      "Epoch 261/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1238 - acc: 0.9684 - val_loss: 0.1389 - val_acc: 0.9709\n",
      "Epoch 262/1000\n",
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1229 - acc: 0.9684 - val_loss: 0.1440 - val_acc: 0.9709\n",
      "Epoch 263/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1231 - acc: 0.9684 - val_loss: 0.1426 - val_acc: 0.9709\n",
      "Epoch 264/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1233 - acc: 0.9684 - val_loss: 0.1369 - val_acc: 0.9709\n",
      "Epoch 265/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1230 - acc: 0.9684 - val_loss: 0.1440 - val_acc: 0.9709\n",
      "Epoch 266/1000\n",
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1228 - acc: 0.9684 - val_loss: 0.1473 - val_acc: 0.9709\n",
      "Epoch 267/1000\n",
      "16079/16079 [==============================] - 1s 65us/step - loss: 0.1230 - acc: 0.9684 - val_loss: 0.1431 - val_acc: 0.9709\n",
      "Epoch 268/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1236 - acc: 0.9684 - val_loss: 0.1404 - val_acc: 0.9709\n",
      "Epoch 269/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1228 - acc: 0.9684 - val_loss: 0.1395 - val_acc: 0.9709\n",
      "Epoch 270/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1224 - acc: 0.9684 - val_loss: 0.1437 - val_acc: 0.9709\n",
      "Epoch 271/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1223 - acc: 0.9684 - val_loss: 0.1357 - val_acc: 0.9709\n",
      "Epoch 272/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1228 - acc: 0.9684 - val_loss: 0.1381 - val_acc: 0.9709\n",
      "Epoch 273/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1226 - acc: 0.9684 - val_loss: 0.1400 - val_acc: 0.9709\n",
      "Epoch 274/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1231 - acc: 0.9684 - val_loss: 0.1510 - val_acc: 0.9709\n",
      "Epoch 275/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1227 - acc: 0.9684 - val_loss: 0.1450 - val_acc: 0.9709\n",
      "Epoch 276/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1224 - acc: 0.9684 - val_loss: 0.1456 - val_acc: 0.9709\n",
      "Epoch 277/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1225 - acc: 0.9684 - val_loss: 0.1439 - val_acc: 0.9709\n",
      "Epoch 278/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1222 - acc: 0.9684 - val_loss: 0.1426 - val_acc: 0.9709\n",
      "Epoch 279/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1227 - acc: 0.9684 - val_loss: 0.1455 - val_acc: 0.9709\n",
      "Epoch 280/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1223 - acc: 0.9684 - val_loss: 0.1491 - val_acc: 0.9709\n",
      "Epoch 281/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1224 - acc: 0.9684 - val_loss: 0.1449 - val_acc: 0.9709\n",
      "Epoch 282/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1228 - acc: 0.9684 - val_loss: 0.1401 - val_acc: 0.9709\n",
      "Epoch 283/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1223 - acc: 0.9684 - val_loss: 0.1416 - val_acc: 0.9709\n",
      "Epoch 284/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1225 - acc: 0.9684 - val_loss: 0.1399 - val_acc: 0.9709\n",
      "Epoch 285/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1226 - acc: 0.9684 - val_loss: 0.1410 - val_acc: 0.9709\n",
      "Epoch 286/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1222 - acc: 0.9684 - val_loss: 0.1446 - val_acc: 0.9709\n",
      "Epoch 287/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1220 - acc: 0.9684 - val_loss: 0.1518 - val_acc: 0.9709\n",
      "Epoch 288/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1226 - acc: 0.9684 - val_loss: 0.1503 - val_acc: 0.9709\n",
      "Epoch 289/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1225 - acc: 0.9684 - val_loss: 0.1393 - val_acc: 0.9709\n",
      "Epoch 290/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1230 - acc: 0.9684 - val_loss: 0.1401 - val_acc: 0.9709\n",
      "Epoch 291/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1220 - acc: 0.9684 - val_loss: 0.1468 - val_acc: 0.9709\n",
      "Epoch 292/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1224 - acc: 0.9684 - val_loss: 0.1472 - val_acc: 0.9709\n",
      "Epoch 293/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1221 - acc: 0.9684 - val_loss: 0.1454 - val_acc: 0.9709\n",
      "Epoch 294/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1224 - acc: 0.9684 - val_loss: 0.1424 - val_acc: 0.9709\n",
      "Epoch 295/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1225 - acc: 0.9684 - val_loss: 0.1383 - val_acc: 0.9709\n",
      "Epoch 296/1000\n",
      "16079/16079 [==============================] - 1s 66us/step - loss: 0.1224 - acc: 0.9684 - val_loss: 0.1373 - val_acc: 0.9709\n",
      "Epoch 297/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1222 - acc: 0.9684 - val_loss: 0.1393 - val_acc: 0.9709\n",
      "Epoch 298/1000\n",
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1214 - acc: 0.9684 - val_loss: 0.1419 - val_acc: 0.9709\n",
      "Epoch 299/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1219 - acc: 0.9684 - val_loss: 0.1481 - val_acc: 0.9709\n",
      "Epoch 300/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1222 - acc: 0.9684 - val_loss: 0.1466 - val_acc: 0.9709\n",
      "Epoch 301/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1218 - acc: 0.9684 - val_loss: 0.1442 - val_acc: 0.9709\n",
      "Epoch 302/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1225 - acc: 0.9684 - val_loss: 0.1434 - val_acc: 0.9709\n",
      "Epoch 303/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1215 - acc: 0.9684 - val_loss: 0.1452 - val_acc: 0.9709\n",
      "Epoch 304/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1216 - acc: 0.9684 - val_loss: 0.1438 - val_acc: 0.9709\n",
      "Epoch 305/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1218 - acc: 0.9684 - val_loss: 0.1399 - val_acc: 0.9709\n",
      "Epoch 306/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1220 - acc: 0.9684 - val_loss: 0.1449 - val_acc: 0.9709\n",
      "Epoch 307/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1212 - acc: 0.9684 - val_loss: 0.1530 - val_acc: 0.9709\n",
      "Epoch 308/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1218 - acc: 0.9684 - val_loss: 0.1430 - val_acc: 0.9709\n",
      "Epoch 309/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1214 - acc: 0.9684 - val_loss: 0.1473 - val_acc: 0.9709\n",
      "Epoch 310/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1213 - acc: 0.9684 - val_loss: 0.1484 - val_acc: 0.9709\n",
      "Epoch 311/1000\n",
      "16079/16079 [==============================] - 1s 64us/step - loss: 0.1215 - acc: 0.9684 - val_loss: 0.1588 - val_acc: 0.9709\n",
      "Epoch 312/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1214 - acc: 0.9684 - val_loss: 0.1563 - val_acc: 0.9709\n",
      "Epoch 313/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1212 - acc: 0.9684 - val_loss: 0.1495 - val_acc: 0.9709\n",
      "Epoch 314/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1218 - acc: 0.9684 - val_loss: 0.1459 - val_acc: 0.9709\n",
      "Epoch 315/1000\n",
      "16079/16079 [==============================] - 1s 62us/step - loss: 0.1215 - acc: 0.9684 - val_loss: 0.1458 - val_acc: 0.9709\n",
      "Epoch 316/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1213 - acc: 0.9684 - val_loss: 0.1413 - val_acc: 0.9709\n",
      "Epoch 317/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1212 - acc: 0.9684 - val_loss: 0.1452 - val_acc: 0.9709\n",
      "Epoch 318/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1216 - acc: 0.9684 - val_loss: 0.1523 - val_acc: 0.9709\n",
      "Epoch 319/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1215 - acc: 0.9684 - val_loss: 0.1478 - val_acc: 0.9709\n",
      "Epoch 320/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1210 - acc: 0.9684 - val_loss: 0.1464 - val_acc: 0.9709\n",
      "Epoch 321/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1211 - acc: 0.9684 - val_loss: 0.1437 - val_acc: 0.9709\n",
      "Epoch 322/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1223 - acc: 0.9684 - val_loss: 0.1452 - val_acc: 0.9709\n",
      "Epoch 323/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1221 - acc: 0.9684 - val_loss: 0.1382 - val_acc: 0.9709\n",
      "Epoch 324/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1215 - acc: 0.9684 - val_loss: 0.1419 - val_acc: 0.9709\n",
      "Epoch 325/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1212 - acc: 0.9684 - val_loss: 0.1474 - val_acc: 0.9709\n",
      "Epoch 326/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1207 - acc: 0.9684 - val_loss: 0.1502 - val_acc: 0.9709\n",
      "Epoch 327/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1208 - acc: 0.9684 - val_loss: 0.1467 - val_acc: 0.9709\n",
      "Epoch 328/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1213 - acc: 0.9684 - val_loss: 0.1417 - val_acc: 0.9709\n",
      "Epoch 329/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1208 - acc: 0.9684 - val_loss: 0.1410 - val_acc: 0.9709\n",
      "Epoch 330/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1208 - acc: 0.9684 - val_loss: 0.1392 - val_acc: 0.9709\n",
      "Epoch 331/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1214 - acc: 0.9684 - val_loss: 0.1482 - val_acc: 0.9709\n",
      "Epoch 332/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1209 - acc: 0.9684 - val_loss: 0.1538 - val_acc: 0.9709\n",
      "Epoch 333/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1208 - acc: 0.9684 - val_loss: 0.1457 - val_acc: 0.9709\n",
      "Epoch 334/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1217 - acc: 0.9684 - val_loss: 0.1515 - val_acc: 0.9709\n",
      "Epoch 335/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1205 - acc: 0.9684 - val_loss: 0.1435 - val_acc: 0.9709\n",
      "Epoch 336/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1207 - acc: 0.9684 - val_loss: 0.1478 - val_acc: 0.9709\n",
      "Epoch 337/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1208 - acc: 0.9684 - val_loss: 0.1461 - val_acc: 0.9709\n",
      "Epoch 338/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1201 - acc: 0.9684 - val_loss: 0.1567 - val_acc: 0.9709\n",
      "Epoch 339/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1211 - acc: 0.9684 - val_loss: 0.1436 - val_acc: 0.9709\n",
      "Epoch 340/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1211 - acc: 0.9685 - val_loss: 0.1430 - val_acc: 0.9707\n",
      "Epoch 341/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1204 - acc: 0.9685 - val_loss: 0.1536 - val_acc: 0.9709\n",
      "Epoch 342/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1203 - acc: 0.9685 - val_loss: 0.1446 - val_acc: 0.9707\n",
      "Epoch 343/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1206 - acc: 0.9684 - val_loss: 0.1426 - val_acc: 0.9707\n",
      "Epoch 344/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1207 - acc: 0.9685 - val_loss: 0.1543 - val_acc: 0.9704\n",
      "Epoch 345/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1201 - acc: 0.9684 - val_loss: 0.1499 - val_acc: 0.9707\n",
      "Epoch 346/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1202 - acc: 0.9685 - val_loss: 0.1466 - val_acc: 0.9707\n",
      "Epoch 347/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1205 - acc: 0.9684 - val_loss: 0.1605 - val_acc: 0.9707\n",
      "Epoch 348/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1203 - acc: 0.9685 - val_loss: 0.1571 - val_acc: 0.9707\n",
      "Epoch 349/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1217 - acc: 0.9684 - val_loss: 0.1427 - val_acc: 0.9707\n",
      "Epoch 350/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1205 - acc: 0.9685 - val_loss: 0.1498 - val_acc: 0.9707\n",
      "Epoch 351/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1209 - acc: 0.9685 - val_loss: 0.1466 - val_acc: 0.9704\n",
      "Epoch 352/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1202 - acc: 0.9683 - val_loss: 0.1512 - val_acc: 0.9704\n",
      "Epoch 353/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1206 - acc: 0.9685 - val_loss: 0.1495 - val_acc: 0.9707\n",
      "Epoch 354/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1209 - acc: 0.9685 - val_loss: 0.1538 - val_acc: 0.9704\n",
      "Epoch 355/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1203 - acc: 0.9685 - val_loss: 0.1544 - val_acc: 0.9707\n",
      "Epoch 356/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1204 - acc: 0.9685 - val_loss: 0.1540 - val_acc: 0.9704\n",
      "Epoch 357/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1192 - acc: 0.9685 - val_loss: 0.1412 - val_acc: 0.9704\n",
      "Epoch 358/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1208 - acc: 0.9685 - val_loss: 0.1442 - val_acc: 0.9707\n",
      "Epoch 359/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1206 - acc: 0.9684 - val_loss: 0.1479 - val_acc: 0.9702\n",
      "Epoch 360/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1199 - acc: 0.9685 - val_loss: 0.1634 - val_acc: 0.9704\n",
      "Epoch 361/1000\n",
      "16079/16079 [==============================] - 1s 64us/step - loss: 0.1206 - acc: 0.9685 - val_loss: 0.1516 - val_acc: 0.9702\n",
      "Epoch 362/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1202 - acc: 0.9685 - val_loss: 0.1538 - val_acc: 0.9707\n",
      "Epoch 363/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1198 - acc: 0.9685 - val_loss: 0.1522 - val_acc: 0.9702\n",
      "Epoch 364/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1199 - acc: 0.9685 - val_loss: 0.1466 - val_acc: 0.9704\n",
      "Epoch 365/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1194 - acc: 0.9686 - val_loss: 0.1526 - val_acc: 0.9707\n",
      "Epoch 366/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1197 - acc: 0.9685 - val_loss: 0.1543 - val_acc: 0.9702\n",
      "Epoch 367/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1204 - acc: 0.9685 - val_loss: 0.1558 - val_acc: 0.9707\n",
      "Epoch 368/1000\n",
      "16079/16079 [==============================] - 1s 64us/step - loss: 0.1190 - acc: 0.9685 - val_loss: 0.1568 - val_acc: 0.9707\n",
      "Epoch 369/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1203 - acc: 0.9686 - val_loss: 0.1508 - val_acc: 0.9707\n",
      "Epoch 370/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1191 - acc: 0.9686 - val_loss: 0.1602 - val_acc: 0.9704\n",
      "Epoch 371/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1205 - acc: 0.9685 - val_loss: 0.1375 - val_acc: 0.9704\n",
      "Epoch 372/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1194 - acc: 0.9686 - val_loss: 0.1622 - val_acc: 0.9702\n",
      "Epoch 373/1000\n",
      "16079/16079 [==============================] - 1s 64us/step - loss: 0.1197 - acc: 0.9686 - val_loss: 0.1455 - val_acc: 0.9709\n",
      "Epoch 374/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1210 - acc: 0.9685 - val_loss: 0.1625 - val_acc: 0.9704\n",
      "Epoch 375/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1195 - acc: 0.9687 - val_loss: 0.1511 - val_acc: 0.9707\n",
      "Epoch 376/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1197 - acc: 0.9685 - val_loss: 0.1502 - val_acc: 0.9702\n",
      "Epoch 377/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1197 - acc: 0.9686 - val_loss: 0.1512 - val_acc: 0.9704\n",
      "Epoch 378/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1197 - acc: 0.9685 - val_loss: 0.1509 - val_acc: 0.9702\n",
      "Epoch 379/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1194 - acc: 0.9686 - val_loss: 0.1520 - val_acc: 0.9704\n",
      "Epoch 380/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1198 - acc: 0.9686 - val_loss: 0.1489 - val_acc: 0.9704\n",
      "Epoch 381/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1192 - acc: 0.9687 - val_loss: 0.1528 - val_acc: 0.9702\n",
      "Epoch 382/1000\n",
      "16079/16079 [==============================] - 1s 66us/step - loss: 0.1196 - acc: 0.9687 - val_loss: 0.1573 - val_acc: 0.9697\n",
      "Epoch 383/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1206 - acc: 0.9683 - val_loss: 0.1526 - val_acc: 0.9702\n",
      "Epoch 384/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1194 - acc: 0.9684 - val_loss: 0.1394 - val_acc: 0.9704\n",
      "Epoch 385/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1193 - acc: 0.9685 - val_loss: 0.1525 - val_acc: 0.9704\n",
      "Epoch 386/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1192 - acc: 0.9685 - val_loss: 0.1543 - val_acc: 0.9699\n",
      "Epoch 387/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1188 - acc: 0.9686 - val_loss: 0.1554 - val_acc: 0.9702\n",
      "Epoch 388/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1185 - acc: 0.9686 - val_loss: 0.1448 - val_acc: 0.9702\n",
      "Epoch 389/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1189 - acc: 0.9686 - val_loss: 0.1580 - val_acc: 0.9704\n",
      "Epoch 390/1000\n",
      "16079/16079 [==============================] - 1s 62us/step - loss: 0.1198 - acc: 0.9683 - val_loss: 0.1452 - val_acc: 0.9704\n",
      "Epoch 391/1000\n",
      "16079/16079 [==============================] - 1s 58us/step - loss: 0.1194 - acc: 0.9683 - val_loss: 0.1499 - val_acc: 0.9702\n",
      "Epoch 392/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1182 - acc: 0.9686 - val_loss: 0.1537 - val_acc: 0.9704\n",
      "Epoch 393/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1200 - acc: 0.9687 - val_loss: 0.1527 - val_acc: 0.9699\n",
      "Epoch 394/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1190 - acc: 0.9684 - val_loss: 0.1557 - val_acc: 0.9707\n",
      "Epoch 395/1000\n",
      "16079/16079 [==============================] - 1s 66us/step - loss: 0.1191 - acc: 0.9683 - val_loss: 0.1584 - val_acc: 0.9702\n",
      "Epoch 396/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1191 - acc: 0.9686 - val_loss: 0.1513 - val_acc: 0.9704\n",
      "Epoch 397/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1187 - acc: 0.9687 - val_loss: 0.1532 - val_acc: 0.9704\n",
      "Epoch 398/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1182 - acc: 0.9686 - val_loss: 0.1557 - val_acc: 0.9704\n",
      "Epoch 399/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1192 - acc: 0.9684 - val_loss: 0.1582 - val_acc: 0.9699\n",
      "Epoch 400/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1186 - acc: 0.9686 - val_loss: 0.1620 - val_acc: 0.9699\n",
      "Epoch 401/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1187 - acc: 0.9685 - val_loss: 0.1609 - val_acc: 0.9697\n",
      "Epoch 402/1000\n",
      "16079/16079 [==============================] - 1s 66us/step - loss: 0.1189 - acc: 0.9685 - val_loss: 0.1540 - val_acc: 0.9702\n",
      "Epoch 403/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1183 - acc: 0.9687 - val_loss: 0.1588 - val_acc: 0.9702\n",
      "Epoch 404/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1197 - acc: 0.9685 - val_loss: 0.1500 - val_acc: 0.9702\n",
      "Epoch 405/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1187 - acc: 0.9687 - val_loss: 0.1543 - val_acc: 0.9702\n",
      "Epoch 406/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1182 - acc: 0.9685 - val_loss: 0.1584 - val_acc: 0.9697\n",
      "Epoch 407/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1188 - acc: 0.9685 - val_loss: 0.1517 - val_acc: 0.9699\n",
      "Epoch 408/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1186 - acc: 0.9687 - val_loss: 0.1512 - val_acc: 0.9699\n",
      "Epoch 409/1000\n",
      "16079/16079 [==============================] - 1s 62us/step - loss: 0.1185 - acc: 0.9687 - val_loss: 0.1564 - val_acc: 0.9704\n",
      "Epoch 410/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1184 - acc: 0.9685 - val_loss: 0.1575 - val_acc: 0.9704\n",
      "Epoch 411/1000\n",
      "16079/16079 [==============================] - 1s 66us/step - loss: 0.1187 - acc: 0.9686 - val_loss: 0.1516 - val_acc: 0.9704\n",
      "Epoch 412/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1191 - acc: 0.9685 - val_loss: 0.1590 - val_acc: 0.9704\n",
      "Epoch 413/1000\n",
      "16079/16079 [==============================] - 1s 64us/step - loss: 0.1189 - acc: 0.9687 - val_loss: 0.1539 - val_acc: 0.9699\n",
      "Epoch 414/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1183 - acc: 0.9686 - val_loss: 0.1504 - val_acc: 0.9704\n",
      "Epoch 415/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1190 - acc: 0.9688 - val_loss: 0.1589 - val_acc: 0.9702\n",
      "Epoch 416/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1181 - acc: 0.9687 - val_loss: 0.1599 - val_acc: 0.9702\n",
      "Epoch 417/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1192 - acc: 0.9686 - val_loss: 0.1550 - val_acc: 0.9704\n",
      "Epoch 418/1000\n",
      "16079/16079 [==============================] - 1s 61us/step - loss: 0.1179 - acc: 0.9685 - val_loss: 0.1538 - val_acc: 0.9702\n",
      "Epoch 419/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1182 - acc: 0.9685 - val_loss: 0.1420 - val_acc: 0.9704\n",
      "Epoch 420/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1195 - acc: 0.9686 - val_loss: 0.1519 - val_acc: 0.9702\n",
      "Epoch 421/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1184 - acc: 0.9687 - val_loss: 0.1537 - val_acc: 0.9702\n",
      "Epoch 422/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1184 - acc: 0.9685 - val_loss: 0.1492 - val_acc: 0.9699\n",
      "Epoch 423/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1182 - acc: 0.9685 - val_loss: 0.1518 - val_acc: 0.9702\n",
      "Epoch 424/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1176 - acc: 0.9688 - val_loss: 0.1496 - val_acc: 0.9707\n",
      "Epoch 425/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1178 - acc: 0.9685 - val_loss: 0.1557 - val_acc: 0.9699\n",
      "Epoch 426/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1180 - acc: 0.9688 - val_loss: 0.1559 - val_acc: 0.9702\n",
      "Epoch 427/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1178 - acc: 0.9687 - val_loss: 0.1528 - val_acc: 0.9704\n",
      "Epoch 428/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1181 - acc: 0.9685 - val_loss: 0.1479 - val_acc: 0.9702\n",
      "Epoch 429/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1178 - acc: 0.9687 - val_loss: 0.1567 - val_acc: 0.9702\n",
      "Epoch 430/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1185 - acc: 0.9687 - val_loss: 0.1636 - val_acc: 0.9704\n",
      "Epoch 431/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1180 - acc: 0.9684 - val_loss: 0.1579 - val_acc: 0.9702\n",
      "Epoch 432/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1175 - acc: 0.9687 - val_loss: 0.1639 - val_acc: 0.9699\n",
      "Epoch 433/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1183 - acc: 0.9687 - val_loss: 0.1573 - val_acc: 0.9702\n",
      "Epoch 434/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1189 - acc: 0.9684 - val_loss: 0.1522 - val_acc: 0.9704\n",
      "Epoch 435/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1175 - acc: 0.9687 - val_loss: 0.1575 - val_acc: 0.9702\n",
      "Epoch 436/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1177 - acc: 0.9685 - val_loss: 0.1547 - val_acc: 0.9704\n",
      "Epoch 437/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1176 - acc: 0.9687 - val_loss: 0.1574 - val_acc: 0.9702\n",
      "Epoch 438/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1171 - acc: 0.9689 - val_loss: 0.1548 - val_acc: 0.9704\n",
      "Epoch 439/1000\n",
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1179 - acc: 0.9685 - val_loss: 0.1459 - val_acc: 0.9704\n",
      "Epoch 440/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1181 - acc: 0.9686 - val_loss: 0.1503 - val_acc: 0.9702\n",
      "Epoch 441/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1182 - acc: 0.9685 - val_loss: 0.1551 - val_acc: 0.9704\n",
      "Epoch 442/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1173 - acc: 0.9687 - val_loss: 0.1585 - val_acc: 0.9704\n",
      "Epoch 443/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1182 - acc: 0.9686 - val_loss: 0.1563 - val_acc: 0.9704\n",
      "Epoch 444/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1172 - acc: 0.9687 - val_loss: 0.1554 - val_acc: 0.9697\n",
      "Epoch 445/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1173 - acc: 0.9686 - val_loss: 0.1590 - val_acc: 0.9699\n",
      "Epoch 446/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1170 - acc: 0.9685 - val_loss: 0.1599 - val_acc: 0.9699\n",
      "Epoch 447/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1201 - acc: 0.9685 - val_loss: 0.1545 - val_acc: 0.9702\n",
      "Epoch 448/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1182 - acc: 0.9684 - val_loss: 0.1644 - val_acc: 0.9704\n",
      "Epoch 449/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1169 - acc: 0.9687 - val_loss: 0.1576 - val_acc: 0.9704\n",
      "Epoch 450/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1177 - acc: 0.9686 - val_loss: 0.1515 - val_acc: 0.9699\n",
      "Epoch 451/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1173 - acc: 0.9685 - val_loss: 0.1630 - val_acc: 0.9699\n",
      "Epoch 452/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1178 - acc: 0.9687 - val_loss: 0.1577 - val_acc: 0.9704\n",
      "Epoch 453/1000\n",
      "16079/16079 [==============================] - 1s 61us/step - loss: 0.1169 - acc: 0.9688 - val_loss: 0.1572 - val_acc: 0.9704\n",
      "Epoch 454/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1169 - acc: 0.9686 - val_loss: 0.1514 - val_acc: 0.9704\n",
      "Epoch 455/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1169 - acc: 0.9686 - val_loss: 0.1578 - val_acc: 0.9704\n",
      "Epoch 456/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1163 - acc: 0.9687 - val_loss: 0.1560 - val_acc: 0.9699\n",
      "Epoch 457/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1174 - acc: 0.9685 - val_loss: 0.1529 - val_acc: 0.9702\n",
      "Epoch 458/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1173 - acc: 0.9686 - val_loss: 0.1536 - val_acc: 0.9702\n",
      "Epoch 459/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1183 - acc: 0.9686 - val_loss: 0.1524 - val_acc: 0.9702\n",
      "Epoch 460/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1178 - acc: 0.9685 - val_loss: 0.1578 - val_acc: 0.9704\n",
      "Epoch 461/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1171 - acc: 0.9686 - val_loss: 0.1554 - val_acc: 0.9699\n",
      "Epoch 462/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1174 - acc: 0.9687 - val_loss: 0.1564 - val_acc: 0.9699\n",
      "Epoch 463/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1166 - acc: 0.9688 - val_loss: 0.1527 - val_acc: 0.9702\n",
      "Epoch 464/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1170 - acc: 0.9687 - val_loss: 0.1598 - val_acc: 0.9699\n",
      "Epoch 465/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1163 - acc: 0.9687 - val_loss: 0.1560 - val_acc: 0.9704\n",
      "Epoch 466/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1181 - acc: 0.9687 - val_loss: 0.1610 - val_acc: 0.9702\n",
      "Epoch 467/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1166 - acc: 0.9688 - val_loss: 0.1536 - val_acc: 0.9704\n",
      "Epoch 468/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1169 - acc: 0.9685 - val_loss: 0.1634 - val_acc: 0.9704\n",
      "Epoch 469/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1189 - acc: 0.9684 - val_loss: 0.1510 - val_acc: 0.9704\n",
      "Epoch 470/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1169 - acc: 0.9686 - val_loss: 0.1662 - val_acc: 0.9699\n",
      "Epoch 471/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1164 - acc: 0.9687 - val_loss: 0.1535 - val_acc: 0.9702\n",
      "Epoch 472/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1165 - acc: 0.9687 - val_loss: 0.1586 - val_acc: 0.9704\n",
      "Epoch 473/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1170 - acc: 0.9688 - val_loss: 0.1671 - val_acc: 0.9707\n",
      "Epoch 474/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1171 - acc: 0.9685 - val_loss: 0.1553 - val_acc: 0.9702\n",
      "Epoch 475/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1163 - acc: 0.9688 - val_loss: 0.1528 - val_acc: 0.9702\n",
      "Epoch 476/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1166 - acc: 0.9687 - val_loss: 0.1584 - val_acc: 0.9702\n",
      "Epoch 477/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1163 - acc: 0.9687 - val_loss: 0.1651 - val_acc: 0.9699\n",
      "Epoch 478/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1159 - acc: 0.9687 - val_loss: 0.1507 - val_acc: 0.9702\n",
      "Epoch 479/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1168 - acc: 0.9688 - val_loss: 0.1660 - val_acc: 0.9699\n",
      "Epoch 480/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1172 - acc: 0.9687 - val_loss: 0.1573 - val_acc: 0.9694\n",
      "Epoch 481/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1165 - acc: 0.9686 - val_loss: 0.1563 - val_acc: 0.9702\n",
      "Epoch 482/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1162 - acc: 0.9685 - val_loss: 0.1583 - val_acc: 0.9697\n",
      "Epoch 483/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1159 - acc: 0.9685 - val_loss: 0.1635 - val_acc: 0.9697\n",
      "Epoch 484/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1158 - acc: 0.9690 - val_loss: 0.1537 - val_acc: 0.9699\n",
      "Epoch 485/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1168 - acc: 0.9685 - val_loss: 0.1508 - val_acc: 0.9702\n",
      "Epoch 486/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1162 - acc: 0.9688 - val_loss: 0.1650 - val_acc: 0.9699\n",
      "Epoch 487/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1160 - acc: 0.9688 - val_loss: 0.1600 - val_acc: 0.9699\n",
      "Epoch 488/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1159 - acc: 0.9686 - val_loss: 0.1511 - val_acc: 0.9702\n",
      "Epoch 489/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1169 - acc: 0.9687 - val_loss: 0.1507 - val_acc: 0.9702\n",
      "Epoch 490/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1165 - acc: 0.9688 - val_loss: 0.1625 - val_acc: 0.9697\n",
      "Epoch 491/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1159 - acc: 0.9687 - val_loss: 0.1559 - val_acc: 0.9702\n",
      "Epoch 492/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1172 - acc: 0.9683 - val_loss: 0.1575 - val_acc: 0.9702\n",
      "Epoch 493/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1156 - acc: 0.9688 - val_loss: 0.1643 - val_acc: 0.9699\n",
      "Epoch 494/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1153 - acc: 0.9689 - val_loss: 0.1632 - val_acc: 0.9699\n",
      "Epoch 495/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1150 - acc: 0.9687 - val_loss: 0.1580 - val_acc: 0.9692\n",
      "Epoch 496/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1165 - acc: 0.9687 - val_loss: 0.1571 - val_acc: 0.9699\n",
      "Epoch 497/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1153 - acc: 0.9688 - val_loss: 0.1700 - val_acc: 0.9699\n",
      "Epoch 498/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1160 - acc: 0.9689 - val_loss: 0.1647 - val_acc: 0.9699\n",
      "Epoch 499/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1157 - acc: 0.9688 - val_loss: 0.1620 - val_acc: 0.9702\n",
      "Epoch 500/1000\n",
      "16079/16079 [==============================] - 2s 117us/step - loss: 0.1178 - acc: 0.9685 - val_loss: 0.1508 - val_acc: 0.9702\n",
      "Epoch 501/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1171 - acc: 0.9685 - val_loss: 0.1545 - val_acc: 0.9704\n",
      "Epoch 502/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1156 - acc: 0.9687 - val_loss: 0.1681 - val_acc: 0.9699\n",
      "Epoch 503/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1160 - acc: 0.9690 - val_loss: 0.1563 - val_acc: 0.9697\n",
      "Epoch 504/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1180 - acc: 0.9687 - val_loss: 0.1509 - val_acc: 0.9702\n",
      "Epoch 505/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1163 - acc: 0.9687 - val_loss: 0.1614 - val_acc: 0.9697\n",
      "Epoch 506/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1151 - acc: 0.9689 - val_loss: 0.1577 - val_acc: 0.9697\n",
      "Epoch 507/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1160 - acc: 0.9685 - val_loss: 0.1548 - val_acc: 0.9697\n",
      "Epoch 508/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1147 - acc: 0.9688 - val_loss: 0.1627 - val_acc: 0.9702\n",
      "Epoch 509/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1154 - acc: 0.9689 - val_loss: 0.1578 - val_acc: 0.9697\n",
      "Epoch 510/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1149 - acc: 0.9689 - val_loss: 0.1645 - val_acc: 0.9699\n",
      "Epoch 511/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1162 - acc: 0.9685 - val_loss: 0.1559 - val_acc: 0.9704\n",
      "Epoch 512/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1150 - acc: 0.9688 - val_loss: 0.1608 - val_acc: 0.9699\n",
      "Epoch 513/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1156 - acc: 0.9685 - val_loss: 0.1551 - val_acc: 0.9702\n",
      "Epoch 514/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1152 - acc: 0.9687 - val_loss: 0.1619 - val_acc: 0.9707\n",
      "Epoch 515/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1159 - acc: 0.9687 - val_loss: 0.1603 - val_acc: 0.9702\n",
      "Epoch 516/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1146 - acc: 0.9689 - val_loss: 0.1641 - val_acc: 0.9697\n",
      "Epoch 517/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1177 - acc: 0.9687 - val_loss: 0.1593 - val_acc: 0.9702\n",
      "Epoch 518/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1152 - acc: 0.9687 - val_loss: 0.1566 - val_acc: 0.9697\n",
      "Epoch 519/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1155 - acc: 0.9686 - val_loss: 0.1632 - val_acc: 0.9694\n",
      "Epoch 520/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1152 - acc: 0.9690 - val_loss: 0.1526 - val_acc: 0.9694\n",
      "Epoch 521/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1150 - acc: 0.9687 - val_loss: 0.1610 - val_acc: 0.9697\n",
      "Epoch 522/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1146 - acc: 0.9690 - val_loss: 0.1592 - val_acc: 0.9699\n",
      "Epoch 523/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1146 - acc: 0.9690 - val_loss: 0.1595 - val_acc: 0.9702\n",
      "Epoch 524/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1150 - acc: 0.9687 - val_loss: 0.1624 - val_acc: 0.9699\n",
      "Epoch 525/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1151 - acc: 0.9687 - val_loss: 0.1622 - val_acc: 0.9697\n",
      "Epoch 526/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1147 - acc: 0.9688 - val_loss: 0.1617 - val_acc: 0.9697\n",
      "Epoch 527/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1148 - acc: 0.9688 - val_loss: 0.1598 - val_acc: 0.9699\n",
      "Epoch 528/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1163 - acc: 0.9686 - val_loss: 0.1532 - val_acc: 0.9704\n",
      "Epoch 529/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1151 - acc: 0.9687 - val_loss: 0.1622 - val_acc: 0.9694\n",
      "Epoch 530/1000\n",
      "16079/16079 [==============================] - 1s 65us/step - loss: 0.1140 - acc: 0.9687 - val_loss: 0.1630 - val_acc: 0.9697\n",
      "Epoch 531/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1148 - acc: 0.9693 - val_loss: 0.1623 - val_acc: 0.9694\n",
      "Epoch 532/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1147 - acc: 0.9688 - val_loss: 0.1729 - val_acc: 0.9694\n",
      "Epoch 533/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1169 - acc: 0.9687 - val_loss: 0.1588 - val_acc: 0.9704\n",
      "Epoch 534/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1155 - acc: 0.9686 - val_loss: 0.1597 - val_acc: 0.9699\n",
      "Epoch 535/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1152 - acc: 0.9687 - val_loss: 0.1565 - val_acc: 0.9697\n",
      "Epoch 536/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1145 - acc: 0.9688 - val_loss: 0.1548 - val_acc: 0.9702\n",
      "Epoch 537/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1142 - acc: 0.9689 - val_loss: 0.1587 - val_acc: 0.9697\n",
      "Epoch 538/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1145 - acc: 0.9688 - val_loss: 0.1579 - val_acc: 0.9694\n",
      "Epoch 539/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1141 - acc: 0.9688 - val_loss: 0.1650 - val_acc: 0.9702\n",
      "Epoch 540/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1134 - acc: 0.9690 - val_loss: 0.1676 - val_acc: 0.9697\n",
      "Epoch 541/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1143 - acc: 0.9689 - val_loss: 0.1665 - val_acc: 0.9697\n",
      "Epoch 542/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1164 - acc: 0.9685 - val_loss: 0.1580 - val_acc: 0.9694\n",
      "Epoch 543/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1139 - acc: 0.9689 - val_loss: 0.1679 - val_acc: 0.9704\n",
      "Epoch 544/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1142 - acc: 0.9687 - val_loss: 0.1660 - val_acc: 0.9697\n",
      "Epoch 545/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1138 - acc: 0.9692 - val_loss: 0.1644 - val_acc: 0.9697\n",
      "Epoch 546/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1144 - acc: 0.9689 - val_loss: 0.1727 - val_acc: 0.9694\n",
      "Epoch 547/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1150 - acc: 0.9684 - val_loss: 0.1521 - val_acc: 0.9702\n",
      "Epoch 548/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1148 - acc: 0.9687 - val_loss: 0.1624 - val_acc: 0.9694\n",
      "Epoch 549/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1146 - acc: 0.9689 - val_loss: 0.1610 - val_acc: 0.9694\n",
      "Epoch 550/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1146 - acc: 0.9687 - val_loss: 0.1627 - val_acc: 0.9692\n",
      "Epoch 551/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1139 - acc: 0.9689 - val_loss: 0.1579 - val_acc: 0.9702\n",
      "Epoch 552/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1177 - acc: 0.9688 - val_loss: 0.1550 - val_acc: 0.9702\n",
      "Epoch 553/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1156 - acc: 0.9686 - val_loss: 0.1575 - val_acc: 0.9694\n",
      "Epoch 554/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1150 - acc: 0.9685 - val_loss: 0.1641 - val_acc: 0.9699\n",
      "Epoch 555/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1138 - acc: 0.9687 - val_loss: 0.1616 - val_acc: 0.9704\n",
      "Epoch 556/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1155 - acc: 0.9688 - val_loss: 0.1604 - val_acc: 0.9702\n",
      "Epoch 557/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1129 - acc: 0.9689 - val_loss: 0.1660 - val_acc: 0.9699\n",
      "Epoch 558/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1143 - acc: 0.9685 - val_loss: 0.1586 - val_acc: 0.9702\n",
      "Epoch 559/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1144 - acc: 0.9687 - val_loss: 0.1611 - val_acc: 0.9694\n",
      "Epoch 560/1000\n",
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1128 - acc: 0.9687 - val_loss: 0.1678 - val_acc: 0.9699\n",
      "Epoch 561/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1131 - acc: 0.9690 - val_loss: 0.1695 - val_acc: 0.9702\n",
      "Epoch 562/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1134 - acc: 0.9693 - val_loss: 0.1646 - val_acc: 0.9702\n",
      "Epoch 563/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1131 - acc: 0.9688 - val_loss: 0.1646 - val_acc: 0.9697\n",
      "Epoch 564/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1140 - acc: 0.9688 - val_loss: 0.1638 - val_acc: 0.9697\n",
      "Epoch 565/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1141 - acc: 0.9689 - val_loss: 0.1583 - val_acc: 0.9702\n",
      "Epoch 566/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1145 - acc: 0.9688 - val_loss: 0.1641 - val_acc: 0.9694\n",
      "Epoch 567/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1136 - acc: 0.9686 - val_loss: 0.1637 - val_acc: 0.9692\n",
      "Epoch 568/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1132 - acc: 0.9689 - val_loss: 0.1639 - val_acc: 0.9699\n",
      "Epoch 569/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1126 - acc: 0.9688 - val_loss: 0.1667 - val_acc: 0.9697\n",
      "Epoch 570/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1135 - acc: 0.9690 - val_loss: 0.1561 - val_acc: 0.9702\n",
      "Epoch 571/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1137 - acc: 0.9689 - val_loss: 0.1612 - val_acc: 0.9702\n",
      "Epoch 572/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1138 - acc: 0.9690 - val_loss: 0.1619 - val_acc: 0.9697\n",
      "Epoch 573/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1130 - acc: 0.9689 - val_loss: 0.1615 - val_acc: 0.9699\n",
      "Epoch 574/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1146 - acc: 0.9687 - val_loss: 0.1629 - val_acc: 0.9697\n",
      "Epoch 575/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1131 - acc: 0.9687 - val_loss: 0.1712 - val_acc: 0.9694\n",
      "Epoch 576/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1143 - acc: 0.9688 - val_loss: 0.1658 - val_acc: 0.9692\n",
      "Epoch 577/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1133 - acc: 0.9690 - val_loss: 0.1620 - val_acc: 0.9699\n",
      "Epoch 578/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1130 - acc: 0.9688 - val_loss: 0.1664 - val_acc: 0.9697\n",
      "Epoch 579/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1131 - acc: 0.9688 - val_loss: 0.1640 - val_acc: 0.9694\n",
      "Epoch 580/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1140 - acc: 0.9688 - val_loss: 0.1721 - val_acc: 0.9694\n",
      "Epoch 581/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1151 - acc: 0.9685 - val_loss: 0.1688 - val_acc: 0.9697\n",
      "Epoch 582/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1124 - acc: 0.9690 - val_loss: 0.1651 - val_acc: 0.9692\n",
      "Epoch 583/1000\n",
      "16079/16079 [==============================] - 1s 64us/step - loss: 0.1118 - acc: 0.9686 - val_loss: 0.1610 - val_acc: 0.9699\n",
      "Epoch 584/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1122 - acc: 0.9690 - val_loss: 0.1617 - val_acc: 0.9699\n",
      "Epoch 585/1000\n",
      "16079/16079 [==============================] - 1s 66us/step - loss: 0.1133 - acc: 0.9689 - val_loss: 0.1748 - val_acc: 0.9697\n",
      "Epoch 586/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1131 - acc: 0.9688 - val_loss: 0.1669 - val_acc: 0.9699\n",
      "Epoch 587/1000\n",
      "16079/16079 [==============================] - 1s 66us/step - loss: 0.1134 - acc: 0.9685 - val_loss: 0.1594 - val_acc: 0.9692\n",
      "Epoch 588/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1141 - acc: 0.9687 - val_loss: 0.1677 - val_acc: 0.9699\n",
      "Epoch 589/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1134 - acc: 0.9687 - val_loss: 0.1660 - val_acc: 0.9699\n",
      "Epoch 590/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1127 - acc: 0.9687 - val_loss: 0.1705 - val_acc: 0.9694\n",
      "Epoch 591/1000\n",
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1131 - acc: 0.9692 - val_loss: 0.1663 - val_acc: 0.9694\n",
      "Epoch 592/1000\n",
      "16079/16079 [==============================] - 1s 59us/step - loss: 0.1130 - acc: 0.9689 - val_loss: 0.1668 - val_acc: 0.9692\n",
      "Epoch 593/1000\n",
      "16079/16079 [==============================] - 1s 62us/step - loss: 0.1143 - acc: 0.9688 - val_loss: 0.1632 - val_acc: 0.9694\n",
      "Epoch 594/1000\n",
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1132 - acc: 0.9688 - val_loss: 0.1605 - val_acc: 0.9689s - loss: 0.1124 - acc: \n",
      "Epoch 595/1000\n",
      "16079/16079 [==============================] - 1s 59us/step - loss: 0.1120 - acc: 0.9690 - val_loss: 0.1588 - val_acc: 0.9689\n",
      "Epoch 596/1000\n",
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1127 - acc: 0.9690 - val_loss: 0.1572 - val_acc: 0.9699\n",
      "Epoch 597/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1131 - acc: 0.9687 - val_loss: 0.1687 - val_acc: 0.9689\n",
      "Epoch 598/1000\n",
      "16079/16079 [==============================] - 1s 63us/step - loss: 0.1131 - acc: 0.9688 - val_loss: 0.1739 - val_acc: 0.9692\n",
      "Epoch 599/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1129 - acc: 0.9688 - val_loss: 0.1686 - val_acc: 0.9697\n",
      "Epoch 600/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1124 - acc: 0.9685 - val_loss: 0.1664 - val_acc: 0.9692\n",
      "Epoch 601/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1137 - acc: 0.9688 - val_loss: 0.1663 - val_acc: 0.9697\n",
      "Epoch 602/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1123 - acc: 0.9692 - val_loss: 0.1677 - val_acc: 0.9707\n",
      "Epoch 603/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1123 - acc: 0.9688 - val_loss: 0.1658 - val_acc: 0.9699\n",
      "Epoch 604/1000\n",
      "16079/16079 [==============================] - 1s 63us/step - loss: 0.1120 - acc: 0.9693 - val_loss: 0.1636 - val_acc: 0.9699\n",
      "Epoch 605/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1124 - acc: 0.9688 - val_loss: 0.1694 - val_acc: 0.9689\n",
      "Epoch 606/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1116 - acc: 0.9689 - val_loss: 0.1584 - val_acc: 0.9694\n",
      "Epoch 607/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1124 - acc: 0.9690 - val_loss: 0.1668 - val_acc: 0.9699\n",
      "Epoch 608/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1146 - acc: 0.9685 - val_loss: 0.1582 - val_acc: 0.9702\n",
      "Epoch 609/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1128 - acc: 0.9685 - val_loss: 0.1689 - val_acc: 0.9704\n",
      "Epoch 610/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1140 - acc: 0.9688 - val_loss: 0.1650 - val_acc: 0.9697\n",
      "Epoch 611/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1109 - acc: 0.9692 - val_loss: 0.1673 - val_acc: 0.9697\n",
      "Epoch 612/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1117 - acc: 0.9688 - val_loss: 0.1684 - val_acc: 0.9694\n",
      "Epoch 613/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1123 - acc: 0.9688 - val_loss: 0.1680 - val_acc: 0.9689\n",
      "Epoch 614/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1130 - acc: 0.9688 - val_loss: 0.1784 - val_acc: 0.9697\n",
      "Epoch 615/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1130 - acc: 0.9688 - val_loss: 0.1712 - val_acc: 0.9694\n",
      "Epoch 616/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1136 - acc: 0.9688 - val_loss: 0.1664 - val_acc: 0.9699\n",
      "Epoch 617/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1122 - acc: 0.9687 - val_loss: 0.1687 - val_acc: 0.9697\n",
      "Epoch 618/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1106 - acc: 0.9693 - val_loss: 0.1600 - val_acc: 0.9697\n",
      "Epoch 619/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1116 - acc: 0.9690 - val_loss: 0.1678 - val_acc: 0.9687\n",
      "Epoch 620/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1142 - acc: 0.9687 - val_loss: 0.1601 - val_acc: 0.9697\n",
      "Epoch 621/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1118 - acc: 0.9691 - val_loss: 0.1639 - val_acc: 0.9689\n",
      "Epoch 622/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1115 - acc: 0.9690 - val_loss: 0.1670 - val_acc: 0.9692\n",
      "Epoch 623/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1115 - acc: 0.9689 - val_loss: 0.1778 - val_acc: 0.9684\n",
      "Epoch 624/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1132 - acc: 0.9693 - val_loss: 0.1638 - val_acc: 0.9697\n",
      "Epoch 625/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1129 - acc: 0.9690 - val_loss: 0.1661 - val_acc: 0.9697\n",
      "Epoch 626/1000\n",
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1115 - acc: 0.9690 - val_loss: 0.1688 - val_acc: 0.9689\n",
      "Epoch 627/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1123 - acc: 0.9684 - val_loss: 0.1624 - val_acc: 0.9697\n",
      "Epoch 628/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1111 - acc: 0.9688 - val_loss: 0.1696 - val_acc: 0.9697\n",
      "Epoch 629/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1117 - acc: 0.9689 - val_loss: 0.1771 - val_acc: 0.9699\n",
      "Epoch 630/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1117 - acc: 0.9688 - val_loss: 0.1772 - val_acc: 0.9699\n",
      "Epoch 631/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1123 - acc: 0.9688 - val_loss: 0.1677 - val_acc: 0.9689\n",
      "Epoch 632/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1131 - acc: 0.9687 - val_loss: 0.1688 - val_acc: 0.9692\n",
      "Epoch 633/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1114 - acc: 0.9691 - val_loss: 0.1668 - val_acc: 0.9694\n",
      "Epoch 634/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1125 - acc: 0.9687 - val_loss: 0.1664 - val_acc: 0.9692\n",
      "Epoch 635/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1117 - acc: 0.9688 - val_loss: 0.1587 - val_acc: 0.9699\n",
      "Epoch 636/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1113 - acc: 0.9684 - val_loss: 0.1745 - val_acc: 0.9692\n",
      "Epoch 637/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1124 - acc: 0.9690 - val_loss: 0.1643 - val_acc: 0.9692\n",
      "Epoch 638/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1124 - acc: 0.9685 - val_loss: 0.1579 - val_acc: 0.9704\n",
      "Epoch 639/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1137 - acc: 0.9686 - val_loss: 0.1624 - val_acc: 0.9687\n",
      "Epoch 640/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1114 - acc: 0.9690 - val_loss: 0.1641 - val_acc: 0.9692\n",
      "Epoch 641/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1123 - acc: 0.9690 - val_loss: 0.1664 - val_acc: 0.9697\n",
      "Epoch 642/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1110 - acc: 0.9687 - val_loss: 0.1646 - val_acc: 0.9697\n",
      "Epoch 643/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1111 - acc: 0.9693 - val_loss: 0.1704 - val_acc: 0.9699\n",
      "Epoch 644/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1141 - acc: 0.9687 - val_loss: 0.1673 - val_acc: 0.9699\n",
      "Epoch 645/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1102 - acc: 0.9691 - val_loss: 0.1688 - val_acc: 0.9694\n",
      "Epoch 646/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1109 - acc: 0.9692 - val_loss: 0.1730 - val_acc: 0.9702\n",
      "Epoch 647/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1133 - acc: 0.9686 - val_loss: 0.1677 - val_acc: 0.9697\n",
      "Epoch 648/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1127 - acc: 0.9687 - val_loss: 0.1687 - val_acc: 0.9689\n",
      "Epoch 649/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1113 - acc: 0.9689 - val_loss: 0.1649 - val_acc: 0.9694\n",
      "Epoch 650/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1108 - acc: 0.9691 - val_loss: 0.1645 - val_acc: 0.9692\n",
      "Epoch 651/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1102 - acc: 0.9691 - val_loss: 0.1687 - val_acc: 0.9692\n",
      "Epoch 652/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1144 - acc: 0.9685 - val_loss: 0.1613 - val_acc: 0.9704\n",
      "Epoch 653/1000\n",
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1119 - acc: 0.9685 - val_loss: 0.1784 - val_acc: 0.9692\n",
      "Epoch 654/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1108 - acc: 0.9690 - val_loss: 0.1613 - val_acc: 0.9689\n",
      "Epoch 655/1000\n",
      "16079/16079 [==============================] - 1s 66us/step - loss: 0.1121 - acc: 0.9692 - val_loss: 0.1723 - val_acc: 0.9684\n",
      "Epoch 656/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1111 - acc: 0.9692 - val_loss: 0.1632 - val_acc: 0.9684\n",
      "Epoch 657/1000\n",
      "16079/16079 [==============================] - 1s 62us/step - loss: 0.1112 - acc: 0.9695 - val_loss: 0.1677 - val_acc: 0.9689\n",
      "Epoch 658/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1102 - acc: 0.9690 - val_loss: 0.1677 - val_acc: 0.9694\n",
      "Epoch 659/1000\n",
      "16079/16079 [==============================] - 1s 66us/step - loss: 0.1105 - acc: 0.9690 - val_loss: 0.1661 - val_acc: 0.9694\n",
      "Epoch 660/1000\n",
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1122 - acc: 0.9691 - val_loss: 0.1675 - val_acc: 0.9689\n",
      "Epoch 661/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1127 - acc: 0.9690 - val_loss: 0.1664 - val_acc: 0.9687\n",
      "Epoch 662/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1107 - acc: 0.9692 - val_loss: 0.1682 - val_acc: 0.9694\n",
      "Epoch 663/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1098 - acc: 0.9690 - val_loss: 0.1752 - val_acc: 0.9694\n",
      "Epoch 664/1000\n",
      "16079/16079 [==============================] - 1s 63us/step - loss: 0.1099 - acc: 0.9693 - val_loss: 0.1671 - val_acc: 0.9687\n",
      "Epoch 665/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1104 - acc: 0.9690 - val_loss: 0.1715 - val_acc: 0.9697\n",
      "Epoch 666/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1118 - acc: 0.9690 - val_loss: 0.1666 - val_acc: 0.9694\n",
      "Epoch 667/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1121 - acc: 0.9687 - val_loss: 0.1633 - val_acc: 0.9697\n",
      "Epoch 668/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1100 - acc: 0.9692 - val_loss: 0.1631 - val_acc: 0.9694\n",
      "Epoch 669/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1106 - acc: 0.9691 - val_loss: 0.1663 - val_acc: 0.9699\n",
      "Epoch 670/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1105 - acc: 0.9693 - val_loss: 0.1791 - val_acc: 0.9689\n",
      "Epoch 671/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1106 - acc: 0.9690 - val_loss: 0.1766 - val_acc: 0.9697\n",
      "Epoch 672/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1108 - acc: 0.9689 - val_loss: 0.1675 - val_acc: 0.9694\n",
      "Epoch 673/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1104 - acc: 0.9695 - val_loss: 0.1856 - val_acc: 0.9694\n",
      "Epoch 674/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1107 - acc: 0.9691 - val_loss: 0.1751 - val_acc: 0.9697\n",
      "Epoch 675/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1119 - acc: 0.9687 - val_loss: 0.1649 - val_acc: 0.9687\n",
      "Epoch 676/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1108 - acc: 0.9690 - val_loss: 0.1690 - val_acc: 0.9689\n",
      "Epoch 677/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1106 - acc: 0.9689 - val_loss: 0.1645 - val_acc: 0.9692\n",
      "Epoch 678/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1131 - acc: 0.9686 - val_loss: 0.1632 - val_acc: 0.9689\n",
      "Epoch 679/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1106 - acc: 0.9689 - val_loss: 0.1726 - val_acc: 0.9687\n",
      "Epoch 680/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1096 - acc: 0.9692 - val_loss: 0.1792 - val_acc: 0.9694\n",
      "Epoch 681/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1121 - acc: 0.9688 - val_loss: 0.1760 - val_acc: 0.9682\n",
      "Epoch 682/1000\n",
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1109 - acc: 0.9692 - val_loss: 0.1717 - val_acc: 0.9689\n",
      "Epoch 683/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1094 - acc: 0.9697 - val_loss: 0.1730 - val_acc: 0.9692\n",
      "Epoch 684/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1097 - acc: 0.9692 - val_loss: 0.1721 - val_acc: 0.9687\n",
      "Epoch 685/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1099 - acc: 0.9689 - val_loss: 0.1766 - val_acc: 0.9679\n",
      "Epoch 686/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1118 - acc: 0.9690 - val_loss: 0.1750 - val_acc: 0.9687\n",
      "Epoch 687/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1087 - acc: 0.9690 - val_loss: 0.1774 - val_acc: 0.9692\n",
      "Epoch 688/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1111 - acc: 0.9693 - val_loss: 0.1674 - val_acc: 0.9692\n",
      "Epoch 689/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1106 - acc: 0.9691 - val_loss: 0.1588 - val_acc: 0.9684\n",
      "Epoch 690/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1108 - acc: 0.9689 - val_loss: 0.1695 - val_acc: 0.9692\n",
      "Epoch 691/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1109 - acc: 0.9690 - val_loss: 0.1647 - val_acc: 0.9692\n",
      "Epoch 692/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1096 - acc: 0.9692 - val_loss: 0.1782 - val_acc: 0.9684\n",
      "Epoch 693/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1083 - acc: 0.9690 - val_loss: 0.1752 - val_acc: 0.9684\n",
      "Epoch 694/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1094 - acc: 0.9693 - val_loss: 0.1825 - val_acc: 0.9692\n",
      "Epoch 695/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1127 - acc: 0.9690 - val_loss: 0.1650 - val_acc: 0.9699\n",
      "Epoch 696/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1106 - acc: 0.9689 - val_loss: 0.1678 - val_acc: 0.9697\n",
      "Epoch 697/1000\n",
      "16079/16079 [==============================] - 1s 66us/step - loss: 0.1098 - acc: 0.9695 - val_loss: 0.1651 - val_acc: 0.9694\n",
      "Epoch 698/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1089 - acc: 0.9693 - val_loss: 0.1710 - val_acc: 0.9692\n",
      "Epoch 699/1000\n",
      "16079/16079 [==============================] - 1s 66us/step - loss: 0.1087 - acc: 0.9692 - val_loss: 0.1797 - val_acc: 0.9692\n",
      "Epoch 700/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1094 - acc: 0.9691 - val_loss: 0.1734 - val_acc: 0.9692\n",
      "Epoch 701/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1095 - acc: 0.9691 - val_loss: 0.1687 - val_acc: 0.9697\n",
      "Epoch 702/1000\n",
      "16079/16079 [==============================] - 1s 55us/step - loss: 0.1118 - acc: 0.9689 - val_loss: 0.1668 - val_acc: 0.9684\n",
      "Epoch 703/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1094 - acc: 0.9692 - val_loss: 0.1762 - val_acc: 0.9692\n",
      "Epoch 704/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1111 - acc: 0.9688 - val_loss: 0.1682 - val_acc: 0.9697\n",
      "Epoch 705/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1095 - acc: 0.9694 - val_loss: 0.1751 - val_acc: 0.9697\n",
      "Epoch 706/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1089 - acc: 0.9694 - val_loss: 0.1729 - val_acc: 0.9697\n",
      "Epoch 707/1000\n",
      "16079/16079 [==============================] - 1s 64us/step - loss: 0.1117 - acc: 0.9693 - val_loss: 0.1746 - val_acc: 0.9692\n",
      "Epoch 708/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1110 - acc: 0.9693 - val_loss: 0.1725 - val_acc: 0.9694\n",
      "Epoch 709/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1090 - acc: 0.9690 - val_loss: 0.1774 - val_acc: 0.9684\n",
      "Epoch 710/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1094 - acc: 0.9692 - val_loss: 0.1756 - val_acc: 0.9694\n",
      "Epoch 711/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1098 - acc: 0.9694 - val_loss: 0.1594 - val_acc: 0.9692\n",
      "Epoch 712/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1092 - acc: 0.9690 - val_loss: 0.1788 - val_acc: 0.9689\n",
      "Epoch 713/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1083 - acc: 0.9690 - val_loss: 0.1771 - val_acc: 0.9692\n",
      "Epoch 714/1000\n",
      "16079/16079 [==============================] - 1s 61us/step - loss: 0.1108 - acc: 0.9685 - val_loss: 0.1782 - val_acc: 0.9687\n",
      "Epoch 715/1000\n",
      "16079/16079 [==============================] - 1s 60us/step - loss: 0.1098 - acc: 0.9696 - val_loss: 0.1787 - val_acc: 0.9692\n",
      "Epoch 716/1000\n",
      "16079/16079 [==============================] - 1s 66us/step - loss: 0.1092 - acc: 0.9694 - val_loss: 0.1753 - val_acc: 0.9692\n",
      "Epoch 717/1000\n",
      "16079/16079 [==============================] - 1s 60us/step - loss: 0.1092 - acc: 0.9696 - val_loss: 0.1662 - val_acc: 0.9692\n",
      "Epoch 718/1000\n",
      "16079/16079 [==============================] - 1s 61us/step - loss: 0.1092 - acc: 0.9694 - val_loss: 0.1811 - val_acc: 0.9692\n",
      "Epoch 719/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1088 - acc: 0.9691 - val_loss: 0.1739 - val_acc: 0.9687\n",
      "Epoch 720/1000\n",
      "16079/16079 [==============================] - 1s 63us/step - loss: 0.1089 - acc: 0.9692 - val_loss: 0.1808 - val_acc: 0.9692\n",
      "Epoch 721/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1115 - acc: 0.9686 - val_loss: 0.1709 - val_acc: 0.9689\n",
      "Epoch 722/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1097 - acc: 0.9690 - val_loss: 0.1695 - val_acc: 0.9684\n",
      "Epoch 723/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1088 - acc: 0.9694 - val_loss: 0.1742 - val_acc: 0.9697\n",
      "Epoch 724/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1089 - acc: 0.9692 - val_loss: 0.1808 - val_acc: 0.9687\n",
      "Epoch 725/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1092 - acc: 0.9688 - val_loss: 0.1870 - val_acc: 0.9689\n",
      "Epoch 726/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1083 - acc: 0.9692 - val_loss: 0.1721 - val_acc: 0.9694\n",
      "Epoch 727/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1091 - acc: 0.9694 - val_loss: 0.1799 - val_acc: 0.9694\n",
      "Epoch 728/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1091 - acc: 0.9695 - val_loss: 0.1816 - val_acc: 0.9684\n",
      "Epoch 729/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1104 - acc: 0.9693 - val_loss: 0.1723 - val_acc: 0.9697\n",
      "Epoch 730/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1084 - acc: 0.9693 - val_loss: 0.1714 - val_acc: 0.9697\n",
      "Epoch 731/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1087 - acc: 0.9691 - val_loss: 0.1681 - val_acc: 0.9687\n",
      "Epoch 732/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1088 - acc: 0.9692 - val_loss: 0.1781 - val_acc: 0.9692\n",
      "Epoch 733/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1118 - acc: 0.9690 - val_loss: 0.1695 - val_acc: 0.9684\n",
      "Epoch 734/1000\n",
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1088 - acc: 0.9690 - val_loss: 0.1671 - val_acc: 0.9692\n",
      "Epoch 735/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1086 - acc: 0.9690 - val_loss: 0.1875 - val_acc: 0.9692\n",
      "Epoch 736/1000\n",
      "16079/16079 [==============================] - ETA: 0s - loss: 0.1080 - acc: 0.969 - 1s 89us/step - loss: 0.1076 - acc: 0.9696 - val_loss: 0.1657 - val_acc: 0.9694\n",
      "Epoch 737/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1089 - acc: 0.9692 - val_loss: 0.1737 - val_acc: 0.9692\n",
      "Epoch 738/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1093 - acc: 0.9690 - val_loss: 0.1662 - val_acc: 0.9694\n",
      "Epoch 739/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1089 - acc: 0.9693 - val_loss: 0.1799 - val_acc: 0.9692\n",
      "Epoch 740/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1081 - acc: 0.9696 - val_loss: 0.1646 - val_acc: 0.9694\n",
      "Epoch 741/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1103 - acc: 0.9689 - val_loss: 0.1774 - val_acc: 0.9689\n",
      "Epoch 742/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1088 - acc: 0.9692 - val_loss: 0.1737 - val_acc: 0.9687\n",
      "Epoch 743/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1083 - acc: 0.9690 - val_loss: 0.1727 - val_acc: 0.9682\n",
      "Epoch 744/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1091 - acc: 0.9688 - val_loss: 0.1724 - val_acc: 0.9694\n",
      "Epoch 745/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1078 - acc: 0.9692 - val_loss: 0.1767 - val_acc: 0.9687\n",
      "Epoch 746/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1098 - acc: 0.9693 - val_loss: 0.1882 - val_acc: 0.9694\n",
      "Epoch 747/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1078 - acc: 0.9695 - val_loss: 0.1793 - val_acc: 0.9682\n",
      "Epoch 748/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1076 - acc: 0.9694 - val_loss: 0.1845 - val_acc: 0.9682\n",
      "Epoch 749/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1124 - acc: 0.9682 - val_loss: 0.1675 - val_acc: 0.9692\n",
      "Epoch 750/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1116 - acc: 0.9690 - val_loss: 0.1728 - val_acc: 0.9679\n",
      "Epoch 751/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1073 - acc: 0.9696 - val_loss: 0.1802 - val_acc: 0.9684\n",
      "Epoch 752/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1100 - acc: 0.9690 - val_loss: 0.1712 - val_acc: 0.9692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 753/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1084 - acc: 0.9695 - val_loss: 0.1788 - val_acc: 0.9689\n",
      "Epoch 754/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1076 - acc: 0.9693 - val_loss: 0.1729 - val_acc: 0.9694\n",
      "Epoch 755/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1145 - acc: 0.9688 - val_loss: 0.1710 - val_acc: 0.9702\n",
      "Epoch 756/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1092 - acc: 0.9693 - val_loss: 0.1770 - val_acc: 0.9682\n",
      "Epoch 757/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1081 - acc: 0.9695 - val_loss: 0.1698 - val_acc: 0.9689\n",
      "Epoch 758/1000\n",
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1080 - acc: 0.9694 - val_loss: 0.1788 - val_acc: 0.9699\n",
      "Epoch 759/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1091 - acc: 0.9692 - val_loss: 0.1761 - val_acc: 0.9684\n",
      "Epoch 760/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1063 - acc: 0.9696 - val_loss: 0.1777 - val_acc: 0.9699\n",
      "Epoch 761/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1084 - acc: 0.9693 - val_loss: 0.1700 - val_acc: 0.9702\n",
      "Epoch 762/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1074 - acc: 0.9695 - val_loss: 0.1754 - val_acc: 0.9694\n",
      "Epoch 763/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1150 - acc: 0.9687 - val_loss: 0.1666 - val_acc: 0.9679\n",
      "Epoch 764/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1102 - acc: 0.9691 - val_loss: 0.1836 - val_acc: 0.9689\n",
      "Epoch 765/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1082 - acc: 0.9691 - val_loss: 0.1718 - val_acc: 0.9694\n",
      "Epoch 766/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1098 - acc: 0.9695 - val_loss: 0.1669 - val_acc: 0.9699\n",
      "Epoch 767/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1095 - acc: 0.9694 - val_loss: 0.1688 - val_acc: 0.9679\n",
      "Epoch 768/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1077 - acc: 0.9695 - val_loss: 0.1707 - val_acc: 0.9692\n",
      "Epoch 769/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1072 - acc: 0.9690 - val_loss: 0.1747 - val_acc: 0.9689\n",
      "Epoch 770/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1081 - acc: 0.9695 - val_loss: 0.1749 - val_acc: 0.9689\n",
      "Epoch 771/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1097 - acc: 0.9690 - val_loss: 0.1695 - val_acc: 0.9694\n",
      "Epoch 772/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1107 - acc: 0.9686 - val_loss: 0.1785 - val_acc: 0.9694\n",
      "Epoch 773/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1076 - acc: 0.9696 - val_loss: 0.1726 - val_acc: 0.9694\n",
      "Epoch 774/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1083 - acc: 0.9687 - val_loss: 0.1700 - val_acc: 0.9699\n",
      "Epoch 775/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1084 - acc: 0.9693 - val_loss: 0.1756 - val_acc: 0.9699\n",
      "Epoch 776/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1083 - acc: 0.9693 - val_loss: 0.1738 - val_acc: 0.9694\n",
      "Epoch 777/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1080 - acc: 0.9692 - val_loss: 0.1787 - val_acc: 0.9699\n",
      "Epoch 778/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1081 - acc: 0.9693 - val_loss: 0.1693 - val_acc: 0.9692\n",
      "Epoch 779/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1069 - acc: 0.9695 - val_loss: 0.1825 - val_acc: 0.9692\n",
      "Epoch 780/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1076 - acc: 0.9693 - val_loss: 0.1759 - val_acc: 0.9689\n",
      "Epoch 781/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1082 - acc: 0.9690 - val_loss: 0.1820 - val_acc: 0.9679\n",
      "Epoch 782/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1105 - acc: 0.9694 - val_loss: 0.1780 - val_acc: 0.9694\n",
      "Epoch 783/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1118 - acc: 0.9689 - val_loss: 0.1675 - val_acc: 0.9692\n",
      "Epoch 784/1000\n",
      "16079/16079 [==============================] - 1s 65us/step - loss: 0.1076 - acc: 0.9689 - val_loss: 0.1820 - val_acc: 0.9692\n",
      "Epoch 785/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1079 - acc: 0.9691 - val_loss: 0.1829 - val_acc: 0.9697\n",
      "Epoch 786/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1068 - acc: 0.9697 - val_loss: 0.1754 - val_acc: 0.9689\n",
      "Epoch 787/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1132 - acc: 0.9680 - val_loss: 0.1668 - val_acc: 0.9687\n",
      "Epoch 788/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1086 - acc: 0.9693 - val_loss: 0.1696 - val_acc: 0.9689\n",
      "Epoch 789/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1073 - acc: 0.9695 - val_loss: 0.1722 - val_acc: 0.9694\n",
      "Epoch 790/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1092 - acc: 0.9695 - val_loss: 0.1727 - val_acc: 0.9684\n",
      "Epoch 791/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1082 - acc: 0.9692 - val_loss: 0.1829 - val_acc: 0.9699\n",
      "Epoch 792/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1076 - acc: 0.9696 - val_loss: 0.1839 - val_acc: 0.9689\n",
      "Epoch 793/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1086 - acc: 0.9690 - val_loss: 0.1702 - val_acc: 0.9687\n",
      "Epoch 794/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1085 - acc: 0.9696 - val_loss: 0.1765 - val_acc: 0.9674\n",
      "Epoch 795/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1080 - acc: 0.9692 - val_loss: 0.1808 - val_acc: 0.9697\n",
      "Epoch 796/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1069 - acc: 0.9693 - val_loss: 0.1879 - val_acc: 0.9694\n",
      "Epoch 797/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1089 - acc: 0.9690 - val_loss: 0.1751 - val_acc: 0.9692\n",
      "Epoch 798/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1100 - acc: 0.9688 - val_loss: 0.1646 - val_acc: 0.9697\n",
      "Epoch 799/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1088 - acc: 0.9688 - val_loss: 0.1708 - val_acc: 0.9697\n",
      "Epoch 800/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1083 - acc: 0.9692 - val_loss: 0.1704 - val_acc: 0.9689\n",
      "Epoch 801/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1069 - acc: 0.9692 - val_loss: 0.1872 - val_acc: 0.9689\n",
      "Epoch 802/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1104 - acc: 0.9686 - val_loss: 0.1713 - val_acc: 0.9684\n",
      "Epoch 803/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1087 - acc: 0.9688 - val_loss: 0.1794 - val_acc: 0.9682\n",
      "Epoch 804/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1073 - acc: 0.9695 - val_loss: 0.1776 - val_acc: 0.9679\n",
      "Epoch 805/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1065 - acc: 0.9692 - val_loss: 0.1771 - val_acc: 0.9689\n",
      "Epoch 806/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1074 - acc: 0.9695 - val_loss: 0.1813 - val_acc: 0.9689\n",
      "Epoch 807/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1081 - acc: 0.9692 - val_loss: 0.1799 - val_acc: 0.9682\n",
      "Epoch 808/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1068 - acc: 0.9695 - val_loss: 0.1829 - val_acc: 0.9692\n",
      "Epoch 809/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1088 - acc: 0.9692 - val_loss: 0.1743 - val_acc: 0.9694\n",
      "Epoch 810/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1076 - acc: 0.9697 - val_loss: 0.1846 - val_acc: 0.9679\n",
      "Epoch 811/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1081 - acc: 0.9695 - val_loss: 0.1694 - val_acc: 0.9694\n",
      "Epoch 812/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1089 - acc: 0.9694 - val_loss: 0.1632 - val_acc: 0.9692\n",
      "Epoch 813/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1080 - acc: 0.9692 - val_loss: 0.1718 - val_acc: 0.9687\n",
      "Epoch 814/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1078 - acc: 0.9693 - val_loss: 0.1764 - val_acc: 0.9692 0s - loss: 0.1\n",
      "Epoch 815/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1055 - acc: 0.9699 - val_loss: 0.1748 - val_acc: 0.9692\n",
      "Epoch 816/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1100 - acc: 0.9690 - val_loss: 0.1718 - val_acc: 0.9694\n",
      "Epoch 817/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1087 - acc: 0.9689 - val_loss: 0.1812 - val_acc: 0.9684\n",
      "Epoch 818/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1099 - acc: 0.9688 - val_loss: 0.1672 - val_acc: 0.9694\n",
      "Epoch 819/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1092 - acc: 0.9690 - val_loss: 0.1785 - val_acc: 0.9679\n",
      "Epoch 820/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1070 - acc: 0.9694 - val_loss: 0.1769 - val_acc: 0.9684\n",
      "Epoch 821/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1100 - acc: 0.9688 - val_loss: 0.1727 - val_acc: 0.9684\n",
      "Epoch 822/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1089 - acc: 0.9693 - val_loss: 0.1744 - val_acc: 0.9694\n",
      "Epoch 823/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1066 - acc: 0.9690 - val_loss: 0.1786 - val_acc: 0.9687\n",
      "Epoch 824/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1105 - acc: 0.9691 - val_loss: 0.1668 - val_acc: 0.9694\n",
      "Epoch 825/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1072 - acc: 0.9693 - val_loss: 0.1749 - val_acc: 0.9684\n",
      "Epoch 826/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1062 - acc: 0.9695 - val_loss: 0.1755 - val_acc: 0.9704\n",
      "Epoch 827/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1094 - acc: 0.9691 - val_loss: 0.1792 - val_acc: 0.9682\n",
      "Epoch 828/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1083 - acc: 0.9695 - val_loss: 0.1747 - val_acc: 0.9684\n",
      "Epoch 829/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1070 - acc: 0.9694 - val_loss: 0.1777 - val_acc: 0.9692\n",
      "Epoch 830/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1060 - acc: 0.9693 - val_loss: 0.1810 - val_acc: 0.9687\n",
      "Epoch 831/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1100 - acc: 0.9692 - val_loss: 0.1833 - val_acc: 0.9687\n",
      "Epoch 832/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1089 - acc: 0.9688 - val_loss: 0.1694 - val_acc: 0.9689\n",
      "Epoch 833/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1069 - acc: 0.9697 - val_loss: 0.1821 - val_acc: 0.9692\n",
      "Epoch 834/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1061 - acc: 0.9697 - val_loss: 0.1785 - val_acc: 0.9687\n",
      "Epoch 835/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1066 - acc: 0.9695 - val_loss: 0.1807 - val_acc: 0.9694\n",
      "Epoch 836/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1102 - acc: 0.9688 - val_loss: 0.1767 - val_acc: 0.9687\n",
      "Epoch 837/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1067 - acc: 0.9697 - val_loss: 0.1872 - val_acc: 0.9679\n",
      "Epoch 838/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1094 - acc: 0.9695 - val_loss: 0.1898 - val_acc: 0.9689\n",
      "Epoch 839/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1090 - acc: 0.9694 - val_loss: 0.1910 - val_acc: 0.9677\n",
      "Epoch 840/1000\n",
      "16079/16079 [==============================] - 1s 73us/step - loss: 0.1097 - acc: 0.9692 - val_loss: 0.1867 - val_acc: 0.9682\n",
      "Epoch 841/1000\n",
      "16079/16079 [==============================] - 1s 65us/step - loss: 0.1093 - acc: 0.9688 - val_loss: 0.1690 - val_acc: 0.9677\n",
      "Epoch 842/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1093 - acc: 0.9690 - val_loss: 0.1891 - val_acc: 0.9684\n",
      "Epoch 843/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1060 - acc: 0.9694 - val_loss: 0.1980 - val_acc: 0.9684\n",
      "Epoch 844/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1081 - acc: 0.9688 - val_loss: 0.1773 - val_acc: 0.9682\n",
      "Epoch 845/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1079 - acc: 0.9691 - val_loss: 0.1836 - val_acc: 0.9689\n",
      "Epoch 846/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1086 - acc: 0.9692 - val_loss: 0.1663 - val_acc: 0.9697\n",
      "Epoch 847/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1094 - acc: 0.9690 - val_loss: 0.1752 - val_acc: 0.9682\n",
      "Epoch 848/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1060 - acc: 0.9693 - val_loss: 0.1767 - val_acc: 0.9697\n",
      "Epoch 849/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1066 - acc: 0.9693 - val_loss: 0.1788 - val_acc: 0.9679\n",
      "Epoch 850/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1063 - acc: 0.9695 - val_loss: 0.1860 - val_acc: 0.9672\n",
      "Epoch 851/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1095 - acc: 0.9694 - val_loss: 0.1724 - val_acc: 0.9684\n",
      "Epoch 852/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1059 - acc: 0.9692 - val_loss: 0.1869 - val_acc: 0.9679\n",
      "Epoch 853/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1052 - acc: 0.9694 - val_loss: 0.1812 - val_acc: 0.9694\n",
      "Epoch 854/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1064 - acc: 0.9696 - val_loss: 0.1835 - val_acc: 0.9687\n",
      "Epoch 855/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1101 - acc: 0.9688 - val_loss: 0.1725 - val_acc: 0.9689\n",
      "Epoch 856/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1067 - acc: 0.9695 - val_loss: 0.1780 - val_acc: 0.9659\n",
      "Epoch 857/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1098 - acc: 0.9692 - val_loss: 0.1773 - val_acc: 0.9692\n",
      "Epoch 858/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1086 - acc: 0.9687 - val_loss: 0.1843 - val_acc: 0.9692\n",
      "Epoch 859/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1060 - acc: 0.9695 - val_loss: 0.1768 - val_acc: 0.9692\n",
      "Epoch 860/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1077 - acc: 0.9695 - val_loss: 0.1801 - val_acc: 0.9689\n",
      "Epoch 861/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1075 - acc: 0.9688 - val_loss: 0.1809 - val_acc: 0.9687\n",
      "Epoch 862/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1071 - acc: 0.9690 - val_loss: 0.1840 - val_acc: 0.9687\n",
      "Epoch 863/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1077 - acc: 0.9694 - val_loss: 0.1721 - val_acc: 0.9697\n",
      "Epoch 864/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1071 - acc: 0.9694 - val_loss: 0.1764 - val_acc: 0.9687\n",
      "Epoch 865/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1070 - acc: 0.9697 - val_loss: 0.1753 - val_acc: 0.9674\n",
      "Epoch 866/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1052 - acc: 0.9692 - val_loss: 0.1786 - val_acc: 0.9679\n",
      "Epoch 867/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1059 - acc: 0.9693 - val_loss: 0.1865 - val_acc: 0.9687\n",
      "Epoch 868/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1071 - acc: 0.9690 - val_loss: 0.1847 - val_acc: 0.9682\n",
      "Epoch 869/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1056 - acc: 0.9696 - val_loss: 0.1720 - val_acc: 0.9684\n",
      "Epoch 870/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1077 - acc: 0.9688 - val_loss: 0.1802 - val_acc: 0.9677\n",
      "Epoch 871/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1066 - acc: 0.9697 - val_loss: 0.1797 - val_acc: 0.9687\n",
      "Epoch 872/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1153 - acc: 0.9683 - val_loss: 0.1707 - val_acc: 0.9697\n",
      "Epoch 873/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1058 - acc: 0.9690 - val_loss: 0.1939 - val_acc: 0.9679\n",
      "Epoch 874/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1057 - acc: 0.9695 - val_loss: 0.1754 - val_acc: 0.9687\n",
      "Epoch 875/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1063 - acc: 0.9697 - val_loss: 0.1840 - val_acc: 0.9692\n",
      "Epoch 876/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1080 - acc: 0.9688 - val_loss: 0.1768 - val_acc: 0.9682\n",
      "Epoch 877/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1052 - acc: 0.9695 - val_loss: 0.1848 - val_acc: 0.9682\n",
      "Epoch 878/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1058 - acc: 0.9705 - val_loss: 0.1873 - val_acc: 0.9684\n",
      "Epoch 879/1000\n",
      "16079/16079 [==============================] - 2s 138us/step - loss: 0.1075 - acc: 0.9693 - val_loss: 0.1730 - val_acc: 0.9692\n",
      "Epoch 880/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1072 - acc: 0.9693 - val_loss: 0.1757 - val_acc: 0.9679\n",
      "Epoch 881/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1076 - acc: 0.9692 - val_loss: 0.1759 - val_acc: 0.9692\n",
      "Epoch 882/1000\n",
      "16079/16079 [==============================] - 2s 119us/step - loss: 0.1068 - acc: 0.9690 - val_loss: 0.1815 - val_acc: 0.9689\n",
      "Epoch 883/1000\n",
      "16079/16079 [==============================] - 2s 129us/step - loss: 0.1061 - acc: 0.9692 - val_loss: 0.1795 - val_acc: 0.9689\n",
      "Epoch 884/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1065 - acc: 0.9695 - val_loss: 0.1924 - val_acc: 0.9679\n",
      "Epoch 885/1000\n",
      "16079/16079 [==============================] - 2s 141us/step - loss: 0.1058 - acc: 0.9700 - val_loss: 0.1744 - val_acc: 0.9687\n",
      "Epoch 886/1000\n",
      "16079/16079 [==============================] - 2s 126us/step - loss: 0.1048 - acc: 0.9693 - val_loss: 0.1878 - val_acc: 0.9689\n",
      "Epoch 887/1000\n",
      "16079/16079 [==============================] - 2s 144us/step - loss: 0.1096 - acc: 0.9695 - val_loss: 0.1839 - val_acc: 0.9697\n",
      "Epoch 888/1000\n",
      "16079/16079 [==============================] - 2s 121us/step - loss: 0.1067 - acc: 0.9695 - val_loss: 0.1796 - val_acc: 0.9684\n",
      "Epoch 889/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1091 - acc: 0.9695 - val_loss: 0.1686 - val_acc: 0.9689\n",
      "Epoch 890/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1075 - acc: 0.9696 - val_loss: 0.1753 - val_acc: 0.9687\n",
      "Epoch 891/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1065 - acc: 0.9697 - val_loss: 0.1847 - val_acc: 0.9682\n",
      "Epoch 892/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1054 - acc: 0.9693 - val_loss: 0.1776 - val_acc: 0.9684\n",
      "Epoch 893/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1124 - acc: 0.9692 - val_loss: 0.1735 - val_acc: 0.9687\n",
      "Epoch 894/1000\n",
      "16079/16079 [==============================] - 2s 128us/step - loss: 0.1051 - acc: 0.9696 - val_loss: 0.1837 - val_acc: 0.9687\n",
      "Epoch 895/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1050 - acc: 0.9699 - val_loss: 0.1903 - val_acc: 0.9679\n",
      "Epoch 896/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1048 - acc: 0.9696 - val_loss: 0.1842 - val_acc: 0.9689\n",
      "Epoch 897/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1074 - acc: 0.9696 - val_loss: 0.1838 - val_acc: 0.9684\n",
      "Epoch 898/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1072 - acc: 0.9693 - val_loss: 0.1744 - val_acc: 0.9692\n",
      "Epoch 899/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1074 - acc: 0.9693 - val_loss: 0.1775 - val_acc: 0.9684\n",
      "Epoch 900/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1046 - acc: 0.9701 - val_loss: 0.1843 - val_acc: 0.9674\n",
      "Epoch 901/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1053 - acc: 0.9701 - val_loss: 0.1876 - val_acc: 0.9692\n",
      "Epoch 902/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1054 - acc: 0.9699 - val_loss: 0.1789 - val_acc: 0.9674\n",
      "Epoch 903/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1057 - acc: 0.9701 - val_loss: 0.1855 - val_acc: 0.9682\n",
      "Epoch 904/1000\n",
      "16079/16079 [==============================] - 1s 67us/step - loss: 0.1063 - acc: 0.9689 - val_loss: 0.1708 - val_acc: 0.9661\n",
      "Epoch 905/1000\n",
      "16079/16079 [==============================] - 1s 66us/step - loss: 0.1072 - acc: 0.9692 - val_loss: 0.1770 - val_acc: 0.9674\n",
      "Epoch 906/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1085 - acc: 0.9692 - val_loss: 0.1771 - val_acc: 0.9692\n",
      "Epoch 907/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1049 - acc: 0.9703 - val_loss: 0.1762 - val_acc: 0.9692\n",
      "Epoch 908/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1092 - acc: 0.9691 - val_loss: 0.1777 - val_acc: 0.9689\n",
      "Epoch 909/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1055 - acc: 0.9693 - val_loss: 0.1765 - val_acc: 0.9684\n",
      "Epoch 910/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1056 - acc: 0.9693 - val_loss: 0.1903 - val_acc: 0.9674\n",
      "Epoch 911/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1071 - acc: 0.9693 - val_loss: 0.1840 - val_acc: 0.9697\n",
      "Epoch 912/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1109 - acc: 0.9688 - val_loss: 0.1668 - val_acc: 0.9687\n",
      "Epoch 913/1000\n",
      "16079/16079 [==============================] - 1s 71us/step - loss: 0.1064 - acc: 0.9694 - val_loss: 0.1730 - val_acc: 0.9687\n",
      "Epoch 914/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1050 - acc: 0.9697 - val_loss: 0.1886 - val_acc: 0.9684\n",
      "Epoch 915/1000\n",
      "16079/16079 [==============================] - 1s 70us/step - loss: 0.1044 - acc: 0.9697 - val_loss: 0.1829 - val_acc: 0.9692\n",
      "Epoch 916/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1064 - acc: 0.9692 - val_loss: 0.1892 - val_acc: 0.9682\n",
      "Epoch 917/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1047 - acc: 0.9696 - val_loss: 0.1778 - val_acc: 0.9692\n",
      "Epoch 918/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1067 - acc: 0.9692 - val_loss: 0.1781 - val_acc: 0.9692\n",
      "Epoch 919/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1065 - acc: 0.9692 - val_loss: 0.1705 - val_acc: 0.9687\n",
      "Epoch 920/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1037 - acc: 0.9703 - val_loss: 0.1781 - val_acc: 0.9677\n",
      "Epoch 921/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1050 - acc: 0.9699 - val_loss: 0.1847 - val_acc: 0.9687\n",
      "Epoch 922/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1056 - acc: 0.9695 - val_loss: 0.1904 - val_acc: 0.9694\n",
      "Epoch 923/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1048 - acc: 0.9698 - val_loss: 0.1837 - val_acc: 0.9689\n",
      "Epoch 924/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1059 - acc: 0.9699 - val_loss: 0.1741 - val_acc: 0.9687\n",
      "Epoch 925/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1080 - acc: 0.9698 - val_loss: 0.1748 - val_acc: 0.9692\n",
      "Epoch 926/1000\n",
      "16079/16079 [==============================] - 1s 68us/step - loss: 0.1058 - acc: 0.9694 - val_loss: 0.1746 - val_acc: 0.9692\n",
      "Epoch 927/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1062 - acc: 0.9693 - val_loss: 0.1800 - val_acc: 0.9679\n",
      "Epoch 928/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1098 - acc: 0.9696 - val_loss: 0.1765 - val_acc: 0.9687\n",
      "Epoch 929/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1041 - acc: 0.9698 - val_loss: 0.1887 - val_acc: 0.9682\n",
      "Epoch 930/1000\n",
      "16079/16079 [==============================] - 1s 69us/step - loss: 0.1075 - acc: 0.9693 - val_loss: 0.1803 - val_acc: 0.9682\n",
      "Epoch 931/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1064 - acc: 0.9695 - val_loss: 0.1860 - val_acc: 0.9682\n",
      "Epoch 932/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1056 - acc: 0.9694 - val_loss: 0.1781 - val_acc: 0.9689\n",
      "Epoch 933/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1077 - acc: 0.9695 - val_loss: 0.1777 - val_acc: 0.9687\n",
      "Epoch 934/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1058 - acc: 0.9696 - val_loss: 0.1781 - val_acc: 0.9694\n",
      "Epoch 935/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1071 - acc: 0.9694 - val_loss: 0.1826 - val_acc: 0.9677\n",
      "Epoch 936/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1056 - acc: 0.9697 - val_loss: 0.1870 - val_acc: 0.9694\n",
      "Epoch 937/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1076 - acc: 0.9695 - val_loss: 0.1756 - val_acc: 0.9694\n",
      "Epoch 938/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1076 - acc: 0.9693 - val_loss: 0.1793 - val_acc: 0.9692\n",
      "Epoch 939/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1051 - acc: 0.9701 - val_loss: 0.1822 - val_acc: 0.9682\n",
      "Epoch 940/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1047 - acc: 0.9701 - val_loss: 0.1806 - val_acc: 0.9689\n",
      "Epoch 941/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1117 - acc: 0.9689 - val_loss: 0.1792 - val_acc: 0.9687\n",
      "Epoch 942/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1047 - acc: 0.9696 - val_loss: 0.1820 - val_acc: 0.9674\n",
      "Epoch 943/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1033 - acc: 0.9705 - val_loss: 0.1831 - val_acc: 0.9692\n",
      "Epoch 944/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1060 - acc: 0.9699 - val_loss: 0.1832 - val_acc: 0.9694\n",
      "Epoch 945/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1040 - acc: 0.9695 - val_loss: 0.1757 - val_acc: 0.9692\n",
      "Epoch 946/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1068 - acc: 0.9694 - val_loss: 0.1875 - val_acc: 0.9679\n",
      "Epoch 947/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1052 - acc: 0.9695 - val_loss: 0.1858 - val_acc: 0.9692\n",
      "Epoch 948/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1064 - acc: 0.9693 - val_loss: 0.1898 - val_acc: 0.9689\n",
      "Epoch 949/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1089 - acc: 0.9696 - val_loss: 0.1905 - val_acc: 0.9692\n",
      "Epoch 950/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1064 - acc: 0.9694 - val_loss: 0.1772 - val_acc: 0.9682\n",
      "Epoch 951/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1074 - acc: 0.9691 - val_loss: 0.1889 - val_acc: 0.9692\n",
      "Epoch 952/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1064 - acc: 0.9695 - val_loss: 0.1834 - val_acc: 0.9669\n",
      "Epoch 953/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1043 - acc: 0.9698 - val_loss: 0.1917 - val_acc: 0.9677\n",
      "Epoch 954/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1056 - acc: 0.9693 - val_loss: 0.1828 - val_acc: 0.9687\n",
      "Epoch 955/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1045 - acc: 0.9700 - val_loss: 0.1859 - val_acc: 0.9684\n",
      "Epoch 956/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1068 - acc: 0.9692 - val_loss: 0.1756 - val_acc: 0.9692\n",
      "Epoch 957/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1060 - acc: 0.9696 - val_loss: 0.1919 - val_acc: 0.9687\n",
      "Epoch 958/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1088 - acc: 0.9694 - val_loss: 0.1805 - val_acc: 0.9684\n",
      "Epoch 959/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1045 - acc: 0.9694 - val_loss: 0.1757 - val_acc: 0.9697\n",
      "Epoch 960/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1038 - acc: 0.9700 - val_loss: 0.1786 - val_acc: 0.9684\n",
      "Epoch 961/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1034 - acc: 0.9698 - val_loss: 0.1879 - val_acc: 0.9692\n",
      "Epoch 962/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1050 - acc: 0.9696 - val_loss: 0.1864 - val_acc: 0.9677\n",
      "Epoch 963/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1123 - acc: 0.9682 - val_loss: 0.1838 - val_acc: 0.9689\n",
      "Epoch 964/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1064 - acc: 0.9695 - val_loss: 0.1799 - val_acc: 0.9687\n",
      "Epoch 965/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1039 - acc: 0.9698 - val_loss: 0.1870 - val_acc: 0.9679\n",
      "Epoch 966/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1062 - acc: 0.9696 - val_loss: 0.1834 - val_acc: 0.9687\n",
      "Epoch 967/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1051 - acc: 0.9692 - val_loss: 0.1821 - val_acc: 0.9684\n",
      "Epoch 968/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1077 - acc: 0.9687 - val_loss: 0.1812 - val_acc: 0.9689\n",
      "Epoch 969/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1042 - acc: 0.9700 - val_loss: 0.1811 - val_acc: 0.9672\n",
      "Epoch 970/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1082 - acc: 0.9687 - val_loss: 0.1819 - val_acc: 0.9682\n",
      "Epoch 971/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1044 - acc: 0.9695 - val_loss: 0.1770 - val_acc: 0.9694\n",
      "Epoch 972/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1044 - acc: 0.9692 - val_loss: 0.1817 - val_acc: 0.9694\n",
      "Epoch 973/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1040 - acc: 0.9700 - val_loss: 0.1883 - val_acc: 0.9677\n",
      "Epoch 974/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1095 - acc: 0.9688 - val_loss: 0.1835 - val_acc: 0.9689\n",
      "Epoch 975/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1050 - acc: 0.9698 - val_loss: 0.1845 - val_acc: 0.9689\n",
      "Epoch 976/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1070 - acc: 0.9696 - val_loss: 0.1909 - val_acc: 0.9679\n",
      "Epoch 977/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1069 - acc: 0.9693 - val_loss: 0.1862 - val_acc: 0.9694\n",
      "Epoch 978/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1036 - acc: 0.9698 - val_loss: 0.1884 - val_acc: 0.9687\n",
      "Epoch 979/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1064 - acc: 0.9698 - val_loss: 0.1785 - val_acc: 0.9684\n",
      "Epoch 980/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1049 - acc: 0.9696 - val_loss: 0.1844 - val_acc: 0.9687\n",
      "Epoch 981/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1039 - acc: 0.9702 - val_loss: 0.1936 - val_acc: 0.9687\n",
      "Epoch 982/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1080 - acc: 0.9692 - val_loss: 0.1749 - val_acc: 0.9684\n",
      "Epoch 983/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1079 - acc: 0.9690 - val_loss: 0.1692 - val_acc: 0.9682\n",
      "Epoch 984/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1072 - acc: 0.9690 - val_loss: 0.1756 - val_acc: 0.9694\n",
      "Epoch 985/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1055 - acc: 0.9695 - val_loss: 0.1720 - val_acc: 0.9692\n",
      "Epoch 986/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1049 - acc: 0.9696 - val_loss: 0.1832 - val_acc: 0.9699\n",
      "Epoch 987/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1038 - acc: 0.9698 - val_loss: 0.1834 - val_acc: 0.9684\n",
      "Epoch 988/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1094 - acc: 0.9690 - val_loss: 0.1668 - val_acc: 0.9684\n",
      "Epoch 989/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1141 - acc: 0.9679 - val_loss: 0.1722 - val_acc: 0.9677\n",
      "Epoch 990/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1053 - acc: 0.9693 - val_loss: 0.1743 - val_acc: 0.9687\n",
      "Epoch 991/1000\n",
      "16079/16079 [==============================] - 2s 93us/step - loss: 0.1060 - acc: 0.9695 - val_loss: 0.1673 - val_acc: 0.9679\n",
      "Epoch 992/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1060 - acc: 0.9696 - val_loss: 0.1945 - val_acc: 0.9682\n",
      "Epoch 993/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1059 - acc: 0.9693 - val_loss: 0.1763 - val_acc: 0.9694\n",
      "Epoch 994/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1040 - acc: 0.9688 - val_loss: 0.1903 - val_acc: 0.9674\n",
      "Epoch 995/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1062 - acc: 0.9692 - val_loss: 0.1848 - val_acc: 0.9682\n",
      "Epoch 996/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1068 - acc: 0.9695 - val_loss: 0.1781 - val_acc: 0.9689\n",
      "Epoch 997/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1068 - acc: 0.9695 - val_loss: 0.1701 - val_acc: 0.9677\n",
      "Epoch 998/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1054 - acc: 0.9697 - val_loss: 0.1723 - val_acc: 0.9694\n",
      "Epoch 999/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1056 - acc: 0.9696 - val_loss: 0.2018 - val_acc: 0.9669\n",
      "Epoch 1000/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1078 - acc: 0.9691 - val_loss: 0.1760 - val_acc: 0.9677\n",
      "1302.175019979477  seconds\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "# Takes about 5230.725239038467  seconds About 1 and 1/2 hours for 1000 epochs\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "    train_X,\n",
    "    # train_X_no_vectors, \n",
    "    train_y, \n",
    "    epochs=1000,\n",
    "    # validation_split=0.4\n",
    "    validation_data=(validation_X, validation_y)\n",
    "    \n",
    ")\n",
    "end = time.time()\n",
    "print(end-start, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_values = history.history['loss']\n",
    "epochs = range(1, len(loss_values)+1)\n",
    "\n",
    "plt.plot(epochs, loss_values, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/akashsri99/deep-learning-iris-dataset-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_predictions = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.5856280e-01, 2.4143717e-01],\n",
       "       [9.9895239e-01, 1.0475651e-03],\n",
       "       [9.5686299e-01, 4.3136995e-02],\n",
       "       [1.0000000e+00, 0.0000000e+00],\n",
       "       [9.8542529e-01, 1.4574680e-02],\n",
       "       [1.0000000e+00, 2.1737091e-13],\n",
       "       [9.9662328e-01, 3.3767601e-03],\n",
       "       [8.2829630e-01, 1.7170373e-01],\n",
       "       [8.6662048e-01, 1.3337952e-01],\n",
       "       [9.9992788e-01, 7.2172195e-05]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_class = np.argmax(test_y,axis=1)\n",
    "y_pred_class = np.argmax(test_y_predictions,axis=1)\n",
    "\n",
    "y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_class[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(648,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_test_class.shape == y_pred_class.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced classification problem\n",
    "https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/\n",
    "This happens because Machine Learning Algorithms are usually designed to improve accuracy by reducing the error. Thus, they do not take into account the class distribution / proportion or balance of classes.\n",
    "\n",
    "[A Review of Class Imbalance Problem (Shaza M. Abd Elrahman and Ajith Abraham)](http://ias04.softcomputing.net/jnic2.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       619\n",
      "           1       0.50      0.03      0.06        29\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       648\n",
      "   macro avg       0.73      0.52      0.52       648\n",
      "weighted avg       0.94      0.96      0.94       648\n",
      "\n",
      "[[618   1]\n",
      " [ 28   1]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_class,y_pred_class))\n",
    "print(confusion_matrix(y_test_class,y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARIES\n",
      "---------\n",
      " btw, why are ou guys calling this abstraction thingy GComm when Dave has a project called GNU Comm (GComm fr short)?\n",
      "as far as I'm concerned, GComm is our internal package name... to the external world, it's GNUe Common\n",
      "but that is a good point\n",
      "pyro is an object system like what gcomm will be\n",
      "by same guys that wrote pygmy\n",
      "the python email client\n",
      "I need a production quality GNUe web shopping cart ;-)\n",
      "anything is possible I know, but ideally we need to get ideas on some sort of php and GEAS interface\n",
      "guess I'll just customize interchange.. hopefully actually have it done in a few days\n",
      "our inventory package isn't completed\n",
      "but if you have an inventory package\n",
      "you should be able to access it via geas\n",
      "I know a web interface for GNUe Forms is in the works, but I'm sure you need something relatively quickly\n",
      "madlocke was working on that, but as you probably know, he's been out of commission (sick) lately\n",
      "a new release (feature wise) is probably about 3 or 4 weeks away since the db upgrade is going to be huge\n",
      "I may make an interim bug fix/small feature release to get some of the email support down\n",
      "but we are seeing a GREAT value in DCL as a communication tool to customers\n",
      "as well as a billing tool\n",
      "and todo tool\n",
      "the todo tool works fine\n",
      "but the communication tool is flawed\n",
      "if the accounts dont have same products\n",
      "it should be fairly easy to implement who can view what\n",
      "by product\n",
      "but then the next step becomes billing (or services)\n",
      "at which point products and services need to be 'separate'\n",
      "but that was because i didnt see dcl as a communications tool\n",
      "your use of services sounds like it could be an action\n",
      "as I said, I am going to be working on this stuff (minus billing) for work, so it will come\n",
      "\n",
      "PREDICTIONS\n",
      "-----------\n",
      " a new release (feature wise) is probably about 3 or 4 weeks away since the db upgrade is going to be huge\n",
      "plz, I dun wanna hear it until I experience it I will just say you are a bunch of whiners :P\n",
      "\n",
      "ROUGE Scores\n",
      "------------\n",
      "Evaluation with Avg\n",
      "\trouge-1:\tP: 0.340909\tR: 0.147059\tF1: 0.205479\n",
      "\trouge-2:\tP: 0.000000\tR: 0.000000\tF1: 0.000000\n",
      "\trouge-3:\tP: 0.000000\tR: 0.000000\tF1: 0.000000\n",
      "\trouge-4:\tP: 0.000000\tR: 0.000000\tF1: 0.000000\n",
      "\trouge-l:\tP: 0.407880\tR: 0.202415\tF1: 0.270561\n",
      "\trouge-w:\tP: 0.224232\tR: 0.057268\tF1: 0.091235\n"
     ]
    }
   ],
   "source": [
    "from rouge_metrics import *\n",
    "from get_sentences_from_line_numbers import *\n",
    "\n",
    "predicted_line_numbers = [index_for_validation_test_split + index + 1 for index, value in enumerate(y_pred_class) if value==1]\n",
    "summaries_line_numbers = [index_for_validation_test_split + index + 1 for index, value in enumerate(y_test_class) if value==1]\n",
    "\n",
    "summaries_chat_lines = get_sentences_of_line_numbers(CHAT_LOGS, summaries_line_numbers)\n",
    "predicted_chat_lines = get_sentences_of_line_numbers(CHAT_LOGS, predicted_line_numbers)\n",
    "print(\"SUMMARIES\\n---------\\n\", summaries_chat_lines)\n",
    "print(\"PREDICTIONS\\n-----------\\n\", predicted_chat_lines)\n",
    "\n",
    "hypotheses = [predicted_chat_lines]\n",
    "references = [summaries_chat_lines]\n",
    "print(\"ROUGE Scores\\n------------\")\n",
    "print_rouge_results(get_rouge_results(hypotheses, references))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Recurrent Neural Network Instead\n",
    "It seems that adding sentence vectors as windows around the sentence does not work well with a feed forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "218\n",
      "(25000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 2,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 2,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 2,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 2,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 2,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(len(X_train[0]))\n",
    "print(X_test.shape)\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   19,  178,   32],\n",
       "       [   0,    0,    0, ...,   16,  145,   95],\n",
       "       [   0,    0,    0, ...,    7,  129,  113],\n",
       "       [ 687,   23,    4, ...,   21,   64, 2574],\n",
       "       [   0,    0,    0, ...,    7,   61,  113]], dtype=int32)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    1,  591,  202,   14,   31,    6,  717,   10,\n",
       "         10,    2,    2,    5,    4,  360,    7,    4,  177,    2,  394,\n",
       "        354,    4,  123,    9, 1035, 1035, 1035,   10,   10,   13,   92,\n",
       "        124,   89,  488,    2,  100,   28, 1668,   14,   31,   23,   27,\n",
       "          2,   29,  220,  468,    8,  124,   14,  286,  170,    8,  157,\n",
       "         46,    5,   27,  239,   16,  179,    2,   38,   32,   25,    2,\n",
       "        451,  202,   14,    6,  717], dtype=int32)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_36 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "imdb_model = Sequential()\n",
    "imdb_model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "imdb_model.add(LSTM(100))\n",
    "imdb_model.add(Dense(1, activation='sigmoid'))\n",
    "imdb_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(imdb_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 204s 8ms/step - loss: 0.5887 - acc: 0.6728 - val_loss: 0.5178 - val_acc: 0.7478\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 193s 8ms/step - loss: 0.3717 - acc: 0.8384 - val_loss: 0.3916 - val_acc: 0.8476\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 195s 8ms/step - loss: 0.2686 - acc: 0.8935 - val_loss: 0.3383 - val_acc: 0.8628\n"
     ]
    }
   ],
   "source": [
    "imdb_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb_loss_values = imdb_history.history['loss']\n",
    "imdb_epochs = range(1, len(imdb_loss_values)+1)\n",
    "\n",
    "plt.plot(imdb_epochs, imdb_loss_values, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16079, 156)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0.006050</td>\n",
       "      <td>0.788992</td>\n",
       "      <td>8.923999e-08</td>\n",
       "      <td>-7.957217e-09</td>\n",
       "      <td>-5.274117e-09</td>\n",
       "      <td>-1.910836e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>5.032557e-08</td>\n",
       "      <td>4.709798e-09</td>\n",
       "      <td>-7.814833e-08</td>\n",
       "      <td>3.333242e-08</td>\n",
       "      <td>-1.757239e-08</td>\n",
       "      <td>-4.967683e-09</td>\n",
       "      <td>3.275183e-08</td>\n",
       "      <td>-4.288362e-08</td>\n",
       "      <td>2.537755e-09</td>\n",
       "      <td>2.285525e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.026734</td>\n",
       "      <td>0.042729</td>\n",
       "      <td>1.138727e-09</td>\n",
       "      <td>-1.818176e-08</td>\n",
       "      <td>4.798956e-09</td>\n",
       "      <td>-5.157600e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>3.880125e-08</td>\n",
       "      <td>-1.505285e-08</td>\n",
       "      <td>-3.228048e-08</td>\n",
       "      <td>6.559005e-08</td>\n",
       "      <td>2.989873e-08</td>\n",
       "      <td>-2.563377e-08</td>\n",
       "      <td>-3.230809e-08</td>\n",
       "      <td>-1.410832e-08</td>\n",
       "      <td>3.297067e-08</td>\n",
       "      <td>8.902228e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.031086</td>\n",
       "      <td>0.051728</td>\n",
       "      <td>-4.177528e-09</td>\n",
       "      <td>-1.188295e-08</td>\n",
       "      <td>2.389462e-08</td>\n",
       "      <td>-1.349990e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>2.331976e-08</td>\n",
       "      <td>-2.124629e-08</td>\n",
       "      <td>-5.314322e-09</td>\n",
       "      <td>4.871865e-08</td>\n",
       "      <td>2.572267e-08</td>\n",
       "      <td>-1.130713e-08</td>\n",
       "      <td>-3.515117e-08</td>\n",
       "      <td>2.479115e-08</td>\n",
       "      <td>9.735289e-09</td>\n",
       "      <td>3.229467e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001743</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.027952</td>\n",
       "      <td>0.142905</td>\n",
       "      <td>-2.529559e-09</td>\n",
       "      <td>-1.465508e-08</td>\n",
       "      <td>8.994467e-09</td>\n",
       "      <td>8.784771e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.564207e-08</td>\n",
       "      <td>1.737575e-09</td>\n",
       "      <td>2.070995e-08</td>\n",
       "      <td>-6.362434e-10</td>\n",
       "      <td>-2.201059e-08</td>\n",
       "      <td>2.865113e-08</td>\n",
       "      <td>-7.716492e-09</td>\n",
       "      <td>-3.310467e-08</td>\n",
       "      <td>-9.572791e-09</td>\n",
       "      <td>-1.052344e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>0.017715</td>\n",
       "      <td>0.084493</td>\n",
       "      <td>-2.828828e-10</td>\n",
       "      <td>4.331747e-10</td>\n",
       "      <td>8.529943e-11</td>\n",
       "      <td>-2.281351e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.957617e-11</td>\n",
       "      <td>-1.465501e-10</td>\n",
       "      <td>1.223374e-10</td>\n",
       "      <td>1.708342e-10</td>\n",
       "      <td>3.674844e-10</td>\n",
       "      <td>1.448876e-10</td>\n",
       "      <td>6.751768e-10</td>\n",
       "      <td>1.813555e-10</td>\n",
       "      <td>6.316711e-10</td>\n",
       "      <td>-4.996572e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   absolute_sentence_position  sentence_length  number_of_special_terms  \\\n",
       "0                    0.000436         0.013699                      0.0   \n",
       "1                    0.000871         0.178082                      0.0   \n",
       "2                    0.001307         0.136986                      0.0   \n",
       "3                    0.001743         0.068493                      0.0   \n",
       "4                    0.002179         0.109589                      0.0   \n",
       "\n",
       "   sentiment_score  normalized_mean_tf_idf  normalized_mean_tf_isf  \\\n",
       "0           0.6249                0.006050                0.788992   \n",
       "1           0.0000                0.026734                0.042729   \n",
       "2           0.0000                0.031086                0.051728   \n",
       "3           0.0000                0.027952                0.142905   \n",
       "4           0.2263                0.017715                0.084493   \n",
       "\n",
       "              1             2             3             4  ...           141  \\\n",
       "0  8.923999e-08 -7.957217e-09 -5.274117e-09 -1.910836e-08  ...  5.032557e-08   \n",
       "1  1.138727e-09 -1.818176e-08  4.798956e-09 -5.157600e-08  ...  3.880125e-08   \n",
       "2 -4.177528e-09 -1.188295e-08  2.389462e-08 -1.349990e-08  ...  2.331976e-08   \n",
       "3 -2.529559e-09 -1.465508e-08  8.994467e-09  8.784771e-09  ...  2.564207e-08   \n",
       "4 -2.828828e-10  4.331747e-10  8.529943e-11 -2.281351e-10  ...  2.957617e-11   \n",
       "\n",
       "            142           143           144           145           146  \\\n",
       "0  4.709798e-09 -7.814833e-08  3.333242e-08 -1.757239e-08 -4.967683e-09   \n",
       "1 -1.505285e-08 -3.228048e-08  6.559005e-08  2.989873e-08 -2.563377e-08   \n",
       "2 -2.124629e-08 -5.314322e-09  4.871865e-08  2.572267e-08 -1.130713e-08   \n",
       "3  1.737575e-09  2.070995e-08 -6.362434e-10 -2.201059e-08  2.865113e-08   \n",
       "4 -1.465501e-10  1.223374e-10  1.708342e-10  3.674844e-10  1.448876e-10   \n",
       "\n",
       "            147           148           149           150  \n",
       "0  3.275183e-08 -4.288362e-08  2.537755e-09  2.285525e-08  \n",
       "1 -3.230809e-08 -1.410832e-08  3.297067e-08  8.902228e-08  \n",
       "2 -3.515117e-08  2.479115e-08  9.735289e-09  3.229467e-08  \n",
       "3 -7.716492e-09 -3.310467e-08 -9.572791e-09 -1.052344e-08  \n",
       "4  6.751768e-10  1.813555e-10  6.316711e-10 -4.996572e-10  \n",
       "\n",
       "[5 rows x 156 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A many to many RNN\n",
    "#get number of columns in training data\n",
    "embedding_vector_length = 32\n",
    "num_of_cols = train_vectors_df.shape[1]\n",
    "rnn_model = Sequential()\n",
    "# rnn_model.add(Dense(num_of_cols, activation='relu', input_shape=(num_of_cols, 1)))\n",
    "rnn_model.add(Embedding(train_vectors_df.shape[0], embedding_vector_length, input_length=num_of_cols))\n",
    "rnn_model.add(Dropout(0.2))\n",
    "rnn_model.add(LSTM(100))\n",
    "rnn_model.add(Dropout(0.2))\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 150, 32)           514528    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 150, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_51 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 567,829\n",
      "Trainable params: 567,829\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16079 samples, validate on 3988 samples\n",
      "Epoch 1/10\n",
      "16079/16079 [==============================] - 35s 2ms/step - loss: 0.1416 - acc: 0.9684 - val_loss: 0.1325 - val_acc: 0.9709\n",
      "Epoch 2/10\n",
      "16079/16079 [==============================] - 36s 2ms/step - loss: 0.1414 - acc: 0.9684 - val_loss: 0.1323 - val_acc: 0.9709\n",
      "Epoch 3/10\n",
      "16079/16079 [==============================] - 36s 2ms/step - loss: 0.1413 - acc: 0.9684 - val_loss: 0.1333 - val_acc: 0.9709\n",
      "Epoch 4/10\n",
      "16079/16079 [==============================] - 36s 2ms/step - loss: 0.1415 - acc: 0.9684 - val_loss: 0.1316 - val_acc: 0.9709\n",
      "Epoch 5/10\n",
      "16079/16079 [==============================] - 35s 2ms/step - loss: 0.1413 - acc: 0.9684 - val_loss: 0.1316 - val_acc: 0.9709\n",
      "Epoch 6/10\n",
      "16079/16079 [==============================] - 36s 2ms/step - loss: 0.1410 - acc: 0.9684 - val_loss: 0.1316 - val_acc: 0.9709\n",
      "Epoch 7/10\n",
      "16079/16079 [==============================] - 36s 2ms/step - loss: 0.1409 - acc: 0.9684 - val_loss: 0.1321 - val_acc: 0.9709\n",
      "Epoch 8/10\n",
      "16079/16079 [==============================] - 36s 2ms/step - loss: 0.1411 - acc: 0.9684 - val_loss: 0.1316 - val_acc: 0.9709\n",
      "Epoch 9/10\n",
      "16079/16079 [==============================] - 36s 2ms/step - loss: 0.1416 - acc: 0.9684 - val_loss: 0.1318 - val_acc: 0.9709\n",
      "Epoch 10/10\n",
      "16079/16079 [==============================] - 36s 2ms/step - loss: 0.1411 - acc: 0.9684 - val_loss: 0.1323 - val_acc: 0.9709\n",
      "357.93049907684326 seconds\n"
     ]
    }
   ],
   "source": [
    "# rnn_history = rnn_model.fit(train_X, train_y, epochs=3, batch_size=64)\n",
    "rnn_start = time.time()\n",
    "rnn_history = rnn_model.fit(\n",
    "    train_vectors_df, \n",
    "    train_y_nums, \n",
    "    validation_data=(validation_vectors_df, validation_y_nums), \n",
    "    epochs=10, \n",
    "    batch_size=64\n",
    ")\n",
    "rnn_end = time.time()\n",
    "print(rnn_end - rnn_start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rnn_loss_values = rnn_history.history['loss']\n",
    "rnn_epochs = range(1, len(rnn_loss_values)+1)\n",
    "\n",
    "plt.plot(rnn_epochs, rnn_loss_values, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_test_y_predictions = rnn_model.predict(validation_vectors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03626591],\n",
       "       [0.03626591],\n",
       "       [0.03626591],\n",
       "       ...,\n",
       "       [0.03626592],\n",
       "       [0.03626592],\n",
       "       [0.03626592]], dtype=float32)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_test_y_predictions[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3988,)\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "rnn_y_test_class = np.argmax(test_y,axis=1)\n",
    "rnn_y_pred_class = np.argmax(rnn_test_y_predictions,axis=1)\n",
    "\n",
    "assert y_test_class.shape == y_pred_class.shape\n",
    "print(rnn_y_pred_class.shape)\n",
    "print(rnn_y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      3872\n",
      "           1       0.00      0.00      0.00       116\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      3988\n",
      "   macro avg       0.49      0.50      0.49      3988\n",
      "weighted avg       0.94      0.97      0.96      3988\n",
      "\n",
      "[[3872    0]\n",
      " [ 116    0]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_y_nums,rnn_y_pred_class))\n",
    "print(confusion_matrix(validation_y_nums,rnn_y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16079 samples, validate on 3988 samples\n",
      "Epoch 1/100\n",
      " 9728/16079 [=================>............] - ETA: 14s - loss: 0.1351 - acc: 0.9704"
     ]
    }
   ],
   "source": [
    "# rnn_history = rnn_model.fit(train_X, train_y, epochs=3, batch_size=64)\n",
    "rnn_start = time.time()\n",
    "rnn_history = rnn_model.fit(\n",
    "    train_X, \n",
    "    train_y_nums, \n",
    "    validation_data=(validation_X, validation_y_nums), \n",
    "    epochs=100, \n",
    "    batch_size=64\n",
    ")\n",
    "rnn_end = time.time()\n",
    "print(rnn_end - rnn_start, \"seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
