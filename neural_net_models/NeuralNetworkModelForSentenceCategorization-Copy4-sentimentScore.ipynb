{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Model for Sentence Categorization\n",
    "\n",
    "A Neural Network model to help classify sentences as important, or not important enough to be part of a summary\n",
    "\n",
    "Built using Keras\n",
    "\n",
    "https://towardsdatascience.com/building-a-deep-learning-model-using-keras-1548ca149d37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_DIR = join(\"..\", \"feature_extraction\", \"feature_outputs\")\n",
    "DATA_FILES_DIR = join(\"..\", \"feature_extraction\", \"data_files\")\n",
    "sentence_vectors_filename = join(FEATURES_DIR, \"sentence_vectors.csv\")\n",
    "summarized_chat_log_ids_filename = join(DATA_FILES_DIR, \"summarized_chat_log_ids.csv\")\n",
    "summarized_chat_date_partitions_filename = join(\n",
    "    DATA_FILES_DIR, \"summarized_chat_date_partitions_cumulative_count.csv\"\n",
    ")\n",
    "concatenated_vectors_filename = join(DATA_FILES_DIR, \"summarized_concatenated_vectors_window_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data using pandas. This takes a couple of seconds\n",
    "# sentence_vectors_df = pd.read_csv(sentence_vectors_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sentence_vectors_df.shape)\n",
    "# sentence_vectors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_chat_log_ids = pd.read_csv(\n",
    "    summarized_chat_log_ids_filename,\n",
    "    names = [\"log_id\", \"is_summary\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20715, 2)\n",
      "   log_id  is_summary\n",
      "0   85350           0\n",
      "1   85351           0\n",
      "2   85352           0\n",
      "3   85353           0\n",
      "4   85354           0\n",
      "       log_id  is_summary\n",
      "20710  624000           0\n",
      "20711  624001           0\n",
      "20712  624002           0\n",
      "20713  624003           0\n",
      "20714  624004           0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 85350,  85351,  85352, ..., 624002, 624003, 624004])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(summarized_chat_log_ids.shape)\n",
    "print(summarized_chat_log_ids.head())\n",
    "print(summarized_chat_log_ids.tail())\n",
    "summarized_chat_log_ids_array = np.array(summarized_chat_log_ids.log_id)\n",
    "summarized_chat_log_ids_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarized_sentence_vectors_df = sentence_vectors_df.filter(summarized_chat_log_ids_array, axis=0)\n",
    "# summarized_num_of_columns = summarized_sentence_vectors_df.shape[0]\n",
    "# # summarized_sentence_vectors_df[\"index\"] = [num for num in range(summarized_num_of_columns)]\n",
    "# # summarized_sentence_vectors_df.set_index([\"index\"])\n",
    "# # summarized_sentence_vectors_df.reset_index\n",
    "# # summarized_sentence_vectors_df.insert(0, \"index\", range(summarized_num_of_columns))\n",
    "# print(summarized_sentence_vectors_df.shape)\n",
    "# summarized_sentence_vectors_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_chat_date_partitions = pd.read_csv(summarized_chat_date_partitions_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_log_id</th>\n",
       "      <th>date_of_log</th>\n",
       "      <th>chat_line_count</th>\n",
       "      <th>cumulative_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>473197</td>\n",
       "      <td>2001-11-07</td>\n",
       "      <td>1990</td>\n",
       "      <td>16079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>495175</td>\n",
       "      <td>2001-11-13</td>\n",
       "      <td>1051</td>\n",
       "      <td>17130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>497259</td>\n",
       "      <td>2001-11-14</td>\n",
       "      <td>536</td>\n",
       "      <td>17666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>526307</td>\n",
       "      <td>2001-11-15</td>\n",
       "      <td>1053</td>\n",
       "      <td>18719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>531627</td>\n",
       "      <td>2001-11-12</td>\n",
       "      <td>1348</td>\n",
       "      <td>20067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>579591</td>\n",
       "      <td>2001-10-24</td>\n",
       "      <td>162</td>\n",
       "      <td>20229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>586206</td>\n",
       "      <td>2001-10-23</td>\n",
       "      <td>165</td>\n",
       "      <td>20394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>623684</td>\n",
       "      <td>2001-10-25</td>\n",
       "      <td>321</td>\n",
       "      <td>20715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    min_log_id date_of_log  chat_line_count  cumulative_count\n",
       "19      473197  2001-11-07             1990             16079\n",
       "20      495175  2001-11-13             1051             17130\n",
       "21      497259  2001-11-14              536             17666\n",
       "22      526307  2001-11-15             1053             18719\n",
       "23      531627  2001-11-12             1348             20067\n",
       "24      579591  2001-10-24              162             20229\n",
       "25      586206  2001-10-23              165             20394\n",
       "26      623684  2001-10-25              321             20715"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_chat_date_partitions.tail(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the first chat log id for the third last chat date. Use the last three chat log dates for testing. The rest to be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20067"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_for_validation_test_split = summarized_chat_date_partitions.tail(4)[\"cumulative_count\"]\n",
    "index_for_validation_test_split = index_for_validation_test_split.values[0]\n",
    "index_for_train_validation_split = summarized_chat_date_partitions.tail(8)[\"cumulative_count\"]\n",
    "index_for_train_validation_split = index_for_train_validation_split.values[0]\n",
    "print(index_for_train_validation_split)\n",
    "index_for_validation_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate sentence vectors for context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_sentence_vectors(data_values, window):\n",
    "    number_of_rows, number_of_columns = data_values.shape\n",
    "    eventual_number_of_columns = number_of_columns +(number_of_columns * window * 2)\n",
    "    new_np_data = np.array([]).reshape(0, eventual_number_of_columns)\n",
    "    \n",
    "    for index in range(data_values.shape[0]):\n",
    "        first_index = index - window\n",
    "        last_index = index + window\n",
    "        start_padding = []\n",
    "        end_padding = []\n",
    "        \n",
    "        if first_index < 0:\n",
    "            start_padding = np.zeros((abs(first_index), number_of_columns))\n",
    "            first_index = 0\n",
    "        if last_index >= number_of_rows:\n",
    "            end_padding = np.zeros((((last_index - number_of_rows) + 1), number_of_columns))\n",
    "            last_index = number_of_rows - 1\n",
    "            \n",
    "        concatenated_row = data_values[first_index:last_index+1]\n",
    "        # print(\"\\n\", index, \"\\n\", concatenated_row, start_padding, \"\\n\\n\")\n",
    "        if len(start_padding) > 0:\n",
    "            # print(start_padding.shape, data_values[first_index: first_index+1].shape)\n",
    "            concatenated_row = np.concatenate((start_padding, concatenated_row))\n",
    "        if len(end_padding) > 0:\n",
    "            # print(end_padding.shape, data_values[first_index: first_index+1].shape)\n",
    "            concatenated_row = np.concatenate((concatenated_row, end_padding))\n",
    "        concatenated_row = concatenated_row.ravel()\n",
    "        # print(index, concatenated_row, \"\\n\\n\")\n",
    "        # print(concatenated_row.shape, new_np_data.shape)\n",
    "        new_np_data = np.concatenate((new_np_data, concatenated_row))\n",
    "    return pd.DataFrame(new_np_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.719329833984375e-05\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# summarized_sentence_vectors_array = summarized_sentence_vectors_df.values\n",
    "context_window = 5\n",
    "\n",
    "# This takes about 40 minutes, or 2386.799 seconds\n",
    "# concatenated_sentence_vectors_df = concatenate_sentence_vectors(summarized_sentence_vectors_array, context_window)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated_sentence_vectors_df = pd.read_csv(concatenated_vectors_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated_sentence_vectors_df = concatenated_sentence_vectors_df.drop(columns=[0])\n",
    "# concatenated_sentence_vectors_df.to_csv(concatenated_vectors_filename)\n",
    "# concatenated_sentence_vectors_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated_sentence_vectors_df.tail(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated_sentence_vectors_df = concatenated_sentence_vectors_df.drop(columns=[\"Unnamed: 0\"])\n",
    "# concatenated_sentence_vectors_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(summarized_sentence_vectors_df.shape)\n",
    "# summarized_sentence_vectors_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_DATA_FILE = join(FEATURES_DIR, \"summarized_chats_features.csv\")\n",
    "# read in data using pandas\n",
    "unnormalized_chat_log_df = pd.read_csv(FEATURES_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_id</th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>mean_tf_isf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "      <th>is_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85350</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.006050</td>\n",
       "      <td>2.758723</td>\n",
       "      <td>0.788992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85351</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.003486</td>\n",
       "      <td>0.026734</td>\n",
       "      <td>0.149401</td>\n",
       "      <td>0.042729</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85352</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.004054</td>\n",
       "      <td>0.031086</td>\n",
       "      <td>0.180867</td>\n",
       "      <td>0.051728</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85353</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.003645</td>\n",
       "      <td>0.027952</td>\n",
       "      <td>0.499669</td>\n",
       "      <td>0.142905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85354</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>0.002310</td>\n",
       "      <td>0.017715</td>\n",
       "      <td>0.295432</td>\n",
       "      <td>0.084493</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   log_id  absolute_sentence_position  sentence_length  \\\n",
       "0   85350                    0.000436                1   \n",
       "1   85351                    0.000871               13   \n",
       "2   85352                    0.001307               10   \n",
       "3   85353                    0.001743                5   \n",
       "4   85354                    0.002179                8   \n",
       "\n",
       "   number_of_special_terms  sentiment_score  mean_tf_idf  \\\n",
       "0                        0           0.6249     0.000789   \n",
       "1                        0           0.0000     0.003486   \n",
       "2                        0           0.0000     0.004054   \n",
       "3                        0           0.0000     0.003645   \n",
       "4                        0           0.2263     0.002310   \n",
       "\n",
       "   normalized_mean_tf_idf  mean_tf_isf  normalized_mean_tf_isf  is_summary  \n",
       "0                0.006050     2.758723                0.788992           0  \n",
       "1                0.026734     0.149401                0.042729           0  \n",
       "2                0.031086     0.180867                0.051728           0  \n",
       "3                0.027952     0.499669                0.142905           0  \n",
       "4                0.017715     0.295432                0.084493           0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unnormalized_chat_log_df.tail()\n",
    "# unnormalized_chat_log_df.insert(0, \"index\", range(unnormalized_chat_log_df.shape[0]))\n",
    "# print(unnormalized_chat_log_df.loc[0:5])\n",
    "unnormalized_chat_log_df.sentiment_score = abs(unnormalized_chat_log_df.sentiment_score)\n",
    "unnormalized_chat_log_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnormalized_merged_log_data_df = pd.merge(\n",
    "#     unnormalized_chat_log_df, \n",
    "#     summarized_sentence_vectors_df, \n",
    "#     left_on=\"log_id\",\n",
    "#     right_index=True\n",
    "# )\n",
    "# print(unnormalized_merged_log_data_df.shape)\n",
    "# unnormalized_merged_log_data_df.head()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing The Data\n",
    "Formula to normalize data\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.9707\n",
      "0.00043572984749455336\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.04073392898973263\n",
      "3.36078268987328\n",
      "0.3123448778466237\n",
      "0.9611808193277522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_data_max_values = unnormalized_chat_log_df.max()\n",
    "chat_data_min_values = unnormalized_chat_log_df.min()\n",
    "\n",
    "max_number_of_special_terms = chat_data_max_values.number_of_special_terms\n",
    "max_sentence_length = chat_data_max_values.sentence_length\n",
    "max_sentiment_score = chat_data_max_values.sentiment_score\n",
    "max_absolute_sentence_position = chat_data_max_values.absolute_sentence_position\n",
    "max_mean_tf_idf = chat_data_max_values.mean_tf_idf\n",
    "max_mean_tf_isf = chat_data_max_values.mean_tf_isf\n",
    "max_normalized_mean_tf_idf = chat_data_max_values.normalized_mean_tf_idf\n",
    "max_normalized_mean_tf_isf = chat_data_max_values.normalized_mean_tf_isf\n",
    "\n",
    "min_number_of_special_terms = chat_data_min_values.number_of_special_terms\n",
    "min_sentence_length = chat_data_min_values.sentence_length\n",
    "min_sentiment_score = chat_data_min_values.sentiment_score\n",
    "min_absolute_sentence_position = chat_data_min_values.absolute_sentence_position\n",
    "min_mean_tf_idf = chat_data_min_values.mean_tf_idf\n",
    "min_mean_tf_isf = chat_data_min_values.mean_tf_isf\n",
    "min_normalized_mean_tf_idf = chat_data_min_values.normalized_mean_tf_idf\n",
    "min_normalized_mean_tf_isf = chat_data_min_values.normalized_mean_tf_isf\n",
    "\n",
    "print(min_sentiment_score)\n",
    "print(max_sentiment_score)\n",
    "print(min_absolute_sentence_position)\n",
    "print(max_absolute_sentence_position)\n",
    "print(min_mean_tf_idf)\n",
    "print(min_mean_tf_isf)\n",
    "print(min_normalized_mean_tf_idf)\n",
    "print(min_normalized_mean_tf_isf)\n",
    "print(max_mean_tf_idf)\n",
    "print(max_mean_tf_isf)\n",
    "print(max_normalized_mean_tf_idf)\n",
    "print(max_normalized_mean_tf_isf)\n",
    "max_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.013699\n",
      "1    0.178082\n",
      "2    0.136986\n",
      "3    0.068493\n",
      "4    0.109589\n",
      "Name: sentence_length, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_id</th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>mean_tf_isf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "      <th>is_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.643762</td>\n",
       "      <td>0.019369</td>\n",
       "      <td>0.019369</td>\n",
       "      <td>0.820857</td>\n",
       "      <td>0.820857</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85351</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085591</td>\n",
       "      <td>0.085591</td>\n",
       "      <td>0.044454</td>\n",
       "      <td>0.044454</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85352</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099525</td>\n",
       "      <td>0.099525</td>\n",
       "      <td>0.053817</td>\n",
       "      <td>0.053817</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85353</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089492</td>\n",
       "      <td>0.089492</td>\n",
       "      <td>0.148677</td>\n",
       "      <td>0.148677</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85354</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.233131</td>\n",
       "      <td>0.056715</td>\n",
       "      <td>0.056715</td>\n",
       "      <td>0.087906</td>\n",
       "      <td>0.087906</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   log_id  absolute_sentence_position  sentence_length  \\\n",
       "0   85350                    0.000000         0.013699   \n",
       "1   85351                    0.000436         0.178082   \n",
       "2   85352                    0.000872         0.136986   \n",
       "3   85353                    0.001308         0.068493   \n",
       "4   85354                    0.001744         0.109589   \n",
       "\n",
       "   number_of_special_terms  sentiment_score  mean_tf_idf  \\\n",
       "0                      0.0         0.643762     0.019369   \n",
       "1                      0.0         0.000000     0.085591   \n",
       "2                      0.0         0.000000     0.099525   \n",
       "3                      0.0         0.000000     0.089492   \n",
       "4                      0.0         0.233131     0.056715   \n",
       "\n",
       "   normalized_mean_tf_idf  mean_tf_isf  normalized_mean_tf_isf  is_summary  \n",
       "0                0.019369     0.820857                0.820857           0  \n",
       "1                0.085591     0.044454                0.044454           0  \n",
       "2                0.099525     0.053817                0.053817           0  \n",
       "3                0.089492     0.148677                0.148677           0  \n",
       "4                0.056715     0.087906                0.087906           0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_log_df = unnormalized_chat_log_df.copy()\n",
    "chat_log_df.sentence_length = (\n",
    "    chat_log_df.sentence_length - min_sentence_length) / (\n",
    "    max_sentence_length - min_sentence_length)\n",
    "chat_log_df.number_of_special_terms = (\n",
    "    chat_log_df.number_of_special_terms - min_number_of_special_terms) / (\n",
    "    max_number_of_special_terms - min_number_of_special_terms)\n",
    "chat_log_df.sentiment_score = (\n",
    "    chat_log_df.sentiment_score - min_sentiment_score) / (\n",
    "    max_sentiment_score - min_sentiment_score)\n",
    "chat_log_df.absolute_sentence_position = (\n",
    "    chat_log_df.absolute_sentence_position - min_absolute_sentence_position) / (\n",
    "    max_absolute_sentence_position - min_absolute_sentence_position)\n",
    "chat_log_df.mean_tf_idf = (\n",
    "    chat_log_df.mean_tf_idf - min_mean_tf_idf) / (\n",
    "    max_mean_tf_idf - min_mean_tf_idf)\n",
    "chat_log_df.mean_tf_isf = (\n",
    "    chat_log_df.mean_tf_isf - min_mean_tf_isf) / (\n",
    "    max_mean_tf_isf - min_mean_tf_isf)\n",
    "chat_log_df.normalized_mean_tf_idf = (\n",
    "    chat_log_df.normalized_mean_tf_idf - min_normalized_mean_tf_idf) / (\n",
    "    max_normalized_mean_tf_idf - min_normalized_mean_tf_idf)\n",
    "chat_log_df.normalized_mean_tf_isf = (\n",
    "    chat_log_df.normalized_mean_tf_isf - min_normalized_mean_tf_isf) / (\n",
    "    max_normalized_mean_tf_isf - min_normalized_mean_tf_isf)\n",
    "\n",
    "print(chat_log_df.sentence_length.head())\n",
    "chat_log_df.iloc[0:5]\n",
    "# chat_log_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_id</th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>mean_tf_isf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "      <th>is_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16074</th>\n",
       "      <td>475182</td>\n",
       "      <td>0.997989</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.208406</td>\n",
       "      <td>0.081768</td>\n",
       "      <td>0.081768</td>\n",
       "      <td>0.059544</td>\n",
       "      <td>0.059544</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16075</th>\n",
       "      <td>475183</td>\n",
       "      <td>0.998492</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052944</td>\n",
       "      <td>0.052944</td>\n",
       "      <td>0.712859</td>\n",
       "      <td>0.712859</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16076</th>\n",
       "      <td>475184</td>\n",
       "      <td>0.998995</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.233131</td>\n",
       "      <td>0.050432</td>\n",
       "      <td>0.050432</td>\n",
       "      <td>0.269761</td>\n",
       "      <td>0.269761</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16077</th>\n",
       "      <td>475185</td>\n",
       "      <td>0.999497</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.656124</td>\n",
       "      <td>0.110526</td>\n",
       "      <td>0.110526</td>\n",
       "      <td>0.522136</td>\n",
       "      <td>0.522136</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16078</th>\n",
       "      <td>475186</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.230761</td>\n",
       "      <td>0.096316</td>\n",
       "      <td>0.096316</td>\n",
       "      <td>0.080129</td>\n",
       "      <td>0.080129</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       log_id  absolute_sentence_position  sentence_length  \\\n",
       "16074  475182                    0.997989         0.136986   \n",
       "16075  475183                    0.998492         0.013699   \n",
       "16076  475184                    0.998995         0.027397   \n",
       "16077  475185                    0.999497         0.013699   \n",
       "16078  475186                    1.000000         0.082192   \n",
       "\n",
       "       number_of_special_terms  sentiment_score  mean_tf_idf  \\\n",
       "16074                      0.0         0.208406     0.081768   \n",
       "16075                      0.0         0.000000     0.052944   \n",
       "16076                      0.0         0.233131     0.050432   \n",
       "16077                      0.0         0.656124     0.110526   \n",
       "16078                      0.0         0.230761     0.096316   \n",
       "\n",
       "       normalized_mean_tf_idf  mean_tf_isf  normalized_mean_tf_isf  is_summary  \n",
       "16074                0.081768     0.059544                0.059544           0  \n",
       "16075                0.052944     0.712859                0.712859           0  \n",
       "16076                0.050432     0.269761                0.269761           0  \n",
       "16077                0.110526     0.522136                0.522136           0  \n",
       "16078                0.096316     0.080129                0.080129           0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = chat_log_df.iloc[:index_for_train_validation_split]\n",
    "validation_df = chat_log_df.iloc[index_for_train_validation_split:index_for_validation_test_split]\n",
    "test_df = chat_log_df.iloc[index_for_validation_test_split:]\n",
    "train_df.tail()\n",
    "\n",
    "# Sentence Vectors only\n",
    "# train_vectors_df = summarized_sentence_vectors_df.iloc[:index_for_train_validation_split]\n",
    "# validation_vectors_df = summarized_sentence_vectors_df.iloc[index_for_train_validation_split:index_for_validation_test_split]\n",
    "# test_vectors_df = summarized_sentence_vectors_df.iloc[index_for_validation_test_split:]\n",
    "# train_vectors_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_id</th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>mean_tf_isf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "      <th>is_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16079</th>\n",
       "      <td>495175</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472649</td>\n",
       "      <td>0.050304</td>\n",
       "      <td>0.050304</td>\n",
       "      <td>0.051808</td>\n",
       "      <td>0.051808</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16080</th>\n",
       "      <td>495176</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>0.899077</td>\n",
       "      <td>0.899077</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16081</th>\n",
       "      <td>495177</td>\n",
       "      <td>0.002420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16082</th>\n",
       "      <td>495178</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040161</td>\n",
       "      <td>0.040161</td>\n",
       "      <td>0.063344</td>\n",
       "      <td>0.063344</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16083</th>\n",
       "      <td>495179</td>\n",
       "      <td>0.004324</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472649</td>\n",
       "      <td>0.038080</td>\n",
       "      <td>0.038080</td>\n",
       "      <td>0.131802</td>\n",
       "      <td>0.131802</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       log_id  absolute_sentence_position  sentence_length  \\\n",
       "16079  495175                    0.000516         0.150685   \n",
       "16080  495176                    0.001468         0.013699   \n",
       "16081  495177                    0.002420         0.000000   \n",
       "16082  495178                    0.003372         0.136986   \n",
       "16083  495179                    0.004324         0.054795   \n",
       "\n",
       "       number_of_special_terms  sentiment_score  mean_tf_idf  \\\n",
       "16079                 0.000000         0.472649     0.050304   \n",
       "16080                 0.000000         0.000000     0.020250   \n",
       "16081                 0.000000         0.472649     0.000000   \n",
       "16082                 0.054054         0.000000     0.040161   \n",
       "16083                 0.000000         0.472649     0.038080   \n",
       "\n",
       "       normalized_mean_tf_idf  mean_tf_isf  normalized_mean_tf_isf  is_summary  \n",
       "16079                0.050304     0.051808                0.051808           0  \n",
       "16080                0.020250     0.899077                0.899077           0  \n",
       "16081                0.000000     0.000000                0.000000           0  \n",
       "16082                0.040161     0.063344                0.063344           0  \n",
       "16083                0.038080     0.131802                0.131802           0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_id</th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>mean_tf_isf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "      <th>is_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20062</th>\n",
       "      <td>532970</td>\n",
       "      <td>0.997031</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043630</td>\n",
       "      <td>0.043630</td>\n",
       "      <td>0.931238</td>\n",
       "      <td>0.931238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20063</th>\n",
       "      <td>532971</td>\n",
       "      <td>0.997774</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099908</td>\n",
       "      <td>0.099908</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0.230029</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20064</th>\n",
       "      <td>532972</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.542598</td>\n",
       "      <td>0.057936</td>\n",
       "      <td>0.057936</td>\n",
       "      <td>0.752096</td>\n",
       "      <td>0.752096</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20065</th>\n",
       "      <td>532973</td>\n",
       "      <td>0.999258</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257546</td>\n",
       "      <td>0.065927</td>\n",
       "      <td>0.065927</td>\n",
       "      <td>0.105421</td>\n",
       "      <td>0.105421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20066</th>\n",
       "      <td>532974</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022086</td>\n",
       "      <td>0.022086</td>\n",
       "      <td>0.340556</td>\n",
       "      <td>0.340556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       log_id  absolute_sentence_position  sentence_length  \\\n",
       "20062  532970                    0.997031         0.013699   \n",
       "20063  532971                    0.997774         0.041096   \n",
       "20064  532972                    0.998516         0.013699   \n",
       "20065  532973                    0.999258         0.068493   \n",
       "20066  532974                    1.000000         0.027397   \n",
       "\n",
       "       number_of_special_terms  sentiment_score  mean_tf_idf  \\\n",
       "20062                      0.0         0.000000     0.043630   \n",
       "20063                      0.0         0.000000     0.099908   \n",
       "20064                      0.0         0.542598     0.057936   \n",
       "20065                      0.0         0.257546     0.065927   \n",
       "20066                      0.0         0.000000     0.022086   \n",
       "\n",
       "       normalized_mean_tf_idf  mean_tf_isf  normalized_mean_tf_isf  is_summary  \n",
       "20062                0.043630     0.931238                0.931238           0  \n",
       "20063                0.099908     0.230029                0.230029           0  \n",
       "20064                0.057936     0.752096                0.752096           0  \n",
       "20065                0.065927     0.105421                0.105421           0  \n",
       "20066                0.022086     0.340556                0.340556           0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_id</th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>mean_tf_isf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "      <th>is_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20067</th>\n",
       "      <td>579591</td>\n",
       "      <td>0.005740</td>\n",
       "      <td>0.232877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.617493</td>\n",
       "      <td>0.014957</td>\n",
       "      <td>0.014957</td>\n",
       "      <td>0.029834</td>\n",
       "      <td>0.029834</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20068</th>\n",
       "      <td>579592</td>\n",
       "      <td>0.011915</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304935</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.328720</td>\n",
       "      <td>0.328720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20069</th>\n",
       "      <td>579593</td>\n",
       "      <td>0.018091</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304935</td>\n",
       "      <td>0.010781</td>\n",
       "      <td>0.010781</td>\n",
       "      <td>0.191207</td>\n",
       "      <td>0.191207</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20070</th>\n",
       "      <td>579594</td>\n",
       "      <td>0.024266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.453693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071</th>\n",
       "      <td>579595</td>\n",
       "      <td>0.030442</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013585</td>\n",
       "      <td>0.013585</td>\n",
       "      <td>0.283935</td>\n",
       "      <td>0.283935</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       log_id  absolute_sentence_position  sentence_length  \\\n",
       "20067  579591                    0.005740         0.232877   \n",
       "20068  579592                    0.011915         0.027397   \n",
       "20069  579593                    0.018091         0.041096   \n",
       "20070  579594                    0.024266         0.000000   \n",
       "20071  579595                    0.030442         0.027397   \n",
       "\n",
       "       number_of_special_terms  sentiment_score  mean_tf_idf  \\\n",
       "20067                      0.0         0.617493     0.014957   \n",
       "20068                      0.0         0.304935     0.002195   \n",
       "20069                      0.0         0.304935     0.010781   \n",
       "20070                      0.0         0.453693     0.000000   \n",
       "20071                      0.0         0.000000     0.013585   \n",
       "\n",
       "       normalized_mean_tf_idf  mean_tf_isf  normalized_mean_tf_isf  is_summary  \n",
       "20067                0.014957     0.029834                0.029834           0  \n",
       "20068                0.002195     0.328720                0.328720           0  \n",
       "20069                0.010781     0.191207                0.191207           0  \n",
       "20070                0.000000     0.000000                0.000000           0  \n",
       "20071                0.013585     0.283935                0.283935           0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_id</th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>mean_tf_isf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "      <th>is_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20710</th>\n",
       "      <td>624000</td>\n",
       "      <td>0.987534</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010594</td>\n",
       "      <td>0.010594</td>\n",
       "      <td>0.292627</td>\n",
       "      <td>0.292627</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20711</th>\n",
       "      <td>624001</td>\n",
       "      <td>0.990650</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019673</td>\n",
       "      <td>0.019673</td>\n",
       "      <td>0.132364</td>\n",
       "      <td>0.132364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20712</th>\n",
       "      <td>624002</td>\n",
       "      <td>0.993767</td>\n",
       "      <td>0.095890</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.542598</td>\n",
       "      <td>0.027545</td>\n",
       "      <td>0.027545</td>\n",
       "      <td>0.091081</td>\n",
       "      <td>0.091081</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20713</th>\n",
       "      <td>624003</td>\n",
       "      <td>0.996883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20714</th>\n",
       "      <td>624004</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019725</td>\n",
       "      <td>0.019725</td>\n",
       "      <td>0.656239</td>\n",
       "      <td>0.656239</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       log_id  absolute_sentence_position  sentence_length  \\\n",
       "20710  624000                    0.987534         0.027397   \n",
       "20711  624001                    0.990650         0.054795   \n",
       "20712  624002                    0.993767         0.095890   \n",
       "20713  624003                    0.996883         0.000000   \n",
       "20714  624004                    1.000000         0.013699   \n",
       "\n",
       "       number_of_special_terms  sentiment_score  mean_tf_idf  \\\n",
       "20710                      0.0         0.000000     0.010594   \n",
       "20711                      0.0         0.000000     0.019673   \n",
       "20712                      0.0         0.542598     0.027545   \n",
       "20713                      0.0         0.472649     0.000000   \n",
       "20714                      0.0         0.000000     0.019725   \n",
       "\n",
       "       normalized_mean_tf_idf  mean_tf_isf  normalized_mean_tf_isf  is_summary  \n",
       "20710                0.010594     0.292627                0.292627           0  \n",
       "20711                0.019673     0.132364                0.132364           0  \n",
       "20712                0.027545     0.091081                0.091081           0  \n",
       "20713                0.000000     0.000000                0.000000           0  \n",
       "20714                0.019725     0.656239                0.656239           0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data has been read in properly\n",
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16079, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019369</td>\n",
       "      <td>0.820857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085591</td>\n",
       "      <td>0.044454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099525</td>\n",
       "      <td>0.053817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089492</td>\n",
       "      <td>0.148677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056715</td>\n",
       "      <td>0.087906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   absolute_sentence_position  sentence_length  number_of_special_terms  \\\n",
       "0                    0.000000         0.013699                      0.0   \n",
       "1                    0.000436         0.178082                      0.0   \n",
       "2                    0.000872         0.136986                      0.0   \n",
       "3                    0.001308         0.068493                      0.0   \n",
       "4                    0.001744         0.109589                      0.0   \n",
       "\n",
       "   normalized_mean_tf_idf  normalized_mean_tf_isf  \n",
       "0                0.019369                0.820857  \n",
       "1                0.085591                0.044454  \n",
       "2                0.099525                0.053817  \n",
       "3                0.089492                0.148677  \n",
       "4                0.056715                0.087906  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a dataframe with all training data except the target column\n",
    "columns_to_drop = [\"log_id\", \"is_summary\", \"mean_tf_idf\", \"mean_tf_isf\", \n",
    "                   \"sentiment_score\"\n",
    "                  ]\n",
    "# Keep only normalized columns\n",
    "train_X = train_df.drop(columns=columns_to_drop)\n",
    "validation_X = validation_df.drop(columns=columns_to_drop)\n",
    "test_X = test_df.drop(columns=columns_to_drop)\n",
    "\n",
    "assert train_X.shape[1] == test_X.shape[1] and test_X.shape[1] == validation_X.shape[1] \n",
    "#check that the target variable has been removed\n",
    "print(train_X.shape)\n",
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_columns = [str(num) for num in range(1, 151)]\n",
    "# train_X_no_vectors = train_X.drop(columns=vector_columns)\n",
    "# validation_X_no_vectors = validation_X.drop(columns=vector_columns)\n",
    "# test_X_no_vectors = test_X.drop(columns=vector_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_X_no_vectors.shape)\n",
    "# assert train_X_no_vectors.shape[1] == validation_X_no_vectors.shape[1] \n",
    "# assert validation_X_no_vectors.shape[1] == test_X_no_vectors.shape[1]\n",
    "# train_X_no_vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3988, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16079</th>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050304</td>\n",
       "      <td>0.051808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16080</th>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>0.899077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16081</th>\n",
       "      <td>0.002420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16082</th>\n",
       "      <td>0.003372</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.040161</td>\n",
       "      <td>0.063344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16083</th>\n",
       "      <td>0.004324</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038080</td>\n",
       "      <td>0.131802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       absolute_sentence_position  sentence_length  number_of_special_terms  \\\n",
       "16079                    0.000516         0.150685                 0.000000   \n",
       "16080                    0.001468         0.013699                 0.000000   \n",
       "16081                    0.002420         0.000000                 0.000000   \n",
       "16082                    0.003372         0.136986                 0.054054   \n",
       "16083                    0.004324         0.054795                 0.000000   \n",
       "\n",
       "       normalized_mean_tf_idf  normalized_mean_tf_isf  \n",
       "16079                0.050304                0.051808  \n",
       "16080                0.020250                0.899077  \n",
       "16081                0.000000                0.000000  \n",
       "16082                0.040161                0.063344  \n",
       "16083                0.038080                0.131802  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(validation_X.shape)\n",
    "validation_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(648, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20067</th>\n",
       "      <td>0.005740</td>\n",
       "      <td>0.232877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014957</td>\n",
       "      <td>0.029834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20068</th>\n",
       "      <td>0.011915</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.328720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20069</th>\n",
       "      <td>0.018091</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010781</td>\n",
       "      <td>0.191207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20070</th>\n",
       "      <td>0.024266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071</th>\n",
       "      <td>0.030442</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013585</td>\n",
       "      <td>0.283935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       absolute_sentence_position  sentence_length  number_of_special_terms  \\\n",
       "20067                    0.005740         0.232877                      0.0   \n",
       "20068                    0.011915         0.027397                      0.0   \n",
       "20069                    0.018091         0.041096                      0.0   \n",
       "20070                    0.024266         0.000000                      0.0   \n",
       "20071                    0.030442         0.027397                      0.0   \n",
       "\n",
       "       normalized_mean_tf_idf  normalized_mean_tf_isf  \n",
       "20067                0.014957                0.029834  \n",
       "20068                0.002195                0.328720  \n",
       "20069                0.010781                0.191207  \n",
       "20070                0.000000                0.000000  \n",
       "20071                0.013585                0.283935  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_X.shape)\n",
    "test_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.is_summary.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When separating the target column, we need to call the `to_categorical()` function so that column will be `one-hot encoded`. Currently, a chat line that is not a summary is represented with a `0` in the `is_summary` column and a chat line that is a summary is represented with a `1`. With one-hot encoding, the integer will be removed and a binary variable is inputted for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a one hot encoding for the target column\n",
    "train_y = to_categorical(train_df.is_summary)\n",
    "train_y_nums = train_df.is_summary.values\n",
    "validation_y = to_categorical(validation_df.is_summary)\n",
    "validation_y_nums = validation_df.is_summary.values\n",
    "test_y = to_categorical(test_df.is_summary)\n",
    "test_y_nums = test_df.is_summary.values\n",
    "# view one hot encoding numpy array\n",
    "print(train_y_nums)\n",
    "train_y[415: 425]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16079, 2)\n",
      "(3988, 2)\n",
      "(648, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_y.shape)\n",
    "print(validation_y.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation is `softmax`. Softmax makes the output sum up to `1` so the output can be interpreted as probabilities. The model will then make its prediction based on which option has a higher probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create model\n",
    "model = Sequential()\n",
    "\n",
    "#get number of columns in training data\n",
    "# n_cols = train_vectors_df.shape[1]\n",
    "n_cols = train_X.shape[1]\n",
    "\n",
    "#add model layers\n",
    "model.add(Dense(n_cols, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(300, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50)                300       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 5,532\n",
      "Trainable params: 5,532\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `categorical_crossentropy` for our loss function. This is the most common choice for classification. A lower score indicates that the model is performing better.\n",
    "\n",
    "We will use the ‘mean absolute error’ metric to see the score on the validation set at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model using mse as a measure of model performance\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set early stopping monitor so the model stops training when it won't improve anymore\n",
    "# early_stopping_monitor = EarlyStopping(patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16079 samples, validate on 3988 samples\n",
      "Epoch 1/1000\n",
      "16079/16079 [==============================] - 3s 215us/step - loss: 0.1645 - acc: 0.9642 - val_loss: 0.1322 - val_acc: 0.9709\n",
      "Epoch 2/1000\n",
      "16079/16079 [==============================] - 2s 121us/step - loss: 0.1355 - acc: 0.9684 - val_loss: 0.1288 - val_acc: 0.9709\n",
      "Epoch 3/1000\n",
      "16079/16079 [==============================] - 2s 138us/step - loss: 0.1336 - acc: 0.9684 - val_loss: 0.1335 - val_acc: 0.9709\n",
      "Epoch 4/1000\n",
      "16079/16079 [==============================] - 2s 118us/step - loss: 0.1331 - acc: 0.9684 - val_loss: 0.1293 - val_acc: 0.9709\n",
      "Epoch 5/1000\n",
      "16079/16079 [==============================] - 2s 134us/step - loss: 0.1324 - acc: 0.9684 - val_loss: 0.1303 - val_acc: 0.9709\n",
      "Epoch 6/1000\n",
      "16079/16079 [==============================] - 2s 135us/step - loss: 0.1328 - acc: 0.9684 - val_loss: 0.1313 - val_acc: 0.9709\n",
      "Epoch 7/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1320 - acc: 0.9684 - val_loss: 0.1297 - val_acc: 0.9709\n",
      "Epoch 8/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1318 - acc: 0.9684 - val_loss: 0.1312 - val_acc: 0.9709\n",
      "Epoch 9/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1314 - acc: 0.9684 - val_loss: 0.1306 - val_acc: 0.9709\n",
      "Epoch 10/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1316 - acc: 0.9684 - val_loss: 0.1315 - val_acc: 0.9709\n",
      "Epoch 11/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1315 - acc: 0.9684 - val_loss: 0.1320 - val_acc: 0.9709\n",
      "Epoch 12/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1313 - acc: 0.9684 - val_loss: 0.1333 - val_acc: 0.9709\n",
      "Epoch 13/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1315 - acc: 0.9684 - val_loss: 0.1310 - val_acc: 0.9709\n",
      "Epoch 14/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1311 - acc: 0.9684 - val_loss: 0.1326 - val_acc: 0.9709\n",
      "Epoch 15/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1309 - acc: 0.9684 - val_loss: 0.1307 - val_acc: 0.9709\n",
      "Epoch 16/1000\n",
      "16079/16079 [==============================] - 2s 126us/step - loss: 0.1311 - acc: 0.9684 - val_loss: 0.1332 - val_acc: 0.9709\n",
      "Epoch 17/1000\n",
      "16079/16079 [==============================] - 2s 133us/step - loss: 0.1310 - acc: 0.9684 - val_loss: 0.1314 - val_acc: 0.9709\n",
      "Epoch 18/1000\n",
      "16079/16079 [==============================] - 2s 134us/step - loss: 0.1309 - acc: 0.9684 - val_loss: 0.1298 - val_acc: 0.9709\n",
      "Epoch 19/1000\n",
      "16079/16079 [==============================] - 2s 122us/step - loss: 0.1307 - acc: 0.9684 - val_loss: 0.1313 - val_acc: 0.9709\n",
      "Epoch 20/1000\n",
      "16079/16079 [==============================] - 2s 127us/step - loss: 0.1309 - acc: 0.9684 - val_loss: 0.1303 - val_acc: 0.9709\n",
      "Epoch 21/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1312 - acc: 0.9684 - val_loss: 0.1303 - val_acc: 0.9709\n",
      "Epoch 22/1000\n",
      "16079/16079 [==============================] - 2s 124us/step - loss: 0.1309 - acc: 0.9684 - val_loss: 0.1308 - val_acc: 0.9709\n",
      "Epoch 23/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1307 - acc: 0.9684 - val_loss: 0.1305 - val_acc: 0.9709\n",
      "Epoch 24/1000\n",
      "16079/16079 [==============================] - 2s 118us/step - loss: 0.1307 - acc: 0.9684 - val_loss: 0.1305 - val_acc: 0.9709\n",
      "Epoch 25/1000\n",
      "16079/16079 [==============================] - 2s 120us/step - loss: 0.1303 - acc: 0.9684 - val_loss: 0.1297 - val_acc: 0.9709\n",
      "Epoch 26/1000\n",
      "16079/16079 [==============================] - 2s 112us/step - loss: 0.1308 - acc: 0.9684 - val_loss: 0.1302 - val_acc: 0.9709\n",
      "Epoch 27/1000\n",
      "16079/16079 [==============================] - 2s 137us/step - loss: 0.1302 - acc: 0.9684 - val_loss: 0.1326 - val_acc: 0.9709\n",
      "Epoch 28/1000\n",
      "16079/16079 [==============================] - 2s 125us/step - loss: 0.1302 - acc: 0.9684 - val_loss: 0.1305 - val_acc: 0.9709\n",
      "Epoch 29/1000\n",
      "16079/16079 [==============================] - 2s 129us/step - loss: 0.1302 - acc: 0.9684 - val_loss: 0.1314 - val_acc: 0.9709\n",
      "Epoch 30/1000\n",
      "16079/16079 [==============================] - 2s 115us/step - loss: 0.1298 - acc: 0.9684 - val_loss: 0.1301 - val_acc: 0.9709\n",
      "Epoch 31/1000\n",
      "16079/16079 [==============================] - 2s 122us/step - loss: 0.1299 - acc: 0.9684 - val_loss: 0.1310 - val_acc: 0.9709\n",
      "Epoch 32/1000\n",
      "16079/16079 [==============================] - 2s 128us/step - loss: 0.1299 - acc: 0.9684 - val_loss: 0.1303 - val_acc: 0.9709\n",
      "Epoch 33/1000\n",
      "16079/16079 [==============================] - 2s 128us/step - loss: 0.1301 - acc: 0.9684 - val_loss: 0.1322 - val_acc: 0.9709\n",
      "Epoch 34/1000\n",
      "16079/16079 [==============================] - 2s 120us/step - loss: 0.1298 - acc: 0.9684 - val_loss: 0.1305 - val_acc: 0.9709\n",
      "Epoch 35/1000\n",
      "16079/16079 [==============================] - 2s 120us/step - loss: 0.1301 - acc: 0.9684 - val_loss: 0.1295 - val_acc: 0.9709\n",
      "Epoch 36/1000\n",
      "16079/16079 [==============================] - 2s 130us/step - loss: 0.1295 - acc: 0.9684 - val_loss: 0.1301 - val_acc: 0.9709\n",
      "Epoch 37/1000\n",
      "16079/16079 [==============================] - 2s 125us/step - loss: 0.1293 - acc: 0.9684 - val_loss: 0.1327 - val_acc: 0.9709\n",
      "Epoch 38/1000\n",
      "16079/16079 [==============================] - 2s 120us/step - loss: 0.1297 - acc: 0.9684 - val_loss: 0.1317 - val_acc: 0.9709\n",
      "Epoch 39/1000\n",
      "16079/16079 [==============================] - 2s 112us/step - loss: 0.1293 - acc: 0.9684 - val_loss: 0.1312 - val_acc: 0.9709\n",
      "Epoch 40/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1290 - acc: 0.9684 - val_loss: 0.1324 - val_acc: 0.9709\n",
      "Epoch 41/1000\n",
      "16079/16079 [==============================] - 2s 93us/step - loss: 0.1299 - acc: 0.9684 - val_loss: 0.1329 - val_acc: 0.9709\n",
      "Epoch 42/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1296 - acc: 0.9684 - val_loss: 0.1307 - val_acc: 0.9709\n",
      "Epoch 43/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1291 - acc: 0.9684 - val_loss: 0.1318 - val_acc: 0.9709\n",
      "Epoch 44/1000\n",
      "16079/16079 [==============================] - 2s 122us/step - loss: 0.1294 - acc: 0.9684 - val_loss: 0.1312 - val_acc: 0.9709\n",
      "Epoch 45/1000\n",
      "16079/16079 [==============================] - 2s 134us/step - loss: 0.1291 - acc: 0.9684 - val_loss: 0.1307 - val_acc: 0.9709\n",
      "Epoch 46/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1293 - acc: 0.9684 - val_loss: 0.1314 - val_acc: 0.9709\n",
      "Epoch 47/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1293 - acc: 0.9684 - val_loss: 0.1307 - val_acc: 0.9709\n",
      "Epoch 48/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1287 - acc: 0.9684 - val_loss: 0.1314 - val_acc: 0.9709\n",
      "Epoch 49/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1291 - acc: 0.9684 - val_loss: 0.1307 - val_acc: 0.9709: 0s - loss: 0.1274 - acc: \n",
      "Epoch 50/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1291 - acc: 0.9684 - val_loss: 0.1323 - val_acc: 0.9709\n",
      "Epoch 51/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1283 - acc: 0.9684 - val_loss: 0.1307 - val_acc: 0.9709\n",
      "Epoch 52/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1290 - acc: 0.9684 - val_loss: 0.1315 - val_acc: 0.9709\n",
      "Epoch 53/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1291 - acc: 0.9684 - val_loss: 0.1329 - val_acc: 0.9709\n",
      "Epoch 54/1000\n",
      "16079/16079 [==============================] - 2s 115us/step - loss: 0.1285 - acc: 0.9684 - val_loss: 0.1310 - val_acc: 0.9709\n",
      "Epoch 55/1000\n",
      "16079/16079 [==============================] - 2s 112us/step - loss: 0.1285 - acc: 0.9684 - val_loss: 0.1311 - val_acc: 0.9709\n",
      "Epoch 56/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1285 - acc: 0.9684 - val_loss: 0.1329 - val_acc: 0.9709\n",
      "Epoch 57/1000\n",
      "16079/16079 [==============================] - 2s 112us/step - loss: 0.1282 - acc: 0.9684 - val_loss: 0.1321 - val_acc: 0.9709\n",
      "Epoch 58/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1284 - acc: 0.9684 - val_loss: 0.1325 - val_acc: 0.9709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1281 - acc: 0.9684 - val_loss: 0.1311 - val_acc: 0.9709\n",
      "Epoch 60/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1285 - acc: 0.9684 - val_loss: 0.1324 - val_acc: 0.9709\n",
      "Epoch 61/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1283 - acc: 0.9684 - val_loss: 0.1328 - val_acc: 0.9709\n",
      "Epoch 62/1000\n",
      "16079/16079 [==============================] - 2s 124us/step - loss: 0.1285 - acc: 0.9684 - val_loss: 0.1332 - val_acc: 0.9709\n",
      "Epoch 63/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1280 - acc: 0.9684 - val_loss: 0.1315 - val_acc: 0.9709\n",
      "Epoch 64/1000\n",
      "16079/16079 [==============================] - 2s 139us/step - loss: 0.1279 - acc: 0.9684 - val_loss: 0.1328 - val_acc: 0.9709\n",
      "Epoch 65/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1279 - acc: 0.9684 - val_loss: 0.1345 - val_acc: 0.9709\n",
      "Epoch 66/1000\n",
      "16079/16079 [==============================] - 2s 123us/step - loss: 0.1282 - acc: 0.9684 - val_loss: 0.1314 - val_acc: 0.9709\n",
      "Epoch 67/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1278 - acc: 0.9684 - val_loss: 0.1317 - val_acc: 0.9709\n",
      "Epoch 68/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1278 - acc: 0.9684 - val_loss: 0.1321 - val_acc: 0.9709\n",
      "Epoch 69/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1280 - acc: 0.9684 - val_loss: 0.1304 - val_acc: 0.9709\n",
      "Epoch 70/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1282 - acc: 0.9684 - val_loss: 0.1359 - val_acc: 0.9709\n",
      "Epoch 71/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1277 - acc: 0.9684 - val_loss: 0.1315 - val_acc: 0.9709\n",
      "Epoch 72/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1278 - acc: 0.9684 - val_loss: 0.1320 - val_acc: 0.9709\n",
      "Epoch 73/1000\n",
      "16079/16079 [==============================] - 2s 93us/step - loss: 0.1280 - acc: 0.9684 - val_loss: 0.1323 - val_acc: 0.9709\n",
      "Epoch 74/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1275 - acc: 0.9684 - val_loss: 0.1374 - val_acc: 0.9709\n",
      "Epoch 75/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1273 - acc: 0.9684 - val_loss: 0.1321 - val_acc: 0.9709\n",
      "Epoch 76/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1271 - acc: 0.9684 - val_loss: 0.1320 - val_acc: 0.9709\n",
      "Epoch 77/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1272 - acc: 0.9684 - val_loss: 0.1333 - val_acc: 0.9709\n",
      "Epoch 78/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1275 - acc: 0.9684 - val_loss: 0.1326 - val_acc: 0.9709\n",
      "Epoch 79/1000\n",
      "16079/16079 [==============================] - 2s 133us/step - loss: 0.1270 - acc: 0.9684 - val_loss: 0.1320 - val_acc: 0.9709\n",
      "Epoch 80/1000\n",
      "16079/16079 [==============================] - 2s 127us/step - loss: 0.1266 - acc: 0.9684 - val_loss: 0.1329 - val_acc: 0.9709\n",
      "Epoch 81/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1269 - acc: 0.9684 - val_loss: 0.1329 - val_acc: 0.9709\n",
      "Epoch 82/1000\n",
      "16079/16079 [==============================] - 2s 113us/step - loss: 0.1271 - acc: 0.9684 - val_loss: 0.1370 - val_acc: 0.9709\n",
      "Epoch 83/1000\n",
      "16079/16079 [==============================] - 2s 119us/step - loss: 0.1268 - acc: 0.9684 - val_loss: 0.1350 - val_acc: 0.9709\n",
      "Epoch 84/1000\n",
      "16079/16079 [==============================] - 2s 130us/step - loss: 0.1269 - acc: 0.9684 - val_loss: 0.1337 - val_acc: 0.9709\n",
      "Epoch 85/1000\n",
      "16079/16079 [==============================] - 2s 120us/step - loss: 0.1269 - acc: 0.9684 - val_loss: 0.1329 - val_acc: 0.9709\n",
      "Epoch 86/1000\n",
      "16079/16079 [==============================] - 2s 112us/step - loss: 0.1269 - acc: 0.9684 - val_loss: 0.1351 - val_acc: 0.9709\n",
      "Epoch 87/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1266 - acc: 0.9684 - val_loss: 0.1334 - val_acc: 0.9709\n",
      "Epoch 88/1000\n",
      "16079/16079 [==============================] - 2s 111us/step - loss: 0.1266 - acc: 0.9684 - val_loss: 0.1354 - val_acc: 0.9709\n",
      "Epoch 89/1000\n",
      "16079/16079 [==============================] - 2s 119us/step - loss: 0.1263 - acc: 0.9684 - val_loss: 0.1315 - val_acc: 0.9709\n",
      "Epoch 90/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1269 - acc: 0.9684 - val_loss: 0.1337 - val_acc: 0.9709\n",
      "Epoch 91/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1264 - acc: 0.9684 - val_loss: 0.1327 - val_acc: 0.9709\n",
      "Epoch 92/1000\n",
      "16079/16079 [==============================] - 2s 122us/step - loss: 0.1265 - acc: 0.9684 - val_loss: 0.1332 - val_acc: 0.9709\n",
      "Epoch 93/1000\n",
      "16079/16079 [==============================] - 2s 115us/step - loss: 0.1265 - acc: 0.9684 - val_loss: 0.1386 - val_acc: 0.9709\n",
      "Epoch 94/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1263 - acc: 0.9684 - val_loss: 0.1345 - val_acc: 0.9709\n",
      "Epoch 95/1000\n",
      "16079/16079 [==============================] - 2s 119us/step - loss: 0.1260 - acc: 0.9684 - val_loss: 0.1344 - val_acc: 0.9709\n",
      "Epoch 96/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1262 - acc: 0.9684 - val_loss: 0.1357 - val_acc: 0.9709\n",
      "Epoch 97/1000\n",
      "16079/16079 [==============================] - 2s 109us/step - loss: 0.1259 - acc: 0.9684 - val_loss: 0.1358 - val_acc: 0.9709\n",
      "Epoch 98/1000\n",
      "16079/16079 [==============================] - 2s 117us/step - loss: 0.1266 - acc: 0.9684 - val_loss: 0.1335 - val_acc: 0.9709\n",
      "Epoch 99/1000\n",
      "16079/16079 [==============================] - 2s 121us/step - loss: 0.1263 - acc: 0.9684 - val_loss: 0.1332 - val_acc: 0.9709\n",
      "Epoch 100/1000\n",
      "16079/16079 [==============================] - 2s 135us/step - loss: 0.1260 - acc: 0.9684 - val_loss: 0.1352 - val_acc: 0.9709\n",
      "Epoch 101/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1257 - acc: 0.9684 - val_loss: 0.1345 - val_acc: 0.9709\n",
      "Epoch 102/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1260 - acc: 0.9684 - val_loss: 0.1316 - val_acc: 0.9709\n",
      "Epoch 103/1000\n",
      "16079/16079 [==============================] - 2s 119us/step - loss: 0.1258 - acc: 0.9684 - val_loss: 0.1343 - val_acc: 0.9709\n",
      "Epoch 104/1000\n",
      "16079/16079 [==============================] - 2s 120us/step - loss: 0.1257 - acc: 0.9684 - val_loss: 0.1382 - val_acc: 0.9709\n",
      "Epoch 105/1000\n",
      "16079/16079 [==============================] - 2s 122us/step - loss: 0.1258 - acc: 0.9684 - val_loss: 0.1340 - val_acc: 0.9709\n",
      "Epoch 106/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1255 - acc: 0.9684 - val_loss: 0.1368 - val_acc: 0.9709\n",
      "Epoch 107/1000\n",
      "16079/16079 [==============================] - 2s 126us/step - loss: 0.1262 - acc: 0.9684 - val_loss: 0.1333 - val_acc: 0.9709\n",
      "Epoch 108/1000\n",
      "16079/16079 [==============================] - 2s 113us/step - loss: 0.1253 - acc: 0.9684 - val_loss: 0.1358 - val_acc: 0.9709\n",
      "Epoch 109/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1257 - acc: 0.9683 - val_loss: 0.1348 - val_acc: 0.9709\n",
      "Epoch 110/1000\n",
      "16079/16079 [==============================] - 2s 124us/step - loss: 0.1251 - acc: 0.9684 - val_loss: 0.1358 - val_acc: 0.9709\n",
      "Epoch 111/1000\n",
      "16079/16079 [==============================] - 2s 112us/step - loss: 0.1250 - acc: 0.9683 - val_loss: 0.1367 - val_acc: 0.9709\n",
      "Epoch 112/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1251 - acc: 0.9684 - val_loss: 0.1357 - val_acc: 0.9709\n",
      "Epoch 113/1000\n",
      "16079/16079 [==============================] - 2s 117us/step - loss: 0.1250 - acc: 0.9684 - val_loss: 0.1376 - val_acc: 0.9709\n",
      "Epoch 114/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1254 - acc: 0.9683 - val_loss: 0.1364 - val_acc: 0.9709\n",
      "Epoch 115/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1251 - acc: 0.9684 - val_loss: 0.1362 - val_acc: 0.9709\n",
      "Epoch 116/1000\n",
      "16079/16079 [==============================] - 2s 112us/step - loss: 0.1250 - acc: 0.9684 - val_loss: 0.1377 - val_acc: 0.9709\n",
      "Epoch 117/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079/16079 [==============================] - 2s 121us/step - loss: 0.1249 - acc: 0.9683 - val_loss: 0.1344 - val_acc: 0.9709\n",
      "Epoch 118/1000\n",
      "16079/16079 [==============================] - 2s 111us/step - loss: 0.1251 - acc: 0.9684 - val_loss: 0.1375 - val_acc: 0.9709\n",
      "Epoch 119/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1257 - acc: 0.9684 - val_loss: 0.1375 - val_acc: 0.9709\n",
      "Epoch 120/1000\n",
      "16079/16079 [==============================] - 2s 118us/step - loss: 0.1245 - acc: 0.9683 - val_loss: 0.1430 - val_acc: 0.9709\n",
      "Epoch 121/1000\n",
      "16079/16079 [==============================] - 2s 122us/step - loss: 0.1248 - acc: 0.9684 - val_loss: 0.1406 - val_acc: 0.9709\n",
      "Epoch 122/1000\n",
      "16079/16079 [==============================] - 2s 111us/step - loss: 0.1251 - acc: 0.9684 - val_loss: 0.1396 - val_acc: 0.9709\n",
      "Epoch 123/1000\n",
      "16079/16079 [==============================] - 2s 122us/step - loss: 0.1242 - acc: 0.9685 - val_loss: 0.1403 - val_acc: 0.9709\n",
      "Epoch 124/1000\n",
      "16079/16079 [==============================] - 2s 109us/step - loss: 0.1242 - acc: 0.9684 - val_loss: 0.1430 - val_acc: 0.9709\n",
      "Epoch 125/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1250 - acc: 0.9684 - val_loss: 0.1365 - val_acc: 0.9709\n",
      "Epoch 126/1000\n",
      "16079/16079 [==============================] - 2s 121us/step - loss: 0.1252 - acc: 0.9683 - val_loss: 0.1357 - val_acc: 0.9709\n",
      "Epoch 127/1000\n",
      "16079/16079 [==============================] - 2s 122us/step - loss: 0.1243 - acc: 0.9684 - val_loss: 0.1371 - val_acc: 0.9709\n",
      "Epoch 128/1000\n",
      "16079/16079 [==============================] - 2s 124us/step - loss: 0.1243 - acc: 0.9684 - val_loss: 0.1388 - val_acc: 0.9709\n",
      "Epoch 129/1000\n",
      "16079/16079 [==============================] - 2s 126us/step - loss: 0.1246 - acc: 0.9683 - val_loss: 0.1381 - val_acc: 0.9709\n",
      "Epoch 130/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1243 - acc: 0.9683 - val_loss: 0.1388 - val_acc: 0.9709\n",
      "Epoch 131/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1240 - acc: 0.9684 - val_loss: 0.1396 - val_acc: 0.9709\n",
      "Epoch 132/1000\n",
      "16079/16079 [==============================] - 2s 119us/step - loss: 0.1242 - acc: 0.9684 - val_loss: 0.1363 - val_acc: 0.9709\n",
      "Epoch 133/1000\n",
      "16079/16079 [==============================] - 2s 113us/step - loss: 0.1245 - acc: 0.9683 - val_loss: 0.1377 - val_acc: 0.9709\n",
      "Epoch 134/1000\n",
      "16079/16079 [==============================] - 2s 121us/step - loss: 0.1245 - acc: 0.9683 - val_loss: 0.1384 - val_acc: 0.9709\n",
      "Epoch 135/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1240 - acc: 0.9683 - val_loss: 0.1417 - val_acc: 0.9709\n",
      "Epoch 136/1000\n",
      "16079/16079 [==============================] - 2s 115us/step - loss: 0.1241 - acc: 0.9684 - val_loss: 0.1415 - val_acc: 0.9709\n",
      "Epoch 137/1000\n",
      "16079/16079 [==============================] - 2s 128us/step - loss: 0.1242 - acc: 0.9683 - val_loss: 0.1395 - val_acc: 0.9709\n",
      "Epoch 138/1000\n",
      "16079/16079 [==============================] - 2s 120us/step - loss: 0.1244 - acc: 0.9684 - val_loss: 0.1386 - val_acc: 0.9709\n",
      "Epoch 139/1000\n",
      "16079/16079 [==============================] - 2s 118us/step - loss: 0.1242 - acc: 0.9683 - val_loss: 0.1399 - val_acc: 0.9709\n",
      "Epoch 140/1000\n",
      "16079/16079 [==============================] - 2s 117us/step - loss: 0.1242 - acc: 0.9684 - val_loss: 0.1357 - val_acc: 0.9709\n",
      "Epoch 141/1000\n",
      "16079/16079 [==============================] - 2s 119us/step - loss: 0.1235 - acc: 0.9684 - val_loss: 0.1368 - val_acc: 0.9709\n",
      "Epoch 142/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1236 - acc: 0.9684 - val_loss: 0.1364 - val_acc: 0.9709\n",
      "Epoch 143/1000\n",
      "16079/16079 [==============================] - 2s 113us/step - loss: 0.1240 - acc: 0.9683 - val_loss: 0.1375 - val_acc: 0.9709\n",
      "Epoch 144/1000\n",
      "16079/16079 [==============================] - 2s 127us/step - loss: 0.1240 - acc: 0.9684 - val_loss: 0.1369 - val_acc: 0.9709\n",
      "Epoch 145/1000\n",
      "16079/16079 [==============================] - 2s 113us/step - loss: 0.1232 - acc: 0.9684 - val_loss: 0.1414 - val_acc: 0.9709\n",
      "Epoch 146/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1233 - acc: 0.9683 - val_loss: 0.1378 - val_acc: 0.9709\n",
      "Epoch 147/1000\n",
      "16079/16079 [==============================] - 2s 123us/step - loss: 0.1237 - acc: 0.9684 - val_loss: 0.1394 - val_acc: 0.9709\n",
      "Epoch 148/1000\n",
      "16079/16079 [==============================] - 2s 122us/step - loss: 0.1235 - acc: 0.9684 - val_loss: 0.1379 - val_acc: 0.9709\n",
      "Epoch 149/1000\n",
      "16079/16079 [==============================] - 2s 113us/step - loss: 0.1241 - acc: 0.9684 - val_loss: 0.1367 - val_acc: 0.9709\n",
      "Epoch 150/1000\n",
      "16079/16079 [==============================] - 2s 120us/step - loss: 0.1244 - acc: 0.9684 - val_loss: 0.1370 - val_acc: 0.9709\n",
      "Epoch 151/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1235 - acc: 0.9684 - val_loss: 0.1405 - val_acc: 0.9709\n",
      "Epoch 152/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1232 - acc: 0.9685 - val_loss: 0.1355 - val_acc: 0.9709\n",
      "Epoch 153/1000\n",
      "16079/16079 [==============================] - 2s 113us/step - loss: 0.1236 - acc: 0.9684 - val_loss: 0.1363 - val_acc: 0.9709\n",
      "Epoch 154/1000\n",
      "16079/16079 [==============================] - 2s 119us/step - loss: 0.1236 - acc: 0.9683 - val_loss: 0.1409 - val_acc: 0.9709\n",
      "Epoch 155/1000\n",
      "16079/16079 [==============================] - 2s 113us/step - loss: 0.1234 - acc: 0.9684 - val_loss: 0.1375 - val_acc: 0.9709\n",
      "Epoch 156/1000\n",
      "16079/16079 [==============================] - 2s 115us/step - loss: 0.1231 - acc: 0.9685 - val_loss: 0.1372 - val_acc: 0.9709\n",
      "Epoch 157/1000\n",
      "16079/16079 [==============================] - 2s 118us/step - loss: 0.1229 - acc: 0.9682 - val_loss: 0.1401 - val_acc: 0.9709\n",
      "Epoch 158/1000\n",
      "16079/16079 [==============================] - 2s 120us/step - loss: 0.1234 - acc: 0.9684 - val_loss: 0.1472 - val_acc: 0.9709\n",
      "Epoch 159/1000\n",
      "16079/16079 [==============================] - 2s 113us/step - loss: 0.1236 - acc: 0.9683 - val_loss: 0.1351 - val_acc: 0.9709\n",
      "Epoch 160/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1236 - acc: 0.9683 - val_loss: 0.1382 - val_acc: 0.9709\n",
      "Epoch 161/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1226 - acc: 0.9684 - val_loss: 0.1367 - val_acc: 0.9709\n",
      "Epoch 162/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1228 - acc: 0.9683 - val_loss: 0.1400 - val_acc: 0.9709\n",
      "Epoch 163/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1237 - acc: 0.9685 - val_loss: 0.1366 - val_acc: 0.9709\n",
      "Epoch 164/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1233 - acc: 0.9683 - val_loss: 0.1393 - val_acc: 0.9709\n",
      "Epoch 165/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1232 - acc: 0.9683 - val_loss: 0.1409 - val_acc: 0.9709\n",
      "Epoch 166/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1230 - acc: 0.9683 - val_loss: 0.1355 - val_acc: 0.9709\n",
      "Epoch 167/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1235 - acc: 0.9684 - val_loss: 0.1358 - val_acc: 0.9709\n",
      "Epoch 168/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1232 - acc: 0.9685 - val_loss: 0.1404 - val_acc: 0.9709\n",
      "Epoch 169/1000\n",
      "16079/16079 [==============================] - 2s 109us/step - loss: 0.1228 - acc: 0.9683 - val_loss: 0.1411 - val_acc: 0.9709\n",
      "Epoch 170/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1226 - acc: 0.9683 - val_loss: 0.1406 - val_acc: 0.9709\n",
      "Epoch 171/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1227 - acc: 0.9683 - val_loss: 0.1415 - val_acc: 0.9709\n",
      "Epoch 172/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1233 - acc: 0.9683 - val_loss: 0.1392 - val_acc: 0.9709\n",
      "Epoch 173/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1227 - acc: 0.9683 - val_loss: 0.1376 - val_acc: 0.9709\n",
      "Epoch 174/1000\n",
      "16079/16079 [==============================] - 2s 111us/step - loss: 0.1231 - acc: 0.9684 - val_loss: 0.1361 - val_acc: 0.9709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1229 - acc: 0.9684 - val_loss: 0.1413 - val_acc: 0.9709\n",
      "Epoch 176/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1225 - acc: 0.9683 - val_loss: 0.1433 - val_acc: 0.9709\n",
      "Epoch 177/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1228 - acc: 0.9685 - val_loss: 0.1389 - val_acc: 0.9709\n",
      "Epoch 178/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1219 - acc: 0.9684 - val_loss: 0.1428 - val_acc: 0.9709\n",
      "Epoch 179/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1227 - acc: 0.9684 - val_loss: 0.1415 - val_acc: 0.9709\n",
      "Epoch 180/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1227 - acc: 0.9684 - val_loss: 0.1413 - val_acc: 0.9709\n",
      "Epoch 181/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1224 - acc: 0.9683 - val_loss: 0.1422 - val_acc: 0.9709\n",
      "Epoch 182/1000\n",
      "16079/16079 [==============================] - 2s 118us/step - loss: 0.1225 - acc: 0.9683 - val_loss: 0.1425 - val_acc: 0.9709\n",
      "Epoch 183/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1221 - acc: 0.9683 - val_loss: 0.1428 - val_acc: 0.9709\n",
      "Epoch 184/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1223 - acc: 0.9683 - val_loss: 0.1404 - val_acc: 0.9709\n",
      "Epoch 185/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1221 - acc: 0.9682 - val_loss: 0.1366 - val_acc: 0.9709\n",
      "Epoch 186/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1219 - acc: 0.9684 - val_loss: 0.1403 - val_acc: 0.9709\n",
      "Epoch 187/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1222 - acc: 0.9684 - val_loss: 0.1434 - val_acc: 0.9709\n",
      "Epoch 188/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1220 - acc: 0.9684 - val_loss: 0.1420 - val_acc: 0.9709\n",
      "Epoch 189/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1222 - acc: 0.9683 - val_loss: 0.1391 - val_acc: 0.9709\n",
      "Epoch 190/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1216 - acc: 0.9683 - val_loss: 0.1387 - val_acc: 0.9709\n",
      "Epoch 191/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1218 - acc: 0.9685 - val_loss: 0.1429 - val_acc: 0.9709\n",
      "Epoch 192/1000\n",
      "16079/16079 [==============================] - 2s 109us/step - loss: 0.1226 - acc: 0.9684 - val_loss: 0.1384 - val_acc: 0.9709\n",
      "Epoch 193/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1225 - acc: 0.9684 - val_loss: 0.1388 - val_acc: 0.9709\n",
      "Epoch 194/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1218 - acc: 0.9685 - val_loss: 0.1411 - val_acc: 0.9709\n",
      "Epoch 195/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1218 - acc: 0.9684 - val_loss: 0.1409 - val_acc: 0.9709\n",
      "Epoch 196/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1217 - acc: 0.9684 - val_loss: 0.1395 - val_acc: 0.9709\n",
      "Epoch 197/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1222 - acc: 0.9685 - val_loss: 0.1423 - val_acc: 0.9709\n",
      "Epoch 198/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1226 - acc: 0.9682 - val_loss: 0.1380 - val_acc: 0.9709\n",
      "Epoch 199/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1217 - acc: 0.9684 - val_loss: 0.1373 - val_acc: 0.9709\n",
      "Epoch 200/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1219 - acc: 0.9685 - val_loss: 0.1396 - val_acc: 0.9709\n",
      "Epoch 201/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1224 - acc: 0.9684 - val_loss: 0.1429 - val_acc: 0.9709\n",
      "Epoch 202/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1216 - acc: 0.9681 - val_loss: 0.1417 - val_acc: 0.9709\n",
      "Epoch 203/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1223 - acc: 0.9685 - val_loss: 0.1389 - val_acc: 0.9709\n",
      "Epoch 204/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1216 - acc: 0.9683 - val_loss: 0.1497 - val_acc: 0.9709\n",
      "Epoch 205/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1217 - acc: 0.9685 - val_loss: 0.1446 - val_acc: 0.9709\n",
      "Epoch 206/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1210 - acc: 0.9686 - val_loss: 0.1493 - val_acc: 0.9709\n",
      "Epoch 207/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1220 - acc: 0.9684 - val_loss: 0.1421 - val_acc: 0.9709\n",
      "Epoch 208/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1214 - acc: 0.9683 - val_loss: 0.1432 - val_acc: 0.9709\n",
      "Epoch 209/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1212 - acc: 0.9683 - val_loss: 0.1467 - val_acc: 0.9709\n",
      "Epoch 210/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1213 - acc: 0.9683 - val_loss: 0.1428 - val_acc: 0.9709\n",
      "Epoch 211/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1210 - acc: 0.9682 - val_loss: 0.1440 - val_acc: 0.9709\n",
      "Epoch 212/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1220 - acc: 0.9685 - val_loss: 0.1409 - val_acc: 0.9709\n",
      "Epoch 213/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1216 - acc: 0.9683 - val_loss: 0.1474 - val_acc: 0.9709\n",
      "Epoch 214/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1209 - acc: 0.9682 - val_loss: 0.1451 - val_acc: 0.9709\n",
      "Epoch 215/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1212 - acc: 0.9683 - val_loss: 0.1476 - val_acc: 0.9709\n",
      "Epoch 216/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1211 - acc: 0.9684 - val_loss: 0.1442 - val_acc: 0.9709\n",
      "Epoch 217/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1212 - acc: 0.9684 - val_loss: 0.1387 - val_acc: 0.9709\n",
      "Epoch 218/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1220 - acc: 0.9684 - val_loss: 0.1415 - val_acc: 0.9709\n",
      "Epoch 219/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1212 - acc: 0.9684 - val_loss: 0.1423 - val_acc: 0.9709\n",
      "Epoch 220/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1204 - acc: 0.9685 - val_loss: 0.1442 - val_acc: 0.9709\n",
      "Epoch 221/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1207 - acc: 0.9683 - val_loss: 0.1467 - val_acc: 0.9709\n",
      "Epoch 222/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1212 - acc: 0.9683 - val_loss: 0.1423 - val_acc: 0.9709\n",
      "Epoch 223/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1211 - acc: 0.9682 - val_loss: 0.1419 - val_acc: 0.9709\n",
      "Epoch 224/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1209 - acc: 0.9685 - val_loss: 0.1478 - val_acc: 0.9709\n",
      "Epoch 225/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1216 - acc: 0.9682 - val_loss: 0.1443 - val_acc: 0.9709\n",
      "Epoch 226/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1211 - acc: 0.9683 - val_loss: 0.1439 - val_acc: 0.9709\n",
      "Epoch 227/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1205 - acc: 0.9683 - val_loss: 0.1473 - val_acc: 0.9709\n",
      "Epoch 228/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1209 - acc: 0.9683 - val_loss: 0.1412 - val_acc: 0.9709\n",
      "Epoch 229/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1203 - acc: 0.9685 - val_loss: 0.1439 - val_acc: 0.9709\n",
      "Epoch 230/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1204 - acc: 0.9684 - val_loss: 0.1391 - val_acc: 0.9709\n",
      "Epoch 231/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1214 - acc: 0.9683 - val_loss: 0.1436 - val_acc: 0.9709\n",
      "Epoch 232/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1202 - acc: 0.9683 - val_loss: 0.1492 - val_acc: 0.9709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1208 - acc: 0.9683 - val_loss: 0.1424 - val_acc: 0.9709\n",
      "Epoch 234/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1204 - acc: 0.9686 - val_loss: 0.1495 - val_acc: 0.9709\n",
      "Epoch 235/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1206 - acc: 0.9683 - val_loss: 0.1490 - val_acc: 0.9709\n",
      "Epoch 236/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1201 - acc: 0.9685 - val_loss: 0.1465 - val_acc: 0.9709\n",
      "Epoch 237/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1207 - acc: 0.9685 - val_loss: 0.1552 - val_acc: 0.9709\n",
      "Epoch 238/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1207 - acc: 0.9684 - val_loss: 0.1421 - val_acc: 0.9709\n",
      "Epoch 239/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1200 - acc: 0.9685 - val_loss: 0.1394 - val_acc: 0.9709\n",
      "Epoch 240/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1206 - acc: 0.9684 - val_loss: 0.1441 - val_acc: 0.9709\n",
      "Epoch 241/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1201 - acc: 0.9684 - val_loss: 0.1489 - val_acc: 0.9709\n",
      "Epoch 242/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1201 - acc: 0.9683 - val_loss: 0.1470 - val_acc: 0.9709\n",
      "Epoch 243/1000\n",
      "16079/16079 [==============================] - 2s 113us/step - loss: 0.1202 - acc: 0.9685 - val_loss: 0.1471 - val_acc: 0.9709\n",
      "Epoch 244/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1200 - acc: 0.9683 - val_loss: 0.1440 - val_acc: 0.9709\n",
      "Epoch 245/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1203 - acc: 0.9685 - val_loss: 0.1525 - val_acc: 0.9707\n",
      "Epoch 246/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1204 - acc: 0.9684 - val_loss: 0.1433 - val_acc: 0.9709\n",
      "Epoch 247/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1203 - acc: 0.9685 - val_loss: 0.1480 - val_acc: 0.9709\n",
      "Epoch 248/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1200 - acc: 0.9685 - val_loss: 0.1470 - val_acc: 0.9709\n",
      "Epoch 249/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1200 - acc: 0.9685 - val_loss: 0.1464 - val_acc: 0.9709\n",
      "Epoch 250/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1196 - acc: 0.9684 - val_loss: 0.1468 - val_acc: 0.9709\n",
      "Epoch 251/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1202 - acc: 0.9684 - val_loss: 0.1492 - val_acc: 0.9709: 0s - loss: 0.1168 - a\n",
      "Epoch 252/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1194 - acc: 0.9686 - val_loss: 0.1432 - val_acc: 0.9709\n",
      "Epoch 253/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1199 - acc: 0.9684 - val_loss: 0.1442 - val_acc: 0.9709\n",
      "Epoch 254/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1198 - acc: 0.9684 - val_loss: 0.1468 - val_acc: 0.9709\n",
      "Epoch 255/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1193 - acc: 0.9685 - val_loss: 0.1581 - val_acc: 0.9709\n",
      "Epoch 256/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1205 - acc: 0.9683 - val_loss: 0.1433 - val_acc: 0.9709\n",
      "Epoch 257/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1192 - acc: 0.9685 - val_loss: 0.1449 - val_acc: 0.9709\n",
      "Epoch 258/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1198 - acc: 0.9685 - val_loss: 0.1436 - val_acc: 0.9709\n",
      "Epoch 259/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1194 - acc: 0.9685 - val_loss: 0.1429 - val_acc: 0.9709\n",
      "Epoch 260/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1189 - acc: 0.9684 - val_loss: 0.1509 - val_acc: 0.9709\n",
      "Epoch 261/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1192 - acc: 0.9685 - val_loss: 0.1492 - val_acc: 0.9709\n",
      "Epoch 262/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1200 - acc: 0.9683 - val_loss: 0.1492 - val_acc: 0.9709\n",
      "Epoch 263/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1192 - acc: 0.9682 - val_loss: 0.1495 - val_acc: 0.9709\n",
      "Epoch 264/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1198 - acc: 0.9687 - val_loss: 0.1463 - val_acc: 0.9709\n",
      "Epoch 265/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1193 - acc: 0.9684 - val_loss: 0.1433 - val_acc: 0.9709\n",
      "Epoch 266/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1194 - acc: 0.9685 - val_loss: 0.1535 - val_acc: 0.9709\n",
      "Epoch 267/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1192 - acc: 0.9686 - val_loss: 0.1427 - val_acc: 0.9709\n",
      "Epoch 268/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1194 - acc: 0.9687 - val_loss: 0.1469 - val_acc: 0.9709\n",
      "Epoch 269/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1190 - acc: 0.9687 - val_loss: 0.1463 - val_acc: 0.9709\n",
      "Epoch 270/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1193 - acc: 0.9686 - val_loss: 0.1463 - val_acc: 0.9709\n",
      "Epoch 271/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1205 - acc: 0.9684 - val_loss: 0.1435 - val_acc: 0.9709\n",
      "Epoch 272/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1186 - acc: 0.9685 - val_loss: 0.1457 - val_acc: 0.9707\n",
      "Epoch 273/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1188 - acc: 0.9685 - val_loss: 0.1489 - val_acc: 0.9709\n",
      "Epoch 274/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1190 - acc: 0.9685 - val_loss: 0.1461 - val_acc: 0.9709\n",
      "Epoch 275/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1196 - acc: 0.9685 - val_loss: 0.1427 - val_acc: 0.9709\n",
      "Epoch 276/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1191 - acc: 0.9686 - val_loss: 0.1451 - val_acc: 0.9709\n",
      "Epoch 277/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1185 - acc: 0.9685 - val_loss: 0.1524 - val_acc: 0.9709\n",
      "Epoch 278/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1197 - acc: 0.9684 - val_loss: 0.1440 - val_acc: 0.9709\n",
      "Epoch 279/1000\n",
      "16079/16079 [==============================] - 2s 133us/step - loss: 0.1194 - acc: 0.9683 - val_loss: 0.1428 - val_acc: 0.9709\n",
      "Epoch 280/1000\n",
      "16079/16079 [==============================] - 2s 138us/step - loss: 0.1186 - acc: 0.9685 - val_loss: 0.1472 - val_acc: 0.9709\n",
      "Epoch 281/1000\n",
      "16079/16079 [==============================] - 2s 131us/step - loss: 0.1188 - acc: 0.9684 - val_loss: 0.1481 - val_acc: 0.9712\n",
      "Epoch 282/1000\n",
      "16079/16079 [==============================] - 2s 127us/step - loss: 0.1192 - acc: 0.9685 - val_loss: 0.1485 - val_acc: 0.9709\n",
      "Epoch 283/1000\n",
      "16079/16079 [==============================] - 2s 120us/step - loss: 0.1184 - acc: 0.9687 - val_loss: 0.1468 - val_acc: 0.9709\n",
      "Epoch 284/1000\n",
      "16079/16079 [==============================] - 2s 133us/step - loss: 0.1188 - acc: 0.9687 - val_loss: 0.1498 - val_acc: 0.9709\n",
      "Epoch 285/1000\n",
      "16079/16079 [==============================] - 2s 132us/step - loss: 0.1190 - acc: 0.9685 - val_loss: 0.1488 - val_acc: 0.9709\n",
      "Epoch 286/1000\n",
      "16079/16079 [==============================] - 2s 117us/step - loss: 0.1185 - acc: 0.9687 - val_loss: 0.1466 - val_acc: 0.9709\n",
      "Epoch 287/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1185 - acc: 0.9688 - val_loss: 0.1476 - val_acc: 0.9709\n",
      "Epoch 288/1000\n",
      "16079/16079 [==============================] - 2s 111us/step - loss: 0.1187 - acc: 0.9685 - val_loss: 0.1494 - val_acc: 0.9709\n",
      "Epoch 289/1000\n",
      "16079/16079 [==============================] - 2s 138us/step - loss: 0.1180 - acc: 0.9686 - val_loss: 0.1469 - val_acc: 0.9709\n",
      "Epoch 290/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1182 - acc: 0.9685 - val_loss: 0.1494 - val_acc: 0.9709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291/1000\n",
      "16079/16079 [==============================] - 2s 109us/step - loss: 0.1189 - acc: 0.9685 - val_loss: 0.1459 - val_acc: 0.9709\n",
      "Epoch 292/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1181 - acc: 0.9686 - val_loss: 0.1470 - val_acc: 0.9709\n",
      "Epoch 293/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1183 - acc: 0.9687 - val_loss: 0.1505 - val_acc: 0.9709\n",
      "Epoch 294/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1183 - acc: 0.9685 - val_loss: 0.1462 - val_acc: 0.9712\n",
      "Epoch 295/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1179 - acc: 0.9685 - val_loss: 0.1503 - val_acc: 0.9709\n",
      "Epoch 296/1000\n",
      "16079/16079 [==============================] - 2s 117us/step - loss: 0.1180 - acc: 0.9688 - val_loss: 0.1484 - val_acc: 0.9709\n",
      "Epoch 297/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1180 - acc: 0.9689 - val_loss: 0.1467 - val_acc: 0.9709\n",
      "Epoch 298/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1179 - acc: 0.9686 - val_loss: 0.1479 - val_acc: 0.9709\n",
      "Epoch 299/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1185 - acc: 0.9685 - val_loss: 0.1453 - val_acc: 0.9709\n",
      "Epoch 300/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1175 - acc: 0.9685 - val_loss: 0.1608 - val_acc: 0.9709\n",
      "Epoch 301/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1186 - acc: 0.9687 - val_loss: 0.1476 - val_acc: 0.9709\n",
      "Epoch 302/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1191 - acc: 0.9685 - val_loss: 0.1446 - val_acc: 0.9712\n",
      "Epoch 303/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1188 - acc: 0.9685 - val_loss: 0.1478 - val_acc: 0.9709\n",
      "Epoch 304/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1190 - acc: 0.9683 - val_loss: 0.1488 - val_acc: 0.9709\n",
      "Epoch 305/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1187 - acc: 0.9683 - val_loss: 0.1504 - val_acc: 0.9709\n",
      "Epoch 306/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1183 - acc: 0.9687 - val_loss: 0.1485 - val_acc: 0.9709\n",
      "Epoch 307/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1178 - acc: 0.9686 - val_loss: 0.1560 - val_acc: 0.9709\n",
      "Epoch 308/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1184 - acc: 0.9685 - val_loss: 0.1506 - val_acc: 0.9709\n",
      "Epoch 309/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1175 - acc: 0.9687 - val_loss: 0.1501 - val_acc: 0.9709\n",
      "Epoch 310/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1170 - acc: 0.9689 - val_loss: 0.1539 - val_acc: 0.9709\n",
      "Epoch 311/1000\n",
      "16079/16079 [==============================] - 2s 120us/step - loss: 0.1181 - acc: 0.9687 - val_loss: 0.1514 - val_acc: 0.9709\n",
      "Epoch 312/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1169 - acc: 0.9685 - val_loss: 0.1453 - val_acc: 0.9709\n",
      "Epoch 313/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1176 - acc: 0.9686 - val_loss: 0.1472 - val_acc: 0.9709\n",
      "Epoch 314/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1171 - acc: 0.9688 - val_loss: 0.1516 - val_acc: 0.9709\n",
      "Epoch 315/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1170 - acc: 0.9686 - val_loss: 0.1541 - val_acc: 0.9709\n",
      "Epoch 316/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1169 - acc: 0.9687 - val_loss: 0.1505 - val_acc: 0.9709\n",
      "Epoch 317/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1169 - acc: 0.9687 - val_loss: 0.1475 - val_acc: 0.9709\n",
      "Epoch 318/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1169 - acc: 0.9688 - val_loss: 0.1518 - val_acc: 0.9709\n",
      "Epoch 319/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1168 - acc: 0.9688 - val_loss: 0.1491 - val_acc: 0.9704\n",
      "Epoch 320/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1172 - acc: 0.9688 - val_loss: 0.1511 - val_acc: 0.9709\n",
      "Epoch 321/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1178 - acc: 0.9688 - val_loss: 0.1491 - val_acc: 0.9709\n",
      "Epoch 322/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1180 - acc: 0.9684 - val_loss: 0.1447 - val_acc: 0.9709\n",
      "Epoch 323/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1175 - acc: 0.9690 - val_loss: 0.1449 - val_acc: 0.9709\n",
      "Epoch 324/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1172 - acc: 0.9685 - val_loss: 0.1501 - val_acc: 0.9709\n",
      "Epoch 325/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1174 - acc: 0.9688 - val_loss: 0.1520 - val_acc: 0.9709\n",
      "Epoch 326/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1163 - acc: 0.9684 - val_loss: 0.1502 - val_acc: 0.9709\n",
      "Epoch 327/1000\n",
      "16079/16079 [==============================] - 2s 111us/step - loss: 0.1177 - acc: 0.9687 - val_loss: 0.1478 - val_acc: 0.9709\n",
      "Epoch 328/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1166 - acc: 0.9688 - val_loss: 0.1511 - val_acc: 0.9709\n",
      "Epoch 329/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1165 - acc: 0.9689 - val_loss: 0.1498 - val_acc: 0.9709\n",
      "Epoch 330/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1167 - acc: 0.9689 - val_loss: 0.1471 - val_acc: 0.9712\n",
      "Epoch 331/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1166 - acc: 0.9688 - val_loss: 0.1487 - val_acc: 0.9709\n",
      "Epoch 332/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1165 - acc: 0.9686 - val_loss: 0.1570 - val_acc: 0.9704\n",
      "Epoch 333/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1167 - acc: 0.9683 - val_loss: 0.1509 - val_acc: 0.9709\n",
      "Epoch 334/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1176 - acc: 0.9686 - val_loss: 0.1529 - val_acc: 0.9712\n",
      "Epoch 335/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1168 - acc: 0.9685 - val_loss: 0.1503 - val_acc: 0.9709\n",
      "Epoch 336/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1163 - acc: 0.9687 - val_loss: 0.1498 - val_acc: 0.9712\n",
      "Epoch 337/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1163 - acc: 0.9688 - val_loss: 0.1496 - val_acc: 0.9707\n",
      "Epoch 338/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1176 - acc: 0.9686 - val_loss: 0.1537 - val_acc: 0.9709\n",
      "Epoch 339/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1165 - acc: 0.9688 - val_loss: 0.1482 - val_acc: 0.9707\n",
      "Epoch 340/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1161 - acc: 0.9689 - val_loss: 0.1504 - val_acc: 0.9712\n",
      "Epoch 341/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1162 - acc: 0.9687 - val_loss: 0.1547 - val_acc: 0.9709\n",
      "Epoch 342/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1168 - acc: 0.9688 - val_loss: 0.1507 - val_acc: 0.9709\n",
      "Epoch 343/1000\n",
      "16079/16079 [==============================] - 2s 111us/step - loss: 0.1159 - acc: 0.9688 - val_loss: 0.1496 - val_acc: 0.9709\n",
      "Epoch 344/1000\n",
      "16079/16079 [==============================] - 2s 131us/step - loss: 0.1158 - acc: 0.9689 - val_loss: 0.1505 - val_acc: 0.9707\n",
      "Epoch 345/1000\n",
      "16079/16079 [==============================] - 2s 141us/step - loss: 0.1163 - acc: 0.9686 - val_loss: 0.1521 - val_acc: 0.9709\n",
      "Epoch 346/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1156 - acc: 0.9688 - val_loss: 0.1513 - val_acc: 0.9709\n",
      "Epoch 347/1000\n",
      "16079/16079 [==============================] - 2s 130us/step - loss: 0.1158 - acc: 0.9688 - val_loss: 0.1506 - val_acc: 0.9712\n",
      "Epoch 348/1000\n",
      "16079/16079 [==============================] - 2s 125us/step - loss: 0.1162 - acc: 0.9687 - val_loss: 0.1479 - val_acc: 0.9709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1163 - acc: 0.9688 - val_loss: 0.1503 - val_acc: 0.9709\n",
      "Epoch 350/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1163 - acc: 0.9690 - val_loss: 0.1505 - val_acc: 0.9709\n",
      "Epoch 351/1000\n",
      "16079/16079 [==============================] - 2s 122us/step - loss: 0.1163 - acc: 0.9688 - val_loss: 0.1506 - val_acc: 0.9707\n",
      "Epoch 352/1000\n",
      "16079/16079 [==============================] - 2s 127us/step - loss: 0.1168 - acc: 0.9687 - val_loss: 0.1453 - val_acc: 0.9704\n",
      "Epoch 353/1000\n",
      "16079/16079 [==============================] - 2s 118us/step - loss: 0.1161 - acc: 0.9687 - val_loss: 0.1531 - val_acc: 0.9707\n",
      "Epoch 354/1000\n",
      "16079/16079 [==============================] - 2s 113us/step - loss: 0.1156 - acc: 0.9688 - val_loss: 0.1483 - val_acc: 0.9707\n",
      "Epoch 355/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1158 - acc: 0.9689 - val_loss: 0.1462 - val_acc: 0.9707\n",
      "Epoch 356/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1157 - acc: 0.9687 - val_loss: 0.1513 - val_acc: 0.9709\n",
      "Epoch 357/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1163 - acc: 0.9689 - val_loss: 0.1504 - val_acc: 0.9709\n",
      "Epoch 358/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1154 - acc: 0.9689 - val_loss: 0.1570 - val_acc: 0.9709\n",
      "Epoch 359/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1152 - acc: 0.9688 - val_loss: 0.1491 - val_acc: 0.9707\n",
      "Epoch 360/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1162 - acc: 0.9689 - val_loss: 0.1517 - val_acc: 0.9707\n",
      "Epoch 361/1000\n",
      "16079/16079 [==============================] - 2s 125us/step - loss: 0.1153 - acc: 0.9687 - val_loss: 0.1470 - val_acc: 0.9709\n",
      "Epoch 362/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1175 - acc: 0.9688 - val_loss: 0.1480 - val_acc: 0.9709\n",
      "Epoch 363/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1170 - acc: 0.9687 - val_loss: 0.1525 - val_acc: 0.9709\n",
      "Epoch 364/1000\n",
      "16079/16079 [==============================] - 2s 128us/step - loss: 0.1154 - acc: 0.9687 - val_loss: 0.1512 - val_acc: 0.9707\n",
      "Epoch 365/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1165 - acc: 0.9690 - val_loss: 0.1482 - val_acc: 0.9707\n",
      "Epoch 366/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1158 - acc: 0.9692 - val_loss: 0.1502 - val_acc: 0.9707\n",
      "Epoch 367/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1147 - acc: 0.9691 - val_loss: 0.1473 - val_acc: 0.9709\n",
      "Epoch 368/1000\n",
      "16079/16079 [==============================] - 2s 113us/step - loss: 0.1152 - acc: 0.9689 - val_loss: 0.1502 - val_acc: 0.9709\n",
      "Epoch 369/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1153 - acc: 0.9690 - val_loss: 0.1557 - val_acc: 0.9709\n",
      "Epoch 370/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1155 - acc: 0.9689 - val_loss: 0.1530 - val_acc: 0.9707\n",
      "Epoch 371/1000\n",
      "16079/16079 [==============================] - 2s 126us/step - loss: 0.1156 - acc: 0.9687 - val_loss: 0.1540 - val_acc: 0.9709\n",
      "Epoch 372/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1151 - acc: 0.9688 - val_loss: 0.1487 - val_acc: 0.9707\n",
      "Epoch 373/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1150 - acc: 0.9689 - val_loss: 0.1493 - val_acc: 0.9707\n",
      "Epoch 374/1000\n",
      "16079/16079 [==============================] - 2s 117us/step - loss: 0.1148 - acc: 0.9688 - val_loss: 0.1491 - val_acc: 0.9709\n",
      "Epoch 375/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1153 - acc: 0.9687 - val_loss: 0.1521 - val_acc: 0.9709\n",
      "Epoch 376/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1149 - acc: 0.9687 - val_loss: 0.1517 - val_acc: 0.9709\n",
      "Epoch 377/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1153 - acc: 0.9690 - val_loss: 0.1529 - val_acc: 0.9707\n",
      "Epoch 378/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1153 - acc: 0.9687 - val_loss: 0.1481 - val_acc: 0.9709\n",
      "Epoch 379/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1146 - acc: 0.9690 - val_loss: 0.1546 - val_acc: 0.9707\n",
      "Epoch 380/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1150 - acc: 0.9688 - val_loss: 0.1544 - val_acc: 0.9709\n",
      "Epoch 381/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1155 - acc: 0.9690 - val_loss: 0.1522 - val_acc: 0.9712\n",
      "Epoch 382/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1157 - acc: 0.9690 - val_loss: 0.1526 - val_acc: 0.9704\n",
      "Epoch 383/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1147 - acc: 0.9689 - val_loss: 0.1504 - val_acc: 0.9707\n",
      "Epoch 384/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1149 - acc: 0.9692 - val_loss: 0.1589 - val_acc: 0.9707\n",
      "Epoch 385/1000\n",
      "16079/16079 [==============================] - 2s 112us/step - loss: 0.1148 - acc: 0.9690 - val_loss: 0.1513 - val_acc: 0.9709\n",
      "Epoch 386/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1177 - acc: 0.9685 - val_loss: 0.1495 - val_acc: 0.9712\n",
      "Epoch 387/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1198 - acc: 0.9686 - val_loss: 0.1556 - val_acc: 0.9712\n",
      "Epoch 388/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1154 - acc: 0.9688 - val_loss: 0.1504 - val_acc: 0.9709\n",
      "Epoch 389/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1153 - acc: 0.9690 - val_loss: 0.1528 - val_acc: 0.9709\n",
      "Epoch 390/1000\n",
      "16079/16079 [==============================] - 2s 115us/step - loss: 0.1153 - acc: 0.9687 - val_loss: 0.1477 - val_acc: 0.9709\n",
      "Epoch 391/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1150 - acc: 0.9690 - val_loss: 0.1499 - val_acc: 0.9707\n",
      "Epoch 392/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1143 - acc: 0.9692 - val_loss: 0.1658 - val_acc: 0.9707\n",
      "Epoch 393/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1146 - acc: 0.9691 - val_loss: 0.1491 - val_acc: 0.9709\n",
      "Epoch 394/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1137 - acc: 0.9693 - val_loss: 0.1562 - val_acc: 0.9704\n",
      "Epoch 395/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1155 - acc: 0.9688 - val_loss: 0.1510 - val_acc: 0.9707\n",
      "Epoch 396/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1162 - acc: 0.9687 - val_loss: 0.1516 - val_acc: 0.9704\n",
      "Epoch 397/1000\n",
      "16079/16079 [==============================] - 2s 115us/step - loss: 0.1153 - acc: 0.9690 - val_loss: 0.1627 - val_acc: 0.9707\n",
      "Epoch 398/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1169 - acc: 0.9688 - val_loss: 0.1624 - val_acc: 0.9707\n",
      "Epoch 399/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1151 - acc: 0.9688 - val_loss: 0.1560 - val_acc: 0.9707\n",
      "Epoch 400/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1148 - acc: 0.9689 - val_loss: 0.1501 - val_acc: 0.9709\n",
      "Epoch 401/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1137 - acc: 0.9690 - val_loss: 0.1511 - val_acc: 0.9709\n",
      "Epoch 402/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1135 - acc: 0.9691 - val_loss: 0.1589 - val_acc: 0.9709\n",
      "Epoch 403/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1148 - acc: 0.9688 - val_loss: 0.1503 - val_acc: 0.9707\n",
      "Epoch 404/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1141 - acc: 0.9691 - val_loss: 0.1547 - val_acc: 0.9707\n",
      "Epoch 405/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1144 - acc: 0.9691 - val_loss: 0.1540 - val_acc: 0.9707\n",
      "Epoch 406/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1145 - acc: 0.9688 - val_loss: 0.1493 - val_acc: 0.9707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 407/1000\n",
      "16079/16079 [==============================] - 2s 119us/step - loss: 0.1137 - acc: 0.9691 - val_loss: 0.1498 - val_acc: 0.9709\n",
      "Epoch 408/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1138 - acc: 0.9688 - val_loss: 0.1530 - val_acc: 0.9704\n",
      "Epoch 409/1000\n",
      "16079/16079 [==============================] - 2s 137us/step - loss: 0.1157 - acc: 0.9688 - val_loss: 0.1479 - val_acc: 0.9712\n",
      "Epoch 410/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1140 - acc: 0.9690 - val_loss: 0.1532 - val_acc: 0.9709\n",
      "Epoch 411/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1141 - acc: 0.9690 - val_loss: 0.1536 - val_acc: 0.9709\n",
      "Epoch 412/1000\n",
      "16079/16079 [==============================] - 2s 139us/step - loss: 0.1137 - acc: 0.9692 - val_loss: 0.1518 - val_acc: 0.9709\n",
      "Epoch 413/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1140 - acc: 0.9690 - val_loss: 0.1570 - val_acc: 0.9704\n",
      "Epoch 414/1000\n",
      "16079/16079 [==============================] - 2s 124us/step - loss: 0.1134 - acc: 0.9692 - val_loss: 0.1542 - val_acc: 0.9709\n",
      "Epoch 415/1000\n",
      "16079/16079 [==============================] - 2s 117us/step - loss: 0.1146 - acc: 0.9687 - val_loss: 0.1546 - val_acc: 0.9704\n",
      "Epoch 416/1000\n",
      "16079/16079 [==============================] - 2s 136us/step - loss: 0.1128 - acc: 0.9693 - val_loss: 0.1539 - val_acc: 0.9709\n",
      "Epoch 417/1000\n",
      "16079/16079 [==============================] - 2s 130us/step - loss: 0.1148 - acc: 0.9686 - val_loss: 0.1515 - val_acc: 0.9702\n",
      "Epoch 418/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1140 - acc: 0.9692 - val_loss: 0.1550 - val_acc: 0.9707\n",
      "Epoch 419/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1143 - acc: 0.9693 - val_loss: 0.1637 - val_acc: 0.9709\n",
      "Epoch 420/1000\n",
      "16079/16079 [==============================] - 2s 127us/step - loss: 0.1157 - acc: 0.9687 - val_loss: 0.1541 - val_acc: 0.9712\n",
      "Epoch 421/1000\n",
      "16079/16079 [==============================] - 2s 128us/step - loss: 0.1145 - acc: 0.9688 - val_loss: 0.1537 - val_acc: 0.9709\n",
      "Epoch 422/1000\n",
      "16079/16079 [==============================] - 2s 125us/step - loss: 0.1133 - acc: 0.9694 - val_loss: 0.1533 - val_acc: 0.9702\n",
      "Epoch 423/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1135 - acc: 0.9690 - val_loss: 0.1503 - val_acc: 0.9707\n",
      "Epoch 424/1000\n",
      "16079/16079 [==============================] - 2s 123us/step - loss: 0.1130 - acc: 0.9694 - val_loss: 0.1523 - val_acc: 0.9709\n",
      "Epoch 425/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1130 - acc: 0.9693 - val_loss: 0.1570 - val_acc: 0.9707\n",
      "Epoch 426/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1132 - acc: 0.9693 - val_loss: 0.1533 - val_acc: 0.9704\n",
      "Epoch 427/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1126 - acc: 0.9692 - val_loss: 0.1555 - val_acc: 0.9707\n",
      "Epoch 428/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1131 - acc: 0.9688 - val_loss: 0.1546 - val_acc: 0.9707\n",
      "Epoch 429/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1141 - acc: 0.9690 - val_loss: 0.1568 - val_acc: 0.9704\n",
      "Epoch 430/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1166 - acc: 0.9683 - val_loss: 0.1571 - val_acc: 0.9702\n",
      "Epoch 431/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1138 - acc: 0.9692 - val_loss: 0.1556 - val_acc: 0.9704\n",
      "Epoch 432/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1134 - acc: 0.9691 - val_loss: 0.1538 - val_acc: 0.9709\n",
      "Epoch 433/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1140 - acc: 0.9687 - val_loss: 0.1526 - val_acc: 0.9704\n",
      "Epoch 434/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1138 - acc: 0.9691 - val_loss: 0.1532 - val_acc: 0.9707\n",
      "Epoch 435/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1129 - acc: 0.9688 - val_loss: 0.1576 - val_acc: 0.9709\n",
      "Epoch 436/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1131 - acc: 0.9687 - val_loss: 0.1565 - val_acc: 0.9709\n",
      "Epoch 437/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1134 - acc: 0.9692 - val_loss: 0.1521 - val_acc: 0.9709\n",
      "Epoch 438/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1130 - acc: 0.9690 - val_loss: 0.1530 - val_acc: 0.9707\n",
      "Epoch 439/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1126 - acc: 0.9692 - val_loss: 0.1522 - val_acc: 0.9707\n",
      "Epoch 440/1000\n",
      "16079/16079 [==============================] - 2s 111us/step - loss: 0.1134 - acc: 0.9692 - val_loss: 0.1534 - val_acc: 0.9704\n",
      "Epoch 441/1000\n",
      "16079/16079 [==============================] - 2s 118us/step - loss: 0.1161 - acc: 0.9690 - val_loss: 0.1526 - val_acc: 0.9714\n",
      "Epoch 442/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1142 - acc: 0.9691 - val_loss: 0.1508 - val_acc: 0.9707\n",
      "Epoch 443/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1160 - acc: 0.9687 - val_loss: 0.1518 - val_acc: 0.9707\n",
      "Epoch 444/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1141 - acc: 0.9690 - val_loss: 0.1526 - val_acc: 0.9712\n",
      "Epoch 445/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1137 - acc: 0.9691 - val_loss: 0.1545 - val_acc: 0.9709\n",
      "Epoch 446/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1135 - acc: 0.9690 - val_loss: 0.1612 - val_acc: 0.9709\n",
      "Epoch 447/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1145 - acc: 0.9687 - val_loss: 0.1524 - val_acc: 0.9714\n",
      "Epoch 448/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1129 - acc: 0.9690 - val_loss: 0.1509 - val_acc: 0.9709\n",
      "Epoch 449/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1127 - acc: 0.9693 - val_loss: 0.1516 - val_acc: 0.9709\n",
      "Epoch 450/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1122 - acc: 0.9693 - val_loss: 0.1523 - val_acc: 0.9709\n",
      "Epoch 451/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1130 - acc: 0.9691 - val_loss: 0.1556 - val_acc: 0.9707\n",
      "Epoch 452/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1121 - acc: 0.9692 - val_loss: 0.1555 - val_acc: 0.9704\n",
      "Epoch 453/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1131 - acc: 0.9689 - val_loss: 0.1562 - val_acc: 0.9709\n",
      "Epoch 454/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1157 - acc: 0.9684 - val_loss: 0.1553 - val_acc: 0.9704\n",
      "Epoch 455/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1125 - acc: 0.9693 - val_loss: 0.1538 - val_acc: 0.9704\n",
      "Epoch 456/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1125 - acc: 0.9695 - val_loss: 0.1552 - val_acc: 0.9709\n",
      "Epoch 457/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1129 - acc: 0.9692 - val_loss: 0.1488 - val_acc: 0.9707\n",
      "Epoch 458/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1133 - acc: 0.9688 - val_loss: 0.1535 - val_acc: 0.9712\n",
      "Epoch 459/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1126 - acc: 0.9693 - val_loss: 0.1574 - val_acc: 0.9707\n",
      "Epoch 460/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1126 - acc: 0.9695 - val_loss: 0.1521 - val_acc: 0.9709\n",
      "Epoch 461/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1119 - acc: 0.9690 - val_loss: 0.1563 - val_acc: 0.9702\n",
      "Epoch 462/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1128 - acc: 0.9693 - val_loss: 0.1544 - val_acc: 0.9707\n",
      "Epoch 463/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1124 - acc: 0.9693 - val_loss: 0.1556 - val_acc: 0.9709\n",
      "Epoch 464/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1141 - acc: 0.9688 - val_loss: 0.1554 - val_acc: 0.9709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 465/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1125 - acc: 0.9692 - val_loss: 0.1545 - val_acc: 0.9704\n",
      "Epoch 466/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1123 - acc: 0.9689 - val_loss: 0.1541 - val_acc: 0.9709\n",
      "Epoch 467/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1125 - acc: 0.9690 - val_loss: 0.1531 - val_acc: 0.9704\n",
      "Epoch 468/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1117 - acc: 0.9693 - val_loss: 0.1553 - val_acc: 0.9714\n",
      "Epoch 469/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1115 - acc: 0.9697 - val_loss: 0.1539 - val_acc: 0.9707\n",
      "Epoch 470/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1126 - acc: 0.9692 - val_loss: 0.1549 - val_acc: 0.9702\n",
      "Epoch 471/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1120 - acc: 0.9692 - val_loss: 0.1545 - val_acc: 0.9704\n",
      "Epoch 472/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1124 - acc: 0.9691 - val_loss: 0.1557 - val_acc: 0.9707\n",
      "Epoch 473/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1135 - acc: 0.9688 - val_loss: 0.1574 - val_acc: 0.9692\n",
      "Epoch 474/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1136 - acc: 0.9691 - val_loss: 0.1575 - val_acc: 0.9702\n",
      "Epoch 475/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1143 - acc: 0.9693 - val_loss: 0.1608 - val_acc: 0.9704\n",
      "Epoch 476/1000\n",
      "16079/16079 [==============================] - 1s 75us/step - loss: 0.1125 - acc: 0.9693 - val_loss: 0.1605 - val_acc: 0.9707\n",
      "Epoch 477/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1142 - acc: 0.9691 - val_loss: 0.1574 - val_acc: 0.9709\n",
      "Epoch 478/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1121 - acc: 0.9692 - val_loss: 0.1593 - val_acc: 0.9704\n",
      "Epoch 479/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1106 - acc: 0.9695 - val_loss: 0.1580 - val_acc: 0.9704\n",
      "Epoch 480/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1111 - acc: 0.9695 - val_loss: 0.1554 - val_acc: 0.9712\n",
      "Epoch 481/1000\n",
      "16079/16079 [==============================] - 1s 81us/step - loss: 0.1112 - acc: 0.9695 - val_loss: 0.1560 - val_acc: 0.9704\n",
      "Epoch 482/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1125 - acc: 0.9692 - val_loss: 0.1679 - val_acc: 0.9709\n",
      "Epoch 483/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1126 - acc: 0.9694 - val_loss: 0.1555 - val_acc: 0.9712\n",
      "Epoch 484/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1124 - acc: 0.9690 - val_loss: 0.1549 - val_acc: 0.9702\n",
      "Epoch 485/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1126 - acc: 0.9693 - val_loss: 0.1574 - val_acc: 0.9704\n",
      "Epoch 486/1000\n",
      "16079/16079 [==============================] - 2s 111us/step - loss: 0.1118 - acc: 0.9693 - val_loss: 0.1564 - val_acc: 0.9702\n",
      "Epoch 487/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1116 - acc: 0.9694 - val_loss: 0.1538 - val_acc: 0.9707\n",
      "Epoch 488/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1116 - acc: 0.9694 - val_loss: 0.1590 - val_acc: 0.9704\n",
      "Epoch 489/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1159 - acc: 0.9688 - val_loss: 0.1576 - val_acc: 0.9689\n",
      "Epoch 490/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1130 - acc: 0.9690 - val_loss: 0.1562 - val_acc: 0.9704\n",
      "Epoch 491/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1115 - acc: 0.9692 - val_loss: 0.1554 - val_acc: 0.9707\n",
      "Epoch 492/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1130 - acc: 0.9694 - val_loss: 0.1522 - val_acc: 0.9709\n",
      "Epoch 493/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1120 - acc: 0.9694 - val_loss: 0.1529 - val_acc: 0.9697\n",
      "Epoch 494/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1113 - acc: 0.9693 - val_loss: 0.1554 - val_acc: 0.9709\n",
      "Epoch 495/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1109 - acc: 0.9695 - val_loss: 0.1607 - val_acc: 0.9702\n",
      "Epoch 496/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1107 - acc: 0.9695 - val_loss: 0.1570 - val_acc: 0.9699\n",
      "Epoch 497/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1138 - acc: 0.9688 - val_loss: 0.1565 - val_acc: 0.9699\n",
      "Epoch 498/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1124 - acc: 0.9694 - val_loss: 0.1553 - val_acc: 0.9704\n",
      "Epoch 499/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1105 - acc: 0.9696 - val_loss: 0.1597 - val_acc: 0.9704\n",
      "Epoch 500/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1115 - acc: 0.9688 - val_loss: 0.1569 - val_acc: 0.9707\n",
      "Epoch 501/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1143 - acc: 0.9690 - val_loss: 0.1566 - val_acc: 0.9697\n",
      "Epoch 502/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1144 - acc: 0.9686 - val_loss: 0.1546 - val_acc: 0.9712\n",
      "Epoch 503/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1117 - acc: 0.9694 - val_loss: 0.1594 - val_acc: 0.9712\n",
      "Epoch 504/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1108 - acc: 0.9690 - val_loss: 0.1562 - val_acc: 0.9707\n",
      "Epoch 505/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1111 - acc: 0.9695 - val_loss: 0.1575 - val_acc: 0.9709\n",
      "Epoch 506/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1110 - acc: 0.9693 - val_loss: 0.1623 - val_acc: 0.9699\n",
      "Epoch 507/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1108 - acc: 0.9698 - val_loss: 0.1596 - val_acc: 0.9707\n",
      "Epoch 508/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1120 - acc: 0.9693 - val_loss: 0.1581 - val_acc: 0.9707\n",
      "Epoch 509/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1107 - acc: 0.9700 - val_loss: 0.1564 - val_acc: 0.9707\n",
      "Epoch 510/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1104 - acc: 0.9694 - val_loss: 0.1605 - val_acc: 0.9699\n",
      "Epoch 511/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1113 - acc: 0.9697 - val_loss: 0.1566 - val_acc: 0.9707\n",
      "Epoch 512/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1109 - acc: 0.9695 - val_loss: 0.1580 - val_acc: 0.9704\n",
      "Epoch 513/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1102 - acc: 0.9694 - val_loss: 0.1581 - val_acc: 0.9707\n",
      "Epoch 514/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1109 - acc: 0.9692 - val_loss: 0.1561 - val_acc: 0.9709\n",
      "Epoch 515/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1127 - acc: 0.9693 - val_loss: 0.1531 - val_acc: 0.9707\n",
      "Epoch 516/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1154 - acc: 0.9690 - val_loss: 0.1525 - val_acc: 0.9704\n",
      "Epoch 517/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1115 - acc: 0.9692 - val_loss: 0.1590 - val_acc: 0.9702\n",
      "Epoch 518/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1137 - acc: 0.9689 - val_loss: 0.1590 - val_acc: 0.9707\n",
      "Epoch 519/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1107 - acc: 0.9690 - val_loss: 0.1608 - val_acc: 0.9697\n",
      "Epoch 520/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1134 - acc: 0.9689 - val_loss: 0.1574 - val_acc: 0.9704\n",
      "Epoch 521/1000\n",
      "16079/16079 [==============================] - 2s 137us/step - loss: 0.1102 - acc: 0.9693 - val_loss: 0.1590 - val_acc: 0.9709\n",
      "Epoch 522/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1112 - acc: 0.9696 - val_loss: 0.1583 - val_acc: 0.9707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 523/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1106 - acc: 0.9696 - val_loss: 0.1583 - val_acc: 0.9709\n",
      "Epoch 524/1000\n",
      "16079/16079 [==============================] - 2s 122us/step - loss: 0.1112 - acc: 0.9694 - val_loss: 0.1569 - val_acc: 0.9704\n",
      "Epoch 525/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1104 - acc: 0.9692 - val_loss: 0.1487 - val_acc: 0.9707\n",
      "Epoch 526/1000\n",
      "16079/16079 [==============================] - 2s 122us/step - loss: 0.1093 - acc: 0.9696 - val_loss: 0.1699 - val_acc: 0.9709\n",
      "Epoch 527/1000\n",
      "16079/16079 [==============================] - 2s 111us/step - loss: 0.1108 - acc: 0.9691 - val_loss: 0.1576 - val_acc: 0.9699\n",
      "Epoch 528/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1096 - acc: 0.9691 - val_loss: 0.1579 - val_acc: 0.9707\n",
      "Epoch 529/1000\n",
      "16079/16079 [==============================] - 2s 113us/step - loss: 0.1100 - acc: 0.9696 - val_loss: 0.1565 - val_acc: 0.9709\n",
      "Epoch 530/1000\n",
      "16079/16079 [==============================] - 2s 119us/step - loss: 0.1133 - acc: 0.9687 - val_loss: 0.1571 - val_acc: 0.9702\n",
      "Epoch 531/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1102 - acc: 0.9692 - val_loss: 0.1609 - val_acc: 0.9704\n",
      "Epoch 532/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1106 - acc: 0.9695 - val_loss: 0.1576 - val_acc: 0.9702\n",
      "Epoch 533/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1111 - acc: 0.9694 - val_loss: 0.1630 - val_acc: 0.9707\n",
      "Epoch 534/1000\n",
      "16079/16079 [==============================] - 2s 113us/step - loss: 0.1099 - acc: 0.9701 - val_loss: 0.1546 - val_acc: 0.9699\n",
      "Epoch 535/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1118 - acc: 0.9690 - val_loss: 0.1570 - val_acc: 0.9704\n",
      "Epoch 536/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1119 - acc: 0.9691 - val_loss: 0.1549 - val_acc: 0.9704\n",
      "Epoch 537/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1110 - acc: 0.9693 - val_loss: 0.1565 - val_acc: 0.9704\n",
      "Epoch 538/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1141 - acc: 0.9695 - val_loss: 0.1524 - val_acc: 0.9707\n",
      "Epoch 539/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1117 - acc: 0.9693 - val_loss: 0.1616 - val_acc: 0.9709\n",
      "Epoch 540/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1158 - acc: 0.9688 - val_loss: 0.1607 - val_acc: 0.9707\n",
      "Epoch 541/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1125 - acc: 0.9693 - val_loss: 0.1576 - val_acc: 0.9699\n",
      "Epoch 542/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1119 - acc: 0.9691 - val_loss: 0.1556 - val_acc: 0.9704\n",
      "Epoch 543/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1116 - acc: 0.9692 - val_loss: 0.1581 - val_acc: 0.9709\n",
      "Epoch 544/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1114 - acc: 0.9697 - val_loss: 0.1589 - val_acc: 0.9709\n",
      "Epoch 545/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1114 - acc: 0.9693 - val_loss: 0.1575 - val_acc: 0.9709\n",
      "Epoch 546/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1121 - acc: 0.9692 - val_loss: 0.1615 - val_acc: 0.9694\n",
      "Epoch 547/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1147 - acc: 0.9686 - val_loss: 0.1583 - val_acc: 0.9707\n",
      "Epoch 548/1000\n",
      "16079/16079 [==============================] - 2s 118us/step - loss: 0.1121 - acc: 0.9690 - val_loss: 0.1571 - val_acc: 0.9712\n",
      "Epoch 549/1000\n",
      "16079/16079 [==============================] - 2s 111us/step - loss: 0.1102 - acc: 0.9692 - val_loss: 0.1615 - val_acc: 0.9709\n",
      "Epoch 550/1000\n",
      "16079/16079 [==============================] - 2s 130us/step - loss: 0.1106 - acc: 0.9691 - val_loss: 0.1593 - val_acc: 0.9704\n",
      "Epoch 551/1000\n",
      "16079/16079 [==============================] - 2s 124us/step - loss: 0.1113 - acc: 0.9698 - val_loss: 0.1621 - val_acc: 0.9702\n",
      "Epoch 552/1000\n",
      "16079/16079 [==============================] - 2s 124us/step - loss: 0.1100 - acc: 0.9700 - val_loss: 0.1632 - val_acc: 0.9707\n",
      "Epoch 553/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1094 - acc: 0.9698 - val_loss: 0.1559 - val_acc: 0.9707\n",
      "Epoch 554/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1092 - acc: 0.9697 - val_loss: 0.1595 - val_acc: 0.9704\n",
      "Epoch 555/1000\n",
      "16079/16079 [==============================] - 2s 117us/step - loss: 0.1102 - acc: 0.9696 - val_loss: 0.1584 - val_acc: 0.9704\n",
      "Epoch 556/1000\n",
      "16079/16079 [==============================] - 2s 121us/step - loss: 0.1093 - acc: 0.9698 - val_loss: 0.1582 - val_acc: 0.9697\n",
      "Epoch 557/1000\n",
      "16079/16079 [==============================] - 2s 133us/step - loss: 0.1094 - acc: 0.9696 - val_loss: 0.1553 - val_acc: 0.9704\n",
      "Epoch 558/1000\n",
      "16079/16079 [==============================] - 2s 124us/step - loss: 0.1098 - acc: 0.9695 - val_loss: 0.1581 - val_acc: 0.9704\n",
      "Epoch 559/1000\n",
      "16079/16079 [==============================] - 2s 115us/step - loss: 0.1106 - acc: 0.9697 - val_loss: 0.1596 - val_acc: 0.9707\n",
      "Epoch 560/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1096 - acc: 0.9694 - val_loss: 0.1600 - val_acc: 0.9707\n",
      "Epoch 561/1000\n",
      "16079/16079 [==============================] - 2s 118us/step - loss: 0.1089 - acc: 0.9696 - val_loss: 0.1584 - val_acc: 0.9704\n",
      "Epoch 562/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1114 - acc: 0.9691 - val_loss: 0.1559 - val_acc: 0.9699\n",
      "Epoch 563/1000\n",
      "16079/16079 [==============================] - 2s 125us/step - loss: 0.1109 - acc: 0.9692 - val_loss: 0.1612 - val_acc: 0.9702\n",
      "Epoch 564/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1097 - acc: 0.9693 - val_loss: 0.1552 - val_acc: 0.9702\n",
      "Epoch 565/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1141 - acc: 0.9688 - val_loss: 0.1603 - val_acc: 0.9704\n",
      "Epoch 566/1000\n",
      "16079/16079 [==============================] - 2s 113us/step - loss: 0.1104 - acc: 0.9692 - val_loss: 0.1614 - val_acc: 0.9707\n",
      "Epoch 567/1000\n",
      "16079/16079 [==============================] - 2s 115us/step - loss: 0.1097 - acc: 0.9696 - val_loss: 0.1579 - val_acc: 0.9702\n",
      "Epoch 568/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1093 - acc: 0.9696 - val_loss: 0.1615 - val_acc: 0.9702\n",
      "Epoch 569/1000\n",
      "16079/16079 [==============================] - 2s 115us/step - loss: 0.1092 - acc: 0.9698 - val_loss: 0.1557 - val_acc: 0.9702\n",
      "Epoch 570/1000\n",
      "16079/16079 [==============================] - 2s 118us/step - loss: 0.1098 - acc: 0.9690 - val_loss: 0.1631 - val_acc: 0.9692\n",
      "Epoch 571/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1094 - acc: 0.9697 - val_loss: 0.1582 - val_acc: 0.9707\n",
      "Epoch 572/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1109 - acc: 0.9696 - val_loss: 0.1588 - val_acc: 0.9709\n",
      "Epoch 573/1000\n",
      "16079/16079 [==============================] - 2s 126us/step - loss: 0.1145 - acc: 0.9692 - val_loss: 0.1572 - val_acc: 0.9704\n",
      "Epoch 574/1000\n",
      "16079/16079 [==============================] - 2s 115us/step - loss: 0.1094 - acc: 0.9694 - val_loss: 0.1593 - val_acc: 0.9709\n",
      "Epoch 575/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1088 - acc: 0.9699 - val_loss: 0.1629 - val_acc: 0.9707\n",
      "Epoch 576/1000\n",
      "16079/16079 [==============================] - 2s 118us/step - loss: 0.1104 - acc: 0.9692 - val_loss: 0.1591 - val_acc: 0.9704\n",
      "Epoch 577/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1094 - acc: 0.9696 - val_loss: 0.1577 - val_acc: 0.9702\n",
      "Epoch 578/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1090 - acc: 0.9696 - val_loss: 0.1624 - val_acc: 0.9712\n",
      "Epoch 579/1000\n",
      "16079/16079 [==============================] - 2s 117us/step - loss: 0.1086 - acc: 0.9696 - val_loss: 0.1594 - val_acc: 0.9709\n",
      "Epoch 580/1000\n",
      "16079/16079 [==============================] - 2s 112us/step - loss: 0.1090 - acc: 0.9694 - val_loss: 0.1585 - val_acc: 0.9707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 581/1000\n",
      "16079/16079 [==============================] - 2s 120us/step - loss: 0.1079 - acc: 0.9700 - val_loss: 0.1675 - val_acc: 0.9699\n",
      "Epoch 582/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1096 - acc: 0.9695 - val_loss: 0.1595 - val_acc: 0.9704\n",
      "Epoch 583/1000\n",
      "16079/16079 [==============================] - 2s 118us/step - loss: 0.1092 - acc: 0.9698 - val_loss: 0.1593 - val_acc: 0.9704\n",
      "Epoch 584/1000\n",
      "16079/16079 [==============================] - 2s 122us/step - loss: 0.1095 - acc: 0.9692 - val_loss: 0.1618 - val_acc: 0.9702\n",
      "Epoch 585/1000\n",
      "16079/16079 [==============================] - 2s 112us/step - loss: 0.1103 - acc: 0.9690 - val_loss: 0.1600 - val_acc: 0.9707\n",
      "Epoch 586/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1081 - acc: 0.9692 - val_loss: 0.1640 - val_acc: 0.9704\n",
      "Epoch 587/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1087 - acc: 0.9693 - val_loss: 0.1619 - val_acc: 0.9704\n",
      "Epoch 588/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1087 - acc: 0.9694 - val_loss: 0.1614 - val_acc: 0.9699\n",
      "Epoch 589/1000\n",
      "16079/16079 [==============================] - 2s 121us/step - loss: 0.1078 - acc: 0.9701 - val_loss: 0.1587 - val_acc: 0.9702\n",
      "Epoch 590/1000\n",
      "16079/16079 [==============================] - 2s 131us/step - loss: 0.1093 - acc: 0.9701 - val_loss: 0.1596 - val_acc: 0.9702\n",
      "Epoch 591/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1083 - acc: 0.9696 - val_loss: 0.1610 - val_acc: 0.9699\n",
      "Epoch 592/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1110 - acc: 0.9694 - val_loss: 0.1579 - val_acc: 0.9709\n",
      "Epoch 593/1000\n",
      "16079/16079 [==============================] - 2s 122us/step - loss: 0.1088 - acc: 0.9698 - val_loss: 0.1619 - val_acc: 0.9704\n",
      "Epoch 594/1000\n",
      "16079/16079 [==============================] - 2s 109us/step - loss: 0.1099 - acc: 0.9695 - val_loss: 0.1631 - val_acc: 0.9707\n",
      "Epoch 595/1000\n",
      "16079/16079 [==============================] - 2s 120us/step - loss: 0.1098 - acc: 0.9695 - val_loss: 0.1624 - val_acc: 0.9704\n",
      "Epoch 596/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1073 - acc: 0.9696 - val_loss: 0.1646 - val_acc: 0.9709\n",
      "Epoch 597/1000\n",
      "16079/16079 [==============================] - 2s 124us/step - loss: 0.1084 - acc: 0.9695 - val_loss: 0.1593 - val_acc: 0.9707\n",
      "Epoch 598/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1091 - acc: 0.9701 - val_loss: 0.1591 - val_acc: 0.9702\n",
      "Epoch 599/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1107 - acc: 0.9691 - val_loss: 0.1604 - val_acc: 0.9702\n",
      "Epoch 600/1000\n",
      "16079/16079 [==============================] - 2s 132us/step - loss: 0.1088 - acc: 0.9696 - val_loss: 0.1667 - val_acc: 0.9702\n",
      "Epoch 601/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1078 - acc: 0.9698 - val_loss: 0.1610 - val_acc: 0.9704\n",
      "Epoch 602/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1089 - acc: 0.9695 - val_loss: 0.1597 - val_acc: 0.9704\n",
      "Epoch 603/1000\n",
      "16079/16079 [==============================] - 2s 93us/step - loss: 0.1090 - acc: 0.9696 - val_loss: 0.1608 - val_acc: 0.9709\n",
      "Epoch 604/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1088 - acc: 0.9699 - val_loss: 0.1600 - val_acc: 0.9704\n",
      "Epoch 605/1000\n",
      "16079/16079 [==============================] - 2s 135us/step - loss: 0.1134 - acc: 0.9691 - val_loss: 0.1629 - val_acc: 0.9709\n",
      "Epoch 606/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1111 - acc: 0.9693 - val_loss: 0.1699 - val_acc: 0.9697\n",
      "Epoch 607/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1126 - acc: 0.9695 - val_loss: 0.1575 - val_acc: 0.9702\n",
      "Epoch 608/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1133 - acc: 0.9695 - val_loss: 0.1556 - val_acc: 0.9694\n",
      "Epoch 609/1000\n",
      "16079/16079 [==============================] - 2s 93us/step - loss: 0.1115 - acc: 0.9695 - val_loss: 0.1595 - val_acc: 0.9699\n",
      "Epoch 610/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1100 - acc: 0.9697 - val_loss: 0.1654 - val_acc: 0.9709\n",
      "Epoch 611/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1100 - acc: 0.9695 - val_loss: 0.1627 - val_acc: 0.9707\n",
      "Epoch 612/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1101 - acc: 0.9698 - val_loss: 0.1664 - val_acc: 0.9694\n",
      "Epoch 613/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1095 - acc: 0.9697 - val_loss: 0.1610 - val_acc: 0.9702\n",
      "Epoch 614/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1090 - acc: 0.9695 - val_loss: 0.1625 - val_acc: 0.9707\n",
      "Epoch 615/1000\n",
      "16079/16079 [==============================] - 2s 115us/step - loss: 0.1105 - acc: 0.9692 - val_loss: 0.1609 - val_acc: 0.9709\n",
      "Epoch 616/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1091 - acc: 0.9695 - val_loss: 0.1607 - val_acc: 0.9704\n",
      "Epoch 617/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1095 - acc: 0.9698 - val_loss: 0.1679 - val_acc: 0.9697\n",
      "Epoch 618/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1128 - acc: 0.9698 - val_loss: 0.1573 - val_acc: 0.9702\n",
      "Epoch 619/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1090 - acc: 0.9698 - val_loss: 0.1635 - val_acc: 0.9704\n",
      "Epoch 620/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1100 - acc: 0.9696 - val_loss: 0.1557 - val_acc: 0.9712\n",
      "Epoch 621/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1096 - acc: 0.9696 - val_loss: 0.1587 - val_acc: 0.9702\n",
      "Epoch 622/1000\n",
      "16079/16079 [==============================] - 2s 109us/step - loss: 0.1134 - acc: 0.9692 - val_loss: 0.1609 - val_acc: 0.9704\n",
      "Epoch 623/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1098 - acc: 0.9697 - val_loss: 0.1673 - val_acc: 0.9707\n",
      "Epoch 624/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1079 - acc: 0.9697 - val_loss: 0.1605 - val_acc: 0.9702\n",
      "Epoch 625/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1078 - acc: 0.9695 - val_loss: 0.1585 - val_acc: 0.9712\n",
      "Epoch 626/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1089 - acc: 0.9693 - val_loss: 0.1586 - val_acc: 0.9704\n",
      "Epoch 627/1000\n",
      "16079/16079 [==============================] - 2s 109us/step - loss: 0.1074 - acc: 0.9695 - val_loss: 0.1687 - val_acc: 0.9704\n",
      "Epoch 628/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1085 - acc: 0.9700 - val_loss: 0.1623 - val_acc: 0.9707\n",
      "Epoch 629/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1079 - acc: 0.9699 - val_loss: 0.1631 - val_acc: 0.9707\n",
      "Epoch 630/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1092 - acc: 0.9694 - val_loss: 0.1659 - val_acc: 0.9697\n",
      "Epoch 631/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1077 - acc: 0.9700 - val_loss: 0.1584 - val_acc: 0.9702\n",
      "Epoch 632/1000\n",
      "16079/16079 [==============================] - 2s 113us/step - loss: 0.1097 - acc: 0.9692 - val_loss: 0.1635 - val_acc: 0.9702\n",
      "Epoch 633/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1097 - acc: 0.9695 - val_loss: 0.1548 - val_acc: 0.9709\n",
      "Epoch 634/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1103 - acc: 0.9698 - val_loss: 0.1565 - val_acc: 0.9704\n",
      "Epoch 635/1000\n",
      "16079/16079 [==============================] - 2s 112us/step - loss: 0.1085 - acc: 0.9696 - val_loss: 0.1745 - val_acc: 0.9702\n",
      "Epoch 636/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1114 - acc: 0.9694 - val_loss: 0.1647 - val_acc: 0.9699\n",
      "Epoch 637/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1111 - acc: 0.9698 - val_loss: 0.1573 - val_acc: 0.9709\n",
      "Epoch 638/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1085 - acc: 0.9694 - val_loss: 0.1618 - val_acc: 0.9704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 639/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1088 - acc: 0.9696 - val_loss: 0.1600 - val_acc: 0.9707\n",
      "Epoch 640/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1081 - acc: 0.9698 - val_loss: 0.1618 - val_acc: 0.9709\n",
      "Epoch 641/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1144 - acc: 0.9689 - val_loss: 0.1590 - val_acc: 0.9697\n",
      "Epoch 642/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1088 - acc: 0.9695 - val_loss: 0.1590 - val_acc: 0.9699\n",
      "Epoch 643/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1079 - acc: 0.9695 - val_loss: 0.1670 - val_acc: 0.9699\n",
      "Epoch 644/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1092 - acc: 0.9696 - val_loss: 0.1602 - val_acc: 0.9699\n",
      "Epoch 645/1000\n",
      "16079/16079 [==============================] - 2s 148us/step - loss: 0.1090 - acc: 0.9697 - val_loss: 0.1579 - val_acc: 0.9704\n",
      "Epoch 646/1000\n",
      "16079/16079 [==============================] - 2s 137us/step - loss: 0.1097 - acc: 0.9698 - val_loss: 0.1630 - val_acc: 0.9697\n",
      "Epoch 647/1000\n",
      "16079/16079 [==============================] - 2s 115us/step - loss: 0.1085 - acc: 0.9701 - val_loss: 0.1673 - val_acc: 0.9699\n",
      "Epoch 648/1000\n",
      "16079/16079 [==============================] - 2s 125us/step - loss: 0.1087 - acc: 0.9699 - val_loss: 0.1612 - val_acc: 0.9704\n",
      "Epoch 649/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1148 - acc: 0.9682 - val_loss: 0.1610 - val_acc: 0.9702\n",
      "Epoch 650/1000\n",
      "16079/16079 [==============================] - 2s 126us/step - loss: 0.1091 - acc: 0.9690 - val_loss: 0.1567 - val_acc: 0.9707\n",
      "Epoch 651/1000\n",
      "16079/16079 [==============================] - 2s 126us/step - loss: 0.1072 - acc: 0.9696 - val_loss: 0.1608 - val_acc: 0.9692\n",
      "Epoch 652/1000\n",
      "16079/16079 [==============================] - 2s 129us/step - loss: 0.1060 - acc: 0.9700 - val_loss: 0.1728 - val_acc: 0.9707\n",
      "Epoch 653/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1085 - acc: 0.9697 - val_loss: 0.1671 - val_acc: 0.9702\n",
      "Epoch 654/1000\n",
      "16079/16079 [==============================] - 2s 118us/step - loss: 0.1080 - acc: 0.9701 - val_loss: 0.1632 - val_acc: 0.9702\n",
      "Epoch 655/1000\n",
      "16079/16079 [==============================] - 2s 121us/step - loss: 0.1084 - acc: 0.9700 - val_loss: 0.1634 - val_acc: 0.9707\n",
      "Epoch 656/1000\n",
      "16079/16079 [==============================] - 2s 150us/step - loss: 0.1089 - acc: 0.9691 - val_loss: 0.1599 - val_acc: 0.9699\n",
      "Epoch 657/1000\n",
      "16079/16079 [==============================] - 2s 139us/step - loss: 0.1061 - acc: 0.9702 - val_loss: 0.1624 - val_acc: 0.9699\n",
      "Epoch 658/1000\n",
      "16079/16079 [==============================] - 2s 136us/step - loss: 0.1075 - acc: 0.9696 - val_loss: 0.1654 - val_acc: 0.9697\n",
      "Epoch 659/1000\n",
      "16079/16079 [==============================] - 2s 119us/step - loss: 0.1061 - acc: 0.9705 - val_loss: 0.1656 - val_acc: 0.9702\n",
      "Epoch 660/1000\n",
      "16079/16079 [==============================] - 2s 129us/step - loss: 0.1075 - acc: 0.9696 - val_loss: 0.1614 - val_acc: 0.9707\n",
      "Epoch 661/1000\n",
      "16079/16079 [==============================] - 2s 130us/step - loss: 0.1086 - acc: 0.9696 - val_loss: 0.1618 - val_acc: 0.9709\n",
      "Epoch 662/1000\n",
      "16079/16079 [==============================] - 2s 112us/step - loss: 0.1090 - acc: 0.9700 - val_loss: 0.1755 - val_acc: 0.9704\n",
      "Epoch 663/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1098 - acc: 0.9694 - val_loss: 0.1606 - val_acc: 0.9702\n",
      "Epoch 664/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1077 - acc: 0.9696 - val_loss: 0.1727 - val_acc: 0.9702\n",
      "Epoch 665/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1078 - acc: 0.9700 - val_loss: 0.1665 - val_acc: 0.9702\n",
      "Epoch 666/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1102 - acc: 0.9694 - val_loss: 0.1632 - val_acc: 0.9704\n",
      "Epoch 667/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1095 - acc: 0.9695 - val_loss: 0.1635 - val_acc: 0.9707\n",
      "Epoch 668/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1088 - acc: 0.9700 - val_loss: 0.1633 - val_acc: 0.9702\n",
      "Epoch 669/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1071 - acc: 0.9703 - val_loss: 0.1637 - val_acc: 0.9707\n",
      "Epoch 670/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1078 - acc: 0.9698 - val_loss: 0.1659 - val_acc: 0.9702\n",
      "Epoch 671/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1075 - acc: 0.9703 - val_loss: 0.1620 - val_acc: 0.9694\n",
      "Epoch 672/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1137 - acc: 0.9685 - val_loss: 0.1630 - val_acc: 0.9699\n",
      "Epoch 673/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1093 - acc: 0.9692 - val_loss: 0.1597 - val_acc: 0.9709\n",
      "Epoch 674/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1090 - acc: 0.9692 - val_loss: 0.1689 - val_acc: 0.9697\n",
      "Epoch 675/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1080 - acc: 0.9699 - val_loss: 0.1669 - val_acc: 0.9699\n",
      "Epoch 676/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1085 - acc: 0.9696 - val_loss: 0.1672 - val_acc: 0.9682\n",
      "Epoch 677/1000\n",
      "16079/16079 [==============================] - 1s 84us/step - loss: 0.1091 - acc: 0.9689 - val_loss: 0.1704 - val_acc: 0.9699\n",
      "Epoch 678/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1080 - acc: 0.9700 - val_loss: 0.1676 - val_acc: 0.9707\n",
      "Epoch 679/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1091 - acc: 0.9700 - val_loss: 0.1657 - val_acc: 0.9704\n",
      "Epoch 680/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1085 - acc: 0.9696 - val_loss: 0.1660 - val_acc: 0.9709\n",
      "Epoch 681/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1102 - acc: 0.9696 - val_loss: 0.1650 - val_acc: 0.9704\n",
      "Epoch 682/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1074 - acc: 0.9701 - val_loss: 0.1662 - val_acc: 0.9702\n",
      "Epoch 683/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1084 - acc: 0.9698 - val_loss: 0.1607 - val_acc: 0.9697\n",
      "Epoch 684/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1079 - acc: 0.9698 - val_loss: 0.1740 - val_acc: 0.9707\n",
      "Epoch 685/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1090 - acc: 0.9694 - val_loss: 0.1634 - val_acc: 0.9702\n",
      "Epoch 686/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1095 - acc: 0.9692 - val_loss: 0.1596 - val_acc: 0.9707\n",
      "Epoch 687/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1074 - acc: 0.9698 - val_loss: 0.1748 - val_acc: 0.9702\n",
      "Epoch 688/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1087 - acc: 0.9701 - val_loss: 0.1687 - val_acc: 0.9697\n",
      "Epoch 689/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1094 - acc: 0.9700 - val_loss: 0.1656 - val_acc: 0.9699\n",
      "Epoch 690/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1086 - acc: 0.9696 - val_loss: 0.1628 - val_acc: 0.9707\n",
      "Epoch 691/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1064 - acc: 0.9701 - val_loss: 0.1619 - val_acc: 0.9697\n",
      "Epoch 692/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1091 - acc: 0.9695 - val_loss: 0.1613 - val_acc: 0.9699\n",
      "Epoch 693/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1096 - acc: 0.9697 - val_loss: 0.1628 - val_acc: 0.9689\n",
      "Epoch 694/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1075 - acc: 0.9695 - val_loss: 0.1652 - val_acc: 0.9689\n",
      "Epoch 695/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1068 - acc: 0.9699 - val_loss: 0.1639 - val_acc: 0.9707\n",
      "Epoch 696/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1086 - acc: 0.9700 - val_loss: 0.1694 - val_acc: 0.9697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 697/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1075 - acc: 0.9695 - val_loss: 0.1675 - val_acc: 0.9702\n",
      "Epoch 698/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1094 - acc: 0.9694 - val_loss: 0.1670 - val_acc: 0.9702\n",
      "Epoch 699/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1083 - acc: 0.9693 - val_loss: 0.1613 - val_acc: 0.9689\n",
      "Epoch 700/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1112 - acc: 0.9696 - val_loss: 0.1612 - val_acc: 0.9697\n",
      "Epoch 701/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1082 - acc: 0.9695 - val_loss: 0.1644 - val_acc: 0.9699\n",
      "Epoch 702/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1076 - acc: 0.9698 - val_loss: 0.1627 - val_acc: 0.9702\n",
      "Epoch 703/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1075 - acc: 0.9701 - val_loss: 0.1656 - val_acc: 0.9707\n",
      "Epoch 704/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1077 - acc: 0.9698 - val_loss: 0.1696 - val_acc: 0.9699\n",
      "Epoch 705/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1064 - acc: 0.9704 - val_loss: 0.1704 - val_acc: 0.9709\n",
      "Epoch 706/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1098 - acc: 0.9696 - val_loss: 0.1751 - val_acc: 0.9692\n",
      "Epoch 707/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1083 - acc: 0.9698 - val_loss: 0.1635 - val_acc: 0.9704\n",
      "Epoch 708/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1087 - acc: 0.9697 - val_loss: 0.1636 - val_acc: 0.9707\n",
      "Epoch 709/1000\n",
      "16079/16079 [==============================] - 2s 113us/step - loss: 0.1074 - acc: 0.9700 - val_loss: 0.1665 - val_acc: 0.9707\n",
      "Epoch 710/1000\n",
      "16079/16079 [==============================] - 1s 85us/step - loss: 0.1079 - acc: 0.9695 - val_loss: 0.1601 - val_acc: 0.9704\n",
      "Epoch 711/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1080 - acc: 0.9696 - val_loss: 0.1656 - val_acc: 0.9712\n",
      "Epoch 712/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1080 - acc: 0.9694 - val_loss: 0.1663 - val_acc: 0.9702\n",
      "Epoch 713/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1070 - acc: 0.9700 - val_loss: 0.1669 - val_acc: 0.9694\n",
      "Epoch 714/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1131 - acc: 0.9685 - val_loss: 0.1629 - val_acc: 0.9709\n",
      "Epoch 715/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1076 - acc: 0.9698 - val_loss: 0.1638 - val_acc: 0.9697\n",
      "Epoch 716/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1060 - acc: 0.9703 - val_loss: 0.1729 - val_acc: 0.9712\n",
      "Epoch 717/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1076 - acc: 0.9700 - val_loss: 0.1710 - val_acc: 0.9699\n",
      "Epoch 718/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1062 - acc: 0.9702 - val_loss: 0.1662 - val_acc: 0.9697\n",
      "Epoch 719/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1077 - acc: 0.9698 - val_loss: 0.1650 - val_acc: 0.9699\n",
      "Epoch 720/1000\n",
      "16079/16079 [==============================] - ETA: 0s - loss: 0.1047 - acc: 0.970 - 1s 90us/step - loss: 0.1067 - acc: 0.9704 - val_loss: 0.1602 - val_acc: 0.9709\n",
      "Epoch 721/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1079 - acc: 0.9703 - val_loss: 0.1638 - val_acc: 0.9704\n",
      "Epoch 722/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1072 - acc: 0.9698 - val_loss: 0.1641 - val_acc: 0.9704\n",
      "Epoch 723/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1083 - acc: 0.9698 - val_loss: 0.1609 - val_acc: 0.9699\n",
      "Epoch 724/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1069 - acc: 0.9702 - val_loss: 0.1651 - val_acc: 0.9702\n",
      "Epoch 725/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1063 - acc: 0.9701 - val_loss: 0.1647 - val_acc: 0.9707\n",
      "Epoch 726/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1082 - acc: 0.9695 - val_loss: 0.1643 - val_acc: 0.9699\n",
      "Epoch 727/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1088 - acc: 0.9700 - val_loss: 0.1651 - val_acc: 0.9694\n",
      "Epoch 728/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1074 - acc: 0.9699 - val_loss: 0.1784 - val_acc: 0.9707\n",
      "Epoch 729/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1077 - acc: 0.9699 - val_loss: 0.1655 - val_acc: 0.9707\n",
      "Epoch 730/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1075 - acc: 0.9703 - val_loss: 0.1702 - val_acc: 0.9699\n",
      "Epoch 731/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1085 - acc: 0.9702 - val_loss: 0.1641 - val_acc: 0.9704\n",
      "Epoch 732/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1061 - acc: 0.9703 - val_loss: 0.1641 - val_acc: 0.9702\n",
      "Epoch 733/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1095 - acc: 0.9694 - val_loss: 0.1678 - val_acc: 0.9697\n",
      "Epoch 734/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1073 - acc: 0.9696 - val_loss: 0.1728 - val_acc: 0.9702\n",
      "Epoch 735/1000\n",
      "16079/16079 [==============================] - 2s 109us/step - loss: 0.1070 - acc: 0.9698 - val_loss: 0.1656 - val_acc: 0.9704\n",
      "Epoch 736/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1064 - acc: 0.9700 - val_loss: 0.1661 - val_acc: 0.9707\n",
      "Epoch 737/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1070 - acc: 0.9700 - val_loss: 0.1658 - val_acc: 0.9702\n",
      "Epoch 738/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1107 - acc: 0.9696 - val_loss: 0.1635 - val_acc: 0.9704\n",
      "Epoch 739/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1069 - acc: 0.9697 - val_loss: 0.1690 - val_acc: 0.9699\n",
      "Epoch 740/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1062 - acc: 0.9702 - val_loss: 0.1636 - val_acc: 0.9704\n",
      "Epoch 741/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1075 - acc: 0.9699 - val_loss: 0.1606 - val_acc: 0.9707\n",
      "Epoch 742/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1074 - acc: 0.9700 - val_loss: 0.1620 - val_acc: 0.9704\n",
      "Epoch 743/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1087 - acc: 0.9695 - val_loss: 0.1635 - val_acc: 0.9702\n",
      "Epoch 744/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1060 - acc: 0.9698 - val_loss: 0.1715 - val_acc: 0.9707\n",
      "Epoch 745/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1071 - acc: 0.9703 - val_loss: 0.1681 - val_acc: 0.9699\n",
      "Epoch 746/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1080 - acc: 0.9696 - val_loss: 0.1615 - val_acc: 0.9704\n",
      "Epoch 747/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1061 - acc: 0.9705 - val_loss: 0.1649 - val_acc: 0.9707\n",
      "Epoch 748/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1066 - acc: 0.9702 - val_loss: 0.1686 - val_acc: 0.9704\n",
      "Epoch 749/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1068 - acc: 0.9698 - val_loss: 0.1717 - val_acc: 0.9699\n",
      "Epoch 750/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1070 - acc: 0.9700 - val_loss: 0.1698 - val_acc: 0.9699\n",
      "Epoch 751/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1076 - acc: 0.9695 - val_loss: 0.1755 - val_acc: 0.9694\n",
      "Epoch 752/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1055 - acc: 0.9701 - val_loss: 0.1644 - val_acc: 0.9712\n",
      "Epoch 753/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1059 - acc: 0.9700 - val_loss: 0.1690 - val_acc: 0.9704\n",
      "Epoch 754/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1070 - acc: 0.9698 - val_loss: 0.1687 - val_acc: 0.9702\n",
      "Epoch 755/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1089 - acc: 0.9698 - val_loss: 0.1631 - val_acc: 0.9704\n",
      "Epoch 756/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1074 - acc: 0.9700 - val_loss: 0.1641 - val_acc: 0.9712\n",
      "Epoch 757/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1072 - acc: 0.9700 - val_loss: 0.1663 - val_acc: 0.9702\n",
      "Epoch 758/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1058 - acc: 0.9701 - val_loss: 0.1706 - val_acc: 0.9702\n",
      "Epoch 759/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1050 - acc: 0.9705 - val_loss: 0.1577 - val_acc: 0.9699\n",
      "Epoch 760/1000\n",
      "16079/16079 [==============================] - 1s 74us/step - loss: 0.1074 - acc: 0.9703 - val_loss: 0.1802 - val_acc: 0.9699\n",
      "Epoch 761/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1105 - acc: 0.9701 - val_loss: 0.1713 - val_acc: 0.9699\n",
      "Epoch 762/1000\n",
      "16079/16079 [==============================] - 1s 86us/step - loss: 0.1060 - acc: 0.9697 - val_loss: 0.1806 - val_acc: 0.9707\n",
      "Epoch 763/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1075 - acc: 0.9701 - val_loss: 0.1670 - val_acc: 0.9704\n",
      "Epoch 764/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1064 - acc: 0.9701 - val_loss: 0.1754 - val_acc: 0.9704\n",
      "Epoch 765/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1075 - acc: 0.9698 - val_loss: 0.1797 - val_acc: 0.9707\n",
      "Epoch 766/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1053 - acc: 0.9703 - val_loss: 0.1692 - val_acc: 0.9707\n",
      "Epoch 767/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1065 - acc: 0.9698 - val_loss: 0.1783 - val_acc: 0.9709\n",
      "Epoch 768/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1097 - acc: 0.9701 - val_loss: 0.1617 - val_acc: 0.9697\n",
      "Epoch 769/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1058 - acc: 0.9706 - val_loss: 0.1795 - val_acc: 0.9697\n",
      "Epoch 770/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1056 - acc: 0.9706 - val_loss: 0.1670 - val_acc: 0.9697\n",
      "Epoch 771/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1064 - acc: 0.9701 - val_loss: 0.1695 - val_acc: 0.9702\n",
      "Epoch 772/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1064 - acc: 0.9697 - val_loss: 0.1668 - val_acc: 0.9697\n",
      "Epoch 773/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1064 - acc: 0.9701 - val_loss: 0.1649 - val_acc: 0.9699\n",
      "Epoch 774/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1068 - acc: 0.9700 - val_loss: 0.1657 - val_acc: 0.9689\n",
      "Epoch 775/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1062 - acc: 0.9702 - val_loss: 0.1666 - val_acc: 0.9699\n",
      "Epoch 776/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1058 - acc: 0.9701 - val_loss: 0.1723 - val_acc: 0.9694\n",
      "Epoch 777/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1086 - acc: 0.9694 - val_loss: 0.1730 - val_acc: 0.9699\n",
      "Epoch 778/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1105 - acc: 0.9697 - val_loss: 0.1623 - val_acc: 0.9699\n",
      "Epoch 779/1000\n",
      "16079/16079 [==============================] - 2s 93us/step - loss: 0.1078 - acc: 0.9697 - val_loss: 0.1692 - val_acc: 0.9692\n",
      "Epoch 780/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1081 - acc: 0.9700 - val_loss: 0.1683 - val_acc: 0.9707\n",
      "Epoch 781/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1052 - acc: 0.9703 - val_loss: 0.1717 - val_acc: 0.9692\n",
      "Epoch 782/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1088 - acc: 0.9696 - val_loss: 0.1686 - val_acc: 0.9694\n",
      "Epoch 783/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1062 - acc: 0.9698 - val_loss: 0.1654 - val_acc: 0.9704\n",
      "Epoch 784/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1067 - acc: 0.9703 - val_loss: 0.1683 - val_acc: 0.9699\n",
      "Epoch 785/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1046 - acc: 0.9704 - val_loss: 0.1659 - val_acc: 0.9702\n",
      "Epoch 786/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1068 - acc: 0.9701 - val_loss: 0.1700 - val_acc: 0.9697\n",
      "Epoch 787/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1055 - acc: 0.9703 - val_loss: 0.1671 - val_acc: 0.9702\n",
      "Epoch 788/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1060 - acc: 0.9697 - val_loss: 0.1831 - val_acc: 0.9694\n",
      "Epoch 789/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1082 - acc: 0.9701 - val_loss: 0.1817 - val_acc: 0.9702\n",
      "Epoch 790/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1068 - acc: 0.9703 - val_loss: 0.1692 - val_acc: 0.9704\n",
      "Epoch 791/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1059 - acc: 0.9706 - val_loss: 0.1759 - val_acc: 0.9709\n",
      "Epoch 792/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1115 - acc: 0.9694 - val_loss: 0.1662 - val_acc: 0.9699\n",
      "Epoch 793/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1055 - acc: 0.9698 - val_loss: 0.1785 - val_acc: 0.9702\n",
      "Epoch 794/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1048 - acc: 0.9701 - val_loss: 0.1802 - val_acc: 0.9694\n",
      "Epoch 795/1000\n",
      "16079/16079 [==============================] - 2s 109us/step - loss: 0.1058 - acc: 0.9705 - val_loss: 0.1713 - val_acc: 0.9697\n",
      "Epoch 796/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1060 - acc: 0.9705 - val_loss: 0.1710 - val_acc: 0.9699\n",
      "Epoch 797/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1052 - acc: 0.9703 - val_loss: 0.1657 - val_acc: 0.9704\n",
      "Epoch 798/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1084 - acc: 0.9701 - val_loss: 0.1622 - val_acc: 0.9702\n",
      "Epoch 799/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1077 - acc: 0.9693 - val_loss: 0.1665 - val_acc: 0.9699\n",
      "Epoch 800/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1071 - acc: 0.9701 - val_loss: 0.1656 - val_acc: 0.9699\n",
      "Epoch 801/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1047 - acc: 0.9698 - val_loss: 0.1736 - val_acc: 0.9704\n",
      "Epoch 802/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1041 - acc: 0.9706 - val_loss: 0.1810 - val_acc: 0.9709\n",
      "Epoch 803/1000\n",
      "16079/16079 [==============================] - 1s 77us/step - loss: 0.1070 - acc: 0.9696 - val_loss: 0.1645 - val_acc: 0.9707\n",
      "Epoch 804/1000\n",
      "16079/16079 [==============================] - 1s 80us/step - loss: 0.1060 - acc: 0.9698 - val_loss: 0.1641 - val_acc: 0.9699\n",
      "Epoch 805/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1069 - acc: 0.9701 - val_loss: 0.1696 - val_acc: 0.9702\n",
      "Epoch 806/1000\n",
      "16079/16079 [==============================] - 1s 82us/step - loss: 0.1070 - acc: 0.9698 - val_loss: 0.1687 - val_acc: 0.9699\n",
      "Epoch 807/1000\n",
      "16079/16079 [==============================] - 1s 76us/step - loss: 0.1082 - acc: 0.9693 - val_loss: 0.1658 - val_acc: 0.9702\n",
      "Epoch 808/1000\n",
      "16079/16079 [==============================] - 1s 79us/step - loss: 0.1079 - acc: 0.9700 - val_loss: 0.1660 - val_acc: 0.9702\n",
      "Epoch 809/1000\n",
      "16079/16079 [==============================] - 1s 72us/step - loss: 0.1068 - acc: 0.9705 - val_loss: 0.1639 - val_acc: 0.9699\n",
      "Epoch 810/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1046 - acc: 0.9704 - val_loss: 0.1701 - val_acc: 0.9707\n",
      "Epoch 811/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1060 - acc: 0.9701 - val_loss: 0.1655 - val_acc: 0.9704\n",
      "Epoch 812/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1051 - acc: 0.9702 - val_loss: 0.1642 - val_acc: 0.9709\n",
      "Epoch 813/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1058 - acc: 0.9698 - val_loss: 0.1681 - val_acc: 0.9707\n",
      "Epoch 814/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1050 - acc: 0.9702 - val_loss: 0.1700 - val_acc: 0.9687\n",
      "Epoch 815/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1057 - acc: 0.9698 - val_loss: 0.1668 - val_acc: 0.9707\n",
      "Epoch 816/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1067 - acc: 0.9698 - val_loss: 0.1690 - val_acc: 0.9697\n",
      "Epoch 817/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1051 - acc: 0.9702 - val_loss: 0.1686 - val_acc: 0.9699\n",
      "Epoch 818/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1064 - acc: 0.9699 - val_loss: 0.1728 - val_acc: 0.9704\n",
      "Epoch 819/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1043 - acc: 0.9710 - val_loss: 0.1708 - val_acc: 0.9689\n",
      "Epoch 820/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1074 - acc: 0.9699 - val_loss: 0.1608 - val_acc: 0.9707\n",
      "Epoch 821/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1050 - acc: 0.9703 - val_loss: 0.1735 - val_acc: 0.9709\n",
      "Epoch 822/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1129 - acc: 0.9696 - val_loss: 0.1730 - val_acc: 0.9702\n",
      "Epoch 823/1000\n",
      "16079/16079 [==============================] - 2s 109us/step - loss: 0.1077 - acc: 0.9695 - val_loss: 0.1694 - val_acc: 0.9692\n",
      "Epoch 824/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1080 - acc: 0.9697 - val_loss: 0.1696 - val_acc: 0.9704\n",
      "Epoch 825/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1059 - acc: 0.9700 - val_loss: 0.1799 - val_acc: 0.9702\n",
      "Epoch 826/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1057 - acc: 0.9700 - val_loss: 0.1710 - val_acc: 0.9702\n",
      "Epoch 827/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1067 - acc: 0.9703 - val_loss: 0.1840 - val_acc: 0.9692\n",
      "Epoch 828/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1070 - acc: 0.9701 - val_loss: 0.1722 - val_acc: 0.9699\n",
      "Epoch 829/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1068 - acc: 0.9701 - val_loss: 0.1715 - val_acc: 0.9697\n",
      "Epoch 830/1000\n",
      "16079/16079 [==============================] - 2s 115us/step - loss: 0.1055 - acc: 0.9701 - val_loss: 0.1695 - val_acc: 0.9699\n",
      "Epoch 831/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1041 - acc: 0.9704 - val_loss: 0.1663 - val_acc: 0.9702\n",
      "Epoch 832/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1062 - acc: 0.9701 - val_loss: 0.1847 - val_acc: 0.9702\n",
      "Epoch 833/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1059 - acc: 0.9698 - val_loss: 0.1688 - val_acc: 0.9689\n",
      "Epoch 834/1000\n",
      "16079/16079 [==============================] - 2s 148us/step - loss: 0.1059 - acc: 0.9700 - val_loss: 0.1695 - val_acc: 0.9687\n",
      "Epoch 835/1000\n",
      "16079/16079 [==============================] - 2s 152us/step - loss: 0.1047 - acc: 0.9707 - val_loss: 0.1670 - val_acc: 0.9697\n",
      "Epoch 836/1000\n",
      "16079/16079 [==============================] - 2s 134us/step - loss: 0.1065 - acc: 0.9702 - val_loss: 0.1725 - val_acc: 0.9692\n",
      "Epoch 837/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1083 - acc: 0.9698 - val_loss: 0.1725 - val_acc: 0.9704\n",
      "Epoch 838/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1080 - acc: 0.9700 - val_loss: 0.1702 - val_acc: 0.9704\n",
      "Epoch 839/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1051 - acc: 0.9706 - val_loss: 0.1718 - val_acc: 0.9707\n",
      "Epoch 840/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1054 - acc: 0.9706 - val_loss: 0.1667 - val_acc: 0.9699\n",
      "Epoch 841/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1039 - acc: 0.9706 - val_loss: 0.1670 - val_acc: 0.9709\n",
      "Epoch 842/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1060 - acc: 0.9698 - val_loss: 0.1693 - val_acc: 0.9702\n",
      "Epoch 843/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1060 - acc: 0.9705 - val_loss: 0.1750 - val_acc: 0.9704\n",
      "Epoch 844/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1051 - acc: 0.9700 - val_loss: 0.1774 - val_acc: 0.9702\n",
      "Epoch 845/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1061 - acc: 0.9701 - val_loss: 0.1700 - val_acc: 0.9707\n",
      "Epoch 846/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1045 - acc: 0.9703 - val_loss: 0.1694 - val_acc: 0.9702\n",
      "Epoch 847/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1057 - acc: 0.9705 - val_loss: 0.1667 - val_acc: 0.9704\n",
      "Epoch 848/1000\n",
      "16079/16079 [==============================] - 2s 111us/step - loss: 0.1076 - acc: 0.9701 - val_loss: 0.1890 - val_acc: 0.9702\n",
      "Epoch 849/1000\n",
      "16079/16079 [==============================] - 2s 128us/step - loss: 0.1064 - acc: 0.9706 - val_loss: 0.1670 - val_acc: 0.9697\n",
      "Epoch 850/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1057 - acc: 0.9705 - val_loss: 0.1714 - val_acc: 0.9709\n",
      "Epoch 851/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1047 - acc: 0.9703 - val_loss: 0.1743 - val_acc: 0.9704\n",
      "Epoch 852/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1053 - acc: 0.9701 - val_loss: 0.1746 - val_acc: 0.9697\n",
      "Epoch 853/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1086 - acc: 0.9696 - val_loss: 0.1819 - val_acc: 0.9699\n",
      "Epoch 854/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1080 - acc: 0.9690 - val_loss: 0.1698 - val_acc: 0.9692\n",
      "Epoch 855/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1048 - acc: 0.9703 - val_loss: 0.1731 - val_acc: 0.9697\n",
      "Epoch 856/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1064 - acc: 0.9704 - val_loss: 0.1659 - val_acc: 0.9704\n",
      "Epoch 857/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1041 - acc: 0.9706 - val_loss: 0.1707 - val_acc: 0.9692\n",
      "Epoch 858/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1047 - acc: 0.9701 - val_loss: 0.1697 - val_acc: 0.9699\n",
      "Epoch 859/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1062 - acc: 0.9705 - val_loss: 0.1698 - val_acc: 0.9707\n",
      "Epoch 860/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1045 - acc: 0.9701 - val_loss: 0.1725 - val_acc: 0.9694\n",
      "Epoch 861/1000\n",
      "16079/16079 [==============================] - 2s 109us/step - loss: 0.1049 - acc: 0.9697 - val_loss: 0.1721 - val_acc: 0.9702\n",
      "Epoch 862/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1043 - acc: 0.9699 - val_loss: 0.1714 - val_acc: 0.9702\n",
      "Epoch 863/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1046 - acc: 0.9708 - val_loss: 0.1648 - val_acc: 0.9707\n",
      "Epoch 864/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1055 - acc: 0.9701 - val_loss: 0.1684 - val_acc: 0.9697\n",
      "Epoch 865/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1046 - acc: 0.9700 - val_loss: 0.1643 - val_acc: 0.9692\n",
      "Epoch 866/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1079 - acc: 0.9699 - val_loss: 0.1651 - val_acc: 0.9704\n",
      "Epoch 867/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1054 - acc: 0.9695 - val_loss: 0.1750 - val_acc: 0.9694\n",
      "Epoch 868/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1045 - acc: 0.9700 - val_loss: 0.1683 - val_acc: 0.9694\n",
      "Epoch 869/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1063 - acc: 0.9698 - val_loss: 0.1812 - val_acc: 0.9694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 870/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1048 - acc: 0.9700 - val_loss: 0.1814 - val_acc: 0.9702\n",
      "Epoch 871/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1047 - acc: 0.9705 - val_loss: 0.1767 - val_acc: 0.9697\n",
      "Epoch 872/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1063 - acc: 0.9696 - val_loss: 0.1740 - val_acc: 0.9689\n",
      "Epoch 873/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1066 - acc: 0.9700 - val_loss: 0.1737 - val_acc: 0.9694\n",
      "Epoch 874/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1040 - acc: 0.9705 - val_loss: 0.1674 - val_acc: 0.9699\n",
      "Epoch 875/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1086 - acc: 0.9701 - val_loss: 0.1666 - val_acc: 0.9704\n",
      "Epoch 876/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1056 - acc: 0.9699 - val_loss: 0.1746 - val_acc: 0.9699\n",
      "Epoch 877/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1091 - acc: 0.9690 - val_loss: 0.1647 - val_acc: 0.9699\n",
      "Epoch 878/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1053 - acc: 0.9700 - val_loss: 0.1758 - val_acc: 0.9702\n",
      "Epoch 879/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1034 - acc: 0.9705 - val_loss: 0.1718 - val_acc: 0.9694\n",
      "Epoch 880/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1118 - acc: 0.9693 - val_loss: 0.1617 - val_acc: 0.9702\n",
      "Epoch 881/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1080 - acc: 0.9690 - val_loss: 0.1754 - val_acc: 0.9694\n",
      "Epoch 882/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1058 - acc: 0.9700 - val_loss: 0.1588 - val_acc: 0.9707\n",
      "Epoch 883/1000\n",
      "16079/16079 [==============================] - 1s 87us/step - loss: 0.1073 - acc: 0.9699 - val_loss: 0.1659 - val_acc: 0.9707\n",
      "Epoch 884/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1036 - acc: 0.9703 - val_loss: 0.1710 - val_acc: 0.9694\n",
      "Epoch 885/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1041 - acc: 0.9699 - val_loss: 0.1670 - val_acc: 0.9694\n",
      "Epoch 886/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1039 - acc: 0.9708 - val_loss: 0.1689 - val_acc: 0.9697\n",
      "Epoch 887/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1028 - acc: 0.9703 - val_loss: 0.1757 - val_acc: 0.9692\n",
      "Epoch 888/1000\n",
      "16079/16079 [==============================] - 2s 128us/step - loss: 0.1032 - acc: 0.9705 - val_loss: 0.1651 - val_acc: 0.9697\n",
      "Epoch 889/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1042 - acc: 0.9707 - val_loss: 0.1768 - val_acc: 0.9699\n",
      "Epoch 890/1000\n",
      "16079/16079 [==============================] - 2s 111us/step - loss: 0.1039 - acc: 0.9703 - val_loss: 0.1737 - val_acc: 0.9692\n",
      "Epoch 891/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1039 - acc: 0.9705 - val_loss: 0.1660 - val_acc: 0.9694\n",
      "Epoch 892/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1048 - acc: 0.9710 - val_loss: 0.1696 - val_acc: 0.9704\n",
      "Epoch 893/1000\n",
      "16079/16079 [==============================] - 2s 111us/step - loss: 0.1041 - acc: 0.9701 - val_loss: 0.1687 - val_acc: 0.9699\n",
      "Epoch 894/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1060 - acc: 0.9698 - val_loss: 0.1684 - val_acc: 0.9702\n",
      "Epoch 895/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1054 - acc: 0.9705 - val_loss: 0.1689 - val_acc: 0.9694\n",
      "Epoch 896/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1035 - acc: 0.9703 - val_loss: 0.1764 - val_acc: 0.9694\n",
      "Epoch 897/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1043 - acc: 0.9705 - val_loss: 0.1653 - val_acc: 0.9707\n",
      "Epoch 898/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1054 - acc: 0.9703 - val_loss: 0.1691 - val_acc: 0.9709\n",
      "Epoch 899/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1031 - acc: 0.9702 - val_loss: 0.1717 - val_acc: 0.9697\n",
      "Epoch 900/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1036 - acc: 0.9703 - val_loss: 0.1682 - val_acc: 0.9697\n",
      "Epoch 901/1000\n",
      "16079/16079 [==============================] - 2s 111us/step - loss: 0.1056 - acc: 0.9701 - val_loss: 0.1810 - val_acc: 0.9689\n",
      "Epoch 902/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1065 - acc: 0.9698 - val_loss: 0.1721 - val_acc: 0.9704\n",
      "Epoch 903/1000\n",
      "16079/16079 [==============================] - 2s 129us/step - loss: 0.1044 - acc: 0.9706 - val_loss: 0.1703 - val_acc: 0.9707\n",
      "Epoch 904/1000\n",
      "16079/16079 [==============================] - 2s 121us/step - loss: 0.1050 - acc: 0.9699 - val_loss: 0.1817 - val_acc: 0.9697\n",
      "Epoch 905/1000\n",
      "16079/16079 [==============================] - 2s 128us/step - loss: 0.1044 - acc: 0.9703 - val_loss: 0.1707 - val_acc: 0.9702\n",
      "Epoch 906/1000\n",
      "16079/16079 [==============================] - 2s 124us/step - loss: 0.1044 - acc: 0.9709 - val_loss: 0.1724 - val_acc: 0.9697\n",
      "Epoch 907/1000\n",
      "16079/16079 [==============================] - 2s 127us/step - loss: 0.1030 - acc: 0.9706 - val_loss: 0.1749 - val_acc: 0.9694\n",
      "Epoch 908/1000\n",
      "16079/16079 [==============================] - 2s 115us/step - loss: 0.1040 - acc: 0.9707 - val_loss: 0.1771 - val_acc: 0.9694\n",
      "Epoch 909/1000\n",
      "16079/16079 [==============================] - 2s 124us/step - loss: 0.1058 - acc: 0.9701 - val_loss: 0.1685 - val_acc: 0.9697\n",
      "Epoch 910/1000\n",
      "16079/16079 [==============================] - 2s 124us/step - loss: 0.1051 - acc: 0.9700 - val_loss: 0.1724 - val_acc: 0.9689\n",
      "Epoch 911/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1043 - acc: 0.9703 - val_loss: 0.1733 - val_acc: 0.9699\n",
      "Epoch 912/1000\n",
      "16079/16079 [==============================] - 2s 125us/step - loss: 0.1058 - acc: 0.9698 - val_loss: 0.1752 - val_acc: 0.9694\n",
      "Epoch 913/1000\n",
      "16079/16079 [==============================] - 2s 130us/step - loss: 0.1037 - acc: 0.9706 - val_loss: 0.1760 - val_acc: 0.9699\n",
      "Epoch 914/1000\n",
      "16079/16079 [==============================] - 2s 121us/step - loss: 0.1033 - acc: 0.9703 - val_loss: 0.1741 - val_acc: 0.9702\n",
      "Epoch 915/1000\n",
      "16079/16079 [==============================] - 2s 118us/step - loss: 0.1045 - acc: 0.9706 - val_loss: 0.1807 - val_acc: 0.9697\n",
      "Epoch 916/1000\n",
      "16079/16079 [==============================] - 2s 131us/step - loss: 0.1038 - acc: 0.9703 - val_loss: 0.1783 - val_acc: 0.9697\n",
      "Epoch 917/1000\n",
      "16079/16079 [==============================] - 2s 109us/step - loss: 0.1052 - acc: 0.9707 - val_loss: 0.1660 - val_acc: 0.9697\n",
      "Epoch 918/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1067 - acc: 0.9692 - val_loss: 0.1729 - val_acc: 0.9687\n",
      "Epoch 919/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1069 - acc: 0.9699 - val_loss: 0.1813 - val_acc: 0.9694\n",
      "Epoch 920/1000\n",
      "16079/16079 [==============================] - 2s 108us/step - loss: 0.1047 - acc: 0.9699 - val_loss: 0.1739 - val_acc: 0.9699\n",
      "Epoch 921/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1057 - acc: 0.9696 - val_loss: 0.1711 - val_acc: 0.9697\n",
      "Epoch 922/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1077 - acc: 0.9697 - val_loss: 0.1649 - val_acc: 0.9704\n",
      "Epoch 923/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1046 - acc: 0.9702 - val_loss: 0.1697 - val_acc: 0.9697\n",
      "Epoch 924/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1044 - acc: 0.9706 - val_loss: 0.1715 - val_acc: 0.9689\n",
      "Epoch 925/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1057 - acc: 0.9706 - val_loss: 0.1714 - val_acc: 0.9704\n",
      "Epoch 926/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1046 - acc: 0.9701 - val_loss: 0.1666 - val_acc: 0.9694\n",
      "Epoch 927/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1037 - acc: 0.9706 - val_loss: 0.1709 - val_acc: 0.9702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 928/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1032 - acc: 0.9707 - val_loss: 0.1725 - val_acc: 0.9689\n",
      "Epoch 929/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1046 - acc: 0.9704 - val_loss: 0.1755 - val_acc: 0.9694\n",
      "Epoch 930/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1028 - acc: 0.9702 - val_loss: 0.1712 - val_acc: 0.9694\n",
      "Epoch 931/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1061 - acc: 0.9703 - val_loss: 0.1772 - val_acc: 0.9689\n",
      "Epoch 932/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1048 - acc: 0.9695 - val_loss: 0.1643 - val_acc: 0.9699\n",
      "Epoch 933/1000\n",
      "16079/16079 [==============================] - 1s 90us/step - loss: 0.1032 - acc: 0.9703 - val_loss: 0.1725 - val_acc: 0.9704\n",
      "Epoch 934/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1047 - acc: 0.9705 - val_loss: 0.1744 - val_acc: 0.9704\n",
      "Epoch 935/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1049 - acc: 0.9701 - val_loss: 0.1681 - val_acc: 0.9694\n",
      "Epoch 936/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1028 - acc: 0.9706 - val_loss: 0.1708 - val_acc: 0.9692\n",
      "Epoch 937/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1064 - acc: 0.9701 - val_loss: 0.1666 - val_acc: 0.9697\n",
      "Epoch 938/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1048 - acc: 0.9705 - val_loss: 0.1792 - val_acc: 0.9704\n",
      "Epoch 939/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1035 - acc: 0.9705 - val_loss: 0.1684 - val_acc: 0.9699\n",
      "Epoch 940/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1028 - acc: 0.9706 - val_loss: 0.1760 - val_acc: 0.9699\n",
      "Epoch 941/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1054 - acc: 0.9701 - val_loss: 0.1767 - val_acc: 0.9699\n",
      "Epoch 942/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1059 - acc: 0.9696 - val_loss: 0.1741 - val_acc: 0.9704\n",
      "Epoch 943/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1064 - acc: 0.9707 - val_loss: 0.1688 - val_acc: 0.9699\n",
      "Epoch 944/1000\n",
      "16079/16079 [==============================] - 2s 117us/step - loss: 0.1048 - acc: 0.9706 - val_loss: 0.1749 - val_acc: 0.9702\n",
      "Epoch 945/1000\n",
      "16079/16079 [==============================] - 1s 91us/step - loss: 0.1053 - acc: 0.9698 - val_loss: 0.1757 - val_acc: 0.9694\n",
      "Epoch 946/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1042 - acc: 0.9702 - val_loss: 0.1769 - val_acc: 0.9702\n",
      "Epoch 947/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1075 - acc: 0.9706 - val_loss: 0.1691 - val_acc: 0.9694\n",
      "Epoch 948/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1104 - acc: 0.9695 - val_loss: 0.1774 - val_acc: 0.9684\n",
      "Epoch 949/1000\n",
      "16079/16079 [==============================] - 2s 119us/step - loss: 0.1063 - acc: 0.9694 - val_loss: 0.1780 - val_acc: 0.9702\n",
      "Epoch 950/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1047 - acc: 0.9701 - val_loss: 0.1831 - val_acc: 0.9699\n",
      "Epoch 951/1000\n",
      "16079/16079 [==============================] - 2s 115us/step - loss: 0.1045 - acc: 0.9697 - val_loss: 0.1736 - val_acc: 0.9697\n",
      "Epoch 952/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1033 - acc: 0.9705 - val_loss: 0.1702 - val_acc: 0.9707\n",
      "Epoch 953/1000\n",
      "16079/16079 [==============================] - 2s 100us/step - loss: 0.1041 - acc: 0.9707 - val_loss: 0.1661 - val_acc: 0.9702\n",
      "Epoch 954/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1024 - acc: 0.9706 - val_loss: 0.1791 - val_acc: 0.9699\n",
      "Epoch 955/1000\n",
      "16079/16079 [==============================] - 2s 105us/step - loss: 0.1042 - acc: 0.9704 - val_loss: 0.1660 - val_acc: 0.9697\n",
      "Epoch 956/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1030 - acc: 0.9703 - val_loss: 0.1698 - val_acc: 0.9699\n",
      "Epoch 957/1000\n",
      "16079/16079 [==============================] - 2s 96us/step - loss: 0.1062 - acc: 0.9695 - val_loss: 0.1781 - val_acc: 0.9704\n",
      "Epoch 958/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1037 - acc: 0.9708 - val_loss: 0.1694 - val_acc: 0.9684\n",
      "Epoch 959/1000\n",
      "16079/16079 [==============================] - 1s 89us/step - loss: 0.1041 - acc: 0.9701 - val_loss: 0.1677 - val_acc: 0.9694\n",
      "Epoch 960/1000\n",
      "16079/16079 [==============================] - 2s 111us/step - loss: 0.1040 - acc: 0.9703 - val_loss: 0.1699 - val_acc: 0.9702\n",
      "Epoch 961/1000\n",
      "16079/16079 [==============================] - 1s 93us/step - loss: 0.1031 - acc: 0.9704 - val_loss: 0.1689 - val_acc: 0.9702\n",
      "Epoch 962/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1048 - acc: 0.9700 - val_loss: 0.1772 - val_acc: 0.9699\n",
      "Epoch 963/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1034 - acc: 0.9702 - val_loss: 0.1754 - val_acc: 0.9704\n",
      "Epoch 964/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1035 - acc: 0.9710 - val_loss: 0.1706 - val_acc: 0.9702\n",
      "Epoch 965/1000\n",
      "16079/16079 [==============================] - 1s 78us/step - loss: 0.1025 - acc: 0.9701 - val_loss: 0.1827 - val_acc: 0.9704\n",
      "Epoch 966/1000\n",
      "16079/16079 [==============================] - 1s 83us/step - loss: 0.1088 - acc: 0.9703 - val_loss: 0.1752 - val_acc: 0.9694\n",
      "Epoch 967/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1036 - acc: 0.9702 - val_loss: 0.1751 - val_acc: 0.9697\n",
      "Epoch 968/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1016 - acc: 0.9711 - val_loss: 0.1747 - val_acc: 0.9694\n",
      "Epoch 969/1000\n",
      "16079/16079 [==============================] - 2s 97us/step - loss: 0.1049 - acc: 0.9703 - val_loss: 0.1758 - val_acc: 0.9694\n",
      "Epoch 970/1000\n",
      "16079/16079 [==============================] - 2s 107us/step - loss: 0.1029 - acc: 0.9702 - val_loss: 0.1752 - val_acc: 0.9697\n",
      "Epoch 971/1000\n",
      "16079/16079 [==============================] - 2s 104us/step - loss: 0.1036 - acc: 0.9700 - val_loss: 0.1680 - val_acc: 0.9702\n",
      "Epoch 972/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1037 - acc: 0.9705 - val_loss: 0.1737 - val_acc: 0.9702\n",
      "Epoch 973/1000\n",
      "16079/16079 [==============================] - 2s 114us/step - loss: 0.1059 - acc: 0.9697 - val_loss: 0.1725 - val_acc: 0.9699\n",
      "Epoch 974/1000\n",
      "16079/16079 [==============================] - 1s 88us/step - loss: 0.1045 - acc: 0.9705 - val_loss: 0.1729 - val_acc: 0.9694\n",
      "Epoch 975/1000\n",
      "16079/16079 [==============================] - 2s 106us/step - loss: 0.1026 - acc: 0.9703 - val_loss: 0.1846 - val_acc: 0.9697\n",
      "Epoch 976/1000\n",
      "16079/16079 [==============================] - 1s 92us/step - loss: 0.1043 - acc: 0.9703 - val_loss: 0.1851 - val_acc: 0.9697\n",
      "Epoch 977/1000\n",
      "16079/16079 [==============================] - 2s 101us/step - loss: 0.1054 - acc: 0.9703 - val_loss: 0.1697 - val_acc: 0.9694\n",
      "Epoch 978/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1074 - acc: 0.9700 - val_loss: 0.1692 - val_acc: 0.9697\n",
      "Epoch 979/1000\n",
      "16079/16079 [==============================] - 2s 110us/step - loss: 0.1046 - acc: 0.9705 - val_loss: 0.1761 - val_acc: 0.9687\n",
      "Epoch 980/1000\n",
      "16079/16079 [==============================] - 2s 152us/step - loss: 0.1040 - acc: 0.9701 - val_loss: 0.1757 - val_acc: 0.9694\n",
      "Epoch 981/1000\n",
      "16079/16079 [==============================] - 2s 141us/step - loss: 0.1026 - acc: 0.9697 - val_loss: 0.1829 - val_acc: 0.9694\n",
      "Epoch 982/1000\n",
      "16079/16079 [==============================] - 2s 146us/step - loss: 0.1032 - acc: 0.9701 - val_loss: 0.1808 - val_acc: 0.9687\n",
      "Epoch 983/1000\n",
      "16079/16079 [==============================] - 2s 115us/step - loss: 0.1033 - acc: 0.9703 - val_loss: 0.1685 - val_acc: 0.9694\n",
      "Epoch 984/1000\n",
      "16079/16079 [==============================] - 2s 122us/step - loss: 0.1035 - acc: 0.9700 - val_loss: 0.1811 - val_acc: 0.9699\n",
      "Epoch 985/1000\n",
      "16079/16079 [==============================] - 2s 118us/step - loss: 0.1036 - acc: 0.9703 - val_loss: 0.1838 - val_acc: 0.9697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 986/1000\n",
      "16079/16079 [==============================] - 2s 99us/step - loss: 0.1052 - acc: 0.9704 - val_loss: 0.1754 - val_acc: 0.9699\n",
      "Epoch 987/1000\n",
      "16079/16079 [==============================] - 2s 98us/step - loss: 0.1025 - acc: 0.9708 - val_loss: 0.1781 - val_acc: 0.9697\n",
      "Epoch 988/1000\n",
      "16079/16079 [==============================] - 2s 103us/step - loss: 0.1019 - acc: 0.9704 - val_loss: 0.1788 - val_acc: 0.9697\n",
      "Epoch 989/1000\n",
      "16079/16079 [==============================] - 2s 94us/step - loss: 0.1035 - acc: 0.9702 - val_loss: 0.1747 - val_acc: 0.9697\n",
      "Epoch 990/1000\n",
      "16079/16079 [==============================] - 2s 129us/step - loss: 0.1031 - acc: 0.9704 - val_loss: 0.1728 - val_acc: 0.9697\n",
      "Epoch 991/1000\n",
      "16079/16079 [==============================] - 2s 111us/step - loss: 0.1023 - acc: 0.9705 - val_loss: 0.1767 - val_acc: 0.9689\n",
      "Epoch 992/1000\n",
      "16079/16079 [==============================] - 2s 95us/step - loss: 0.1010 - acc: 0.9706 - val_loss: 0.1810 - val_acc: 0.9699\n",
      "Epoch 993/1000\n",
      "16079/16079 [==============================] - 2s 102us/step - loss: 0.1015 - acc: 0.9709 - val_loss: 0.1827 - val_acc: 0.9699\n",
      "Epoch 994/1000\n",
      "16079/16079 [==============================] - 2s 134us/step - loss: 0.1044 - acc: 0.9704 - val_loss: 0.1696 - val_acc: 0.9689\n",
      "Epoch 995/1000\n",
      "16079/16079 [==============================] - 2s 116us/step - loss: 0.1065 - acc: 0.9703 - val_loss: 0.1687 - val_acc: 0.9697\n",
      "Epoch 996/1000\n",
      "16079/16079 [==============================] - 2s 123us/step - loss: 0.1039 - acc: 0.9704 - val_loss: 0.1767 - val_acc: 0.9697\n",
      "Epoch 997/1000\n",
      "16079/16079 [==============================] - 2s 136us/step - loss: 0.1026 - acc: 0.9708 - val_loss: 0.1763 - val_acc: 0.9697\n",
      "Epoch 998/1000\n",
      "16079/16079 [==============================] - 2s 126us/step - loss: 0.1030 - acc: 0.9707 - val_loss: 0.1768 - val_acc: 0.9699\n",
      "Epoch 999/1000\n",
      "16079/16079 [==============================] - 2s 122us/step - loss: 0.1022 - acc: 0.9708 - val_loss: 0.1723 - val_acc: 0.9697\n",
      "Epoch 1000/1000\n",
      "16079/16079 [==============================] - 2s 134us/step - loss: 0.1043 - acc: 0.9701 - val_loss: 0.1715 - val_acc: 0.9697\n",
      "1682.35426735878  seconds\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "# Takes about 5230.725239038467  seconds About 1 and 1/2 hours for 1000 epochs\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "    train_X,\n",
    "    # train_X_no_vectors, \n",
    "    train_y, \n",
    "    epochs=1000,\n",
    "    # validation_split=0.4\n",
    "    validation_data=(validation_X, validation_y)\n",
    "    \n",
    ")\n",
    "end = time.time()\n",
    "print(end-start, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd8VFXawPHfkw4kEEpAIECoSkcMIFLEhiCWXcWCiv3Furq66KJrWVlXed1ddX1FBQt2XeysIiCIFQUC0kKICTFCqKGmQMok5/3j3pnMJDOZIcykPt/PJ5/MvffcO+dm4D5zuhhjUEoppaoTVtcZUEopVf9psFBKKeWXBgullFJ+abBQSinllwYLpZRSfmmwUEop5ZcGC6WUUn5psFBKKeWXBgullFJ+RdR1BoKlXbt2Jikpqa6zoZRSDcqaNWv2GWMS/KVrNMEiKSmJlJSUus6GUko1KCLyWyDptBpKKaWUXxoslFJK+aXBQimllF+Nps1CKVV/lJaWkpOTQ1FRUV1nRdliYmJITEwkMjKyRudrsFBKBV1OTg5xcXEkJSUhInWdnSbPGMP+/fvJycmhe/fuNbqGVkMppYKuqKiItm3baqCoJ0SEtm3bHldJT4OFUiokNFDUL8f7eTT5YFFY7OCpJems236orrOilFL1VpMPFkdLy3j2q0w25GiwUKqx2L9/P0OGDGHIkCGccMIJdO7c2bVdUlIS0DWuv/560tPTq00ze/Zs3n777WBkmdGjR7Nu3bqgXCsUmnwDt7NgZkydZkMpFURt27Z1PXj/+te/Ehsby/Tp0z3SGGMwxhAW5v0787x58/y+z+233378mW0gmnzJwlmPZzRaKNXoZWZmMmDAAG655RaGDh3Krl27mDZtGsnJyfTv35+ZM2e60jq/6TscDuLj45kxYwaDBw9m5MiR7N27F4AHH3yQZ555xpV+xowZDB8+nBNPPJEVK1YAUFhYyCWXXMLgwYOZMmUKycnJAZcgjh49yrXXXsvAgQMZOnQo3377LQAbN25k2LBhDBkyhEGDBpGVlUV+fj4TJ05k8ODBDBgwgA8++CCYfzotWWgTnFKh9eh/U9m8My+o1+zXqSWPXNC/Rudu3ryZefPm8eKLLwIwa9Ys2rRpg8Ph4IwzzmDy5Mn069fP45zDhw9z+umnM2vWLO655x5effVVZsyYUeXaxhhWrVrFggULmDlzJosWLeL//u//OOGEE/jwww9Zv349Q4cODTivzz77LFFRUWzcuJHU1FTOO+88MjIyeP7555k+fTqXX345xcXFGGP49NNPSUpK4osvvnDlOZiafMnCScsVSjUNPXv2ZNiwYa7td999l6FDhzJ06FDS0tLYvHlzlXOaNWvGxIkTATjllFPIzs72eu2LL764Sprvv/+eK664AoDBgwfTv3/gQe77779n6tSpAPTv359OnTqRmZnJaaedxmOPPcaTTz7J9u3biYmJYdCgQSxatIgZM2bwww8/0KpVq4DfJxBasrCLFloLpVRo1LQEECotWrRwvc7IyODf//43q1atIj4+nquvvtrrWISoqCjX6/DwcBwOh9drR0dHV0lzPFXcvs6dOnUqI0eO5PPPP+ecc87h9ddfZ+zYsaSkpLBw4ULuvfdezj//fB544IEav3dlTb5kIXZFlMYKpZqevLw84uLiaNmyJbt27WLx4sVBf4/Ro0czf/58wGpr8FZy8WXs2LGu3lZpaWns2rWLXr16kZWVRa9evbjrrruYNGkSGzZsYMeOHcTGxjJ16lTuuece1q5dG9T7aPIlC1wlCw0XSjU1Q4cOpV+/fgwYMIAePXowatSooL/HH/7wB6655hoGDRrE0KFDGTBggM8qonPPPdc1d9OYMWN49dVXufnmmxk4cCCRkZG88cYbREVF8c477/Duu+8SGRlJp06deOyxx1ixYgUzZswgLCyMqKgoV5tMsEhjeUgmJyebmix+lFdUyqC/LuHBSX25aUyPEORMqaYnLS2Nvn371nU26gWHw4HD4SAmJoaMjAzGjx9PRkYGERG1/13d2+ciImuMMcn+zm3yJQsdZ6GUCqWCggLOOussHA4HxhjmzJlTJ4HieDW8HAeZzl+jlAql+Ph41qxZU9fZOG4hbeAWkQkiki4imSJSpVOyiIwVkbUi4hCRyZWOdRWRJSKSJiKbRSQplHk12sStVFA1liruxuJ4P4+QBQsRCQdmAxOBfsAUEelXKdk24DrgHS+XeAP4hzGmLzAc2BuSfNq/9d+1UsETExPD/v37NWDUE871LGJiYmp8jVBWQw0HMo0xWQAi8h5wEeDqN2aMybaPlbufaAeVCGPMl3a6glBl0jXOIlRvoFQTlJiYSE5ODrm5uXWdFWVzrpRXU6EMFp2B7W7bOcCIAM/tAxwSkY+A7sBSYIYxpsw9kYhMA6YBdO3atUaZdI2z0GihVNBERkbWeEU2VT+Fss3CW8txoI/kCGAMMB0YBvTAqq7yvJgxc40xycaY5ISEhJpl0lWy0GihlFK+hDJY5ABd3LYTgZ3HcO7PxpgsY4wD+AQIfPatGtCShVJK+RbKYLEa6C0i3UUkCrgCWHAM57YWEWdx4Uzc2jqCSXvOKqWUfyELFnaJ4A5gMZAGzDfGpIrITBG5EEBEholIDnApMEdEUu1zy7CqoJaJyEasKq2XQpFP0UnKlVLKr5AOyjPGLAQWVtr3sNvr1VjVU97O/RIYFMr8VXq/2norpZRqcHTWWZ2iXCml/NJgYf/WWKGUUr5psBAdZ6GUUv5osLB/6zgLpZTyTYOFtlkopZRfGixEl1VVSil/mnywUEop5Z8GCyeth1JKKZ80WGC1W2ioUEop3zRYYPWI0oKFUkr5psECq5Fbu84qpZRvGizQkoVSSvmjwQJts1BKKX80WKDTlCullD8aLGxaDaWUUr5psAAQnRtKKaWqo8ECezJBjRVKKeWTBgu0gVsppfzRYIHVwK3LqiqllG8aLLBLFhorlFLKJw0W2IPy6joTSilVj2mwoGJNC6WUUt6FNFiIyAQRSReRTBGZ4eX4WBFZKyIOEZlc6ViZiKyzfxaEMp+g1VBKKVWdiFBdWETCgdnAOUAOsFpEFhhjNrsl2wZcB0z3comjxpghocqfO6saSqOFUkr5ErJgAQwHMo0xWQAi8h5wEeAKFsaYbPtYeQjz4Z82cCulVLVCWQ3VGdjutp1j7wtUjIikiMhPIvI7bwlEZJqdJiU3N7fGGdUWC6WUql4og4W3Z/CxfH/vaoxJBq4EnhGRnlUuZsxcY0yyMSY5ISGhpvm01rPQooVSSvkUymCRA3Rx204EdgZ6sjFmp/07C/gaODmYmXOnI7iVUqp6oQwWq4HeItJdRKKAK4CAejWJSGsRibZftwNG4dbWEWxaDaWUUtULWbAwxjiAO4DFQBow3xiTKiIzReRCABEZJiI5wKXAHBFJtU/vC6SIyHpgOTCrUi+qEOQ3lFdXSqmGLZS9oTDGLAQWVtr3sNvr1VjVU5XPWwEMDGXe3Oka3EopVT0dwY2uwa2UUv5osEAbuJVSyh8NFgCIliyUUqoaGiywShZatlBKKd80WKBtFkop5Y8GC5wlC6WUUr5osLBpyUIppXzTYIG9Bre2WSillE8aLNA1uJVSyh8NFuga3Eop5Y8GC5xTlNd1LpRSqv7SYGHTNgullPJNgwV211mNFUop5ZMGC6WUUn5psEAnElRKKX80WGCPs9AWbqWU8kmDBVqyUEopfzRYoBMJKqWUPxoscC6rqpRSyhcNFjhLFhoulFLKFw0WYEULpZRSPmmwsGm5QimlfAtpsBCRCSKSLiKZIjLDy/GxIrJWRBwiMtnL8ZYiskNEngtpPkGjhVJKVSNkwUJEwoHZwESgHzBFRPpVSrYNuA54x8dl/gZ8E6o8OlkN3BotlFLKl1CWLIYDmcaYLGNMCfAecJF7AmNMtjFmA1Be+WQROQXoACwJYR6t90K7ziqlVHVCGSw6A9vdtnPsfX6JSBjwL+DeEOTLy/tpsFBKqeqEMlh462MU6CP5NmChMWZ7dYlEZJqIpIhISm5u7jFn0HUdXVZVKaWqFRHCa+cAXdy2E4GdAZ47EhgjIrcBsUCUiBQYYzwayY0xc4G5AMnJyTV+2mvJQimlqhfKYLEa6C0i3YEdwBXAlYGcaIy5yvlaRK4DkisHCqWUUrUnZNVQxhgHcAewGEgD5htjUkVkpohcCCAiw0QkB7gUmCMiqaHKj9/81tUbK6VUAxDKkgXGmIXAwkr7HnZ7vRqreqq6a7wGvBaC7LnoGtxKKVU9HcGNsyVeo4VSSvmiwQJt4FZKKX80WKCLHymllD8aLNBlVZVSyh8NFlglC6WUUr5psLBpuUIppXzTYIFOJKiUUv5osADQNbiVUqpaAQULEekpItH263EicqeIxIc2a7UnXKC8XMOFUkr5EmjJ4kOgTER6Aa8A3fG9YFGD0zwqgiMljrrOhlJK1VuBBotye66n3wPPGGPuBjqGLlu1q0V0OIXFZXWdDaWUqrcCDRalIjIFuBb4zN4XGZos1b4WUREUFGvJQimlfAk0WFyPtcbE340xv9rTjr8VumzVrhbRERRqNZRSSvkU0KyzxpjNwJ0AItIaiDPGzAplxmpTi+gICrVkoZRSPgXaG+prEWkpIm2A9cA8EXkqtFmrPS2bRVBaZrSRWymlfAi0GqqVMSYPuBiYZ4w5BTg7dNmqXT3atQBg697COs6JUkrVT4EGiwgR6QhcRkUDd6PRp0McADe9sbqOc6KUUvVToMFiJtbyqFuNMatFpAeQEbps1a5uba2SxZ68Ykoc5XWcG6WUqn8CChbGmPeNMYOMMbfa21nGmEtCm7XaEx4mxMVYbf07Dx2t49wopVT9E2gDd6KIfCwie0Vkj4h8KCLVrp3d0Lx14wgAVmzdX8c5UUqp+ifQaqh5wAKgE9AZ+K+9r9EYlNgKgAc+3si0N1J0riillHITaLBIMMbMM8Y47J/XgIQQ5qvWiQjXjOwGwJLNe3hmWQaXvLCCj9bm1HHOlFKq7gUaLPaJyNUiEm7/XA34ra8RkQkiki4imSIyw8vxsSKyVkQcIjLZbX83EVkjIutEJFVEbgn8lmpu5kUDeOcmqzrq2WUZrPntIPfMX8+cb7bqsqtKqSYt0GBxA1a32d3ALmAy1hQgPolIODAbmAj0A6aISL9KybYB11F1BttdwGnGmCHACGCGiHQKMK/H5bRe7XjrxhF0jm/m2vfEF1vYm19cG2+vlFL1UqC9obYZYy40xiQYY9obY36HNUCvOsOBTLvnVAnwHnBRpetmG2M2AOWV9pcYY5xP5+hA8xkso3u34/s/n0H6YxNc+254bTVzvtnKyixtAFdKNT3H8xC+x8/xzsB2t+0ce19ARKSLiGywr/G/xpidx57FmhMRoiPC+fDW0wBI3ZnHE19s4fK5PzF/9XZSsg/UZnaUUqpOHU+wkBocD7ji3xiz3RgzCOgFXCsiHaq8gcg0EUkRkZTc3NxAL31MTunWmi/vHktEWMXt3PfhBia/+GNI3k8ppeqj4wkW/h78OUAXt+1E4JhLB3aJIhUY4+XYXGNMsjEmOSEhdJ2zeneII/Px83j68sEe+5NmfM5L32ax/cARbQBXSjVq1QYLEckXkTwvP/lYYy6qsxroLSLdRSQKuAJrrIZf9iDAZvbr1sAoID2Qc0PpwsGdOemEOI99f1+Yxpgnl7N228E6ypVSSoVetcHCGBNnjGnp5SfOGFPtWhj2Mqx3YM0plQbMN8akishMEbkQQESGiUgOcCkwR0RS7dP7AitFZD3wDfBPY8zG47vV4xceJiy8cwwvXZPM19PHeRy7+c01pGQfYGPO4brJnFJKhZA0luqT5ORkk5KSUqvvuea3g1zywooq+1+6JhljDMO7tyG+eVSt5kkppY6FiKwxxiT7TafB4vjMT9kOxmr09ubr6eNIstfLUEqp+ibQYFGr4xcao8uSu3DZsC6kPnou828eSb+OLT2OP7c8k/XbD9VR7pRSKji0ZBECmXsL+GzDTp5ZWrHkx88PnUPrFlolpZSqX7RkUYd6tY/lj2f34ZELKmY3OflvX/LcVxlk7s3Xtb6VUg2OBosQunZkEveee6Jr+59LfuHsp77l4udXsHlnHq98/2sd5k4ppQJXbfdXdXzCwoSbx/ZgxdZ9tGkRzX/XW2MSt+zO57xnvwPg3P4dKC0zRIYLia2b12V2lVLKJ22zqEXrtx/iotk/+DyePWtSLeZGKaW0zaJeGtwlnuxZk5h/80ivx/fmF1HiKOeRTzexQ9cCV0rVI1oNVQeGd29D1uPn8fqP2Tz6382u/VPm/kTPhFiWbN7DvsISZl08kOZREYSH+ZuzUSmlQkuroeqBa15dxbe/eJ81966zenP3OX1qOUdKqaZCq6EakHnXDWPK8C5ej/17WQYljnKvx5RSqrZoyaIeKSh2kF9UysgnvqpybFSvtpw3sCOXJ3chIjyMtdsOUljsYEzv0E3NrpRq/HRuqAasul5TzSLDef6qoVz/2mpAe1AppY5PoMFCG7jrocFd4nn/lpEUFDtYmXWAF7/ZSlxMBPlFDo6WlrkCBUBufjGHj5ay7UAhbVtEM7hLfB3mXCnVWGnJogEwxiAi/HVBKq+tyK427aRBHXloUj9at4gkOiK8djKolGqwtIG7ERGxus5efWo3eiRUP9355xt2ceoTyzjxwUUUO8pqI3tKqSZAg0UD0qt9LAvuGM0lQxNZMeNMnrpscLXpT3xwEV+n762yv6y8cZQmlVK1R4NFAxMbHcG/LhtMp/hmXDw0ka+nj2Nkj7YATB/fh56VSh7XzVvN4tTdAPzh3Z956JNN9HxgITe9nkJeUWmt518p1TBpm0UjcLSkjC/T9nD+wI7sLyxh087DPL88k9XZB/2eu+bBsxERwsOEqPAwoiLCdMS4Uk2I9oZqQppFhXPh4E4AJMRFc8aJ7SlxlLM6e43fc095bCkAURFhlDjKuSw5kScnV1+9pZRqejRYNFKn90ng4pM7c8/4PqzOPsDd/1lfbXrnKPH5KTm0aRFN1zbNGdO7HQlx0cREaq8qpZo6rYZqIrL3FbJi634mDexIbkERa7cd4r4PNvg97/xBHXnuyqG1kEOlVF2oF11nRWSCiKSLSKaIzPByfKyIrBURh4hMdts/RER+FJFUEdkgIpeHMp9NQVK7Flw5oiutmkfSq30clyV7n4uqss827KKxfKFQStVcyEoWIhIO/AKcA+QAq4EpxpjNbmmSgJbAdGCBMeYDe38fwBhjMkSkE7AG6GuMOeTr/bRkcey25hawN6+YX/cVsjh1N5HhYSxN2+Mz/Y/3n0mbFlFER4Rz6EgJX2zazRXDurjGgXyxcRetW0Rxqt07SylV/9WHBu7hQKYxJsvO0HvARYArWBhjsu1jHtOqGmN+cXu9U0T2AgmAz2Chjl3PhFh6JsQysmdbrhzRlf+s3sbStD08fflgr20cI5/4iogw4d5zT+STdTtJ25XHkC7x9O3YEoBb314L6HxVSjVGoQwWnYHtbts5wIhjvYiIDAeigK1Bypfy4dJTujCqVzsSWzenZ0IsU19ZxeGjnmMxHOWGJ77Y4tp+6JNNpO7MY/0j42s7u9UqKzcUFDto1SyyrrOiVKMQymDhrbP+MdV5iUhH4E3gWmNMlUUdRGQaMA2ga9euNcmjchMWJiS2bg7AoMR41j8ynryiUhZt3I2j3OAoL+d/v9hCYUnFNCIpv1ljOaa9WVEFePLMJQzpEs+864fX7g24efCTTby7ahuZf59IRLiOPVXqeIXyf1EO4N6KmgjsDPRkEWkJfA48aIz5yVsaY8xcY0yyMSY5IUHXdQiFljGRXDasC1eO6Mo1I5P46YGzvKb7Or1ipb+DR0pZnp7Ly99leU1bXgvTjbyfYhVqS8u0cV6pYAhlsFgN9BaR7iISBVwBLAjkRDv9x8Abxpj3Q5hHdYziYiJZ/Zezufn0Hn7TPvZ5Go8vTGPnoaNsP3AEgEWbdtPjgYVMeyO0nREcdkAqKdNVBpUKhpAFC2OMA7gDWAykAfONMakiMlNELgQQkWEikgNcCswRkVT79MuAscB1IrLO/hkSqryqY5MQF839E/uy8oGzaB4Vzls3jmDpPWO9pp37bRanzfqKMU8ut7etpqclm/fgqIUHeakGC6WCQgflqaD5z+pt/PnDjT6PX57chf+kbPfYF6qeU0kzPgfgp/vP4oRWMSF5D6Uag3oxKE81LZcP60r2rEls+dsEfnlsIreN6+nqVgtUCRQAl835EYBdh4/yp/nrgz4TrpYslAoOnRtKBZ1zLqn7JpzExUM7M+2NNWTtK/SadtWvB8jNL+YvH2/iqy172ZtfxAWDOzGwcys27jhMUWkZU0/t5hr4566otMzvvFXaZqFUcGiwUCHVq30cX00fR2lZObn5xVb7Re92JHdrw9NLrbGX17y6iuZR1kP/u4x9fJexz+MaI3u0pXeHOI99h46UMGTml9w/8SRuPr2nz/fXkoVSwaHBQtWKyPAwOsU346s/nU6HljEcLS1zBYu0XXnVnus+rsPp4BGruuqZpRnVBguH3XU2r6iUuOgIryUUpZR/2mahalWPhFhaREfQLja6yjHnmhyV/W72D6zffoi9eUUkzfic7zJyOWoHkKOl1a8zXlJWzvYDRxj01yW88eNv1aY9UuJg0abdAd6JUk2LlixUncmeNYkSRzmbd+WxIecQyd3asGC993GbDy9IZWzvdoA1OvvSUxJdxz5dt4OLhnT2el6po5zM3AIAlmzezbWnJfnMz4OfbOKjtTv4/M7R9O/UqoZ3pVTjpCULVaeiIsIY0iWea0Ym0b2dtX74oMRWvHSNZ0++9dsP8X9fZQLw2/4j/HOJa65JlqT6ninXUW64ft5qAML8VEH9tt8aOFhYXEZ5ueHcp7/lljf9rzaoVFOgwULVG82iwsmeNYkFd4zmnH4dWPnAWTxyQT+/5x0tLWP99ooJib9O3+t67d4bytlesSxtD99lWNOT5Bw8wuzlmRhjKLfHHIlAfrGD9D35LErdTbbdk2vp5j1VJlb05oM1OSTN+JzDR2rWDfjWt9bw4jc6b6aqXzRYqHqrQ8sYrh/Vndn2Sn2tm0cyonubKum+2rKXi2b/wLurtjF/9Xaus0sSAGuyD7peO8sVN76ewtRXVgFwxzs/84/F6WTvP4L7+FTnMrNg9ajafbiIm95I4c53f/ab79dW/ArAbwe8dxc2xvDBmhyKfLS3fLFpN7PcZvZVqj7QNgtV700a1JG2safSq30sxsAXm3bx8KepVdLd/1HV0ePPLc90vY4Mr1oNlW8PAjxS4nBNiVxcWu7xIC8qLSc8zAHAb/u9BwB34XYJpszHhIlfbdnL9PfXk7E3n/sn9vV7PaXqAw0WqkFwX33vmpFJxDePIiYijGnVtCkktW1Ott0OAbA0bS9PfVnR1nH4SClbc62H/8HCUpxFi6LSMo9gUVDsoFmUVQj31+4B1lTvgKtaqzJnt9/c/OJqr/PztoOUlRuSk6qWpkLttR9+JWtfITMvGlDr763qJ62GUg3ShYM7Mb7/CTw4qS8ndohj4Z1j6JHQwiPNA+dVfGsf2Nnq3fTssgzXvsEzl7heHzhS4ipZFDnKKCqtqIaa9mYK2w8cBaz2jMoqz69WUbKwtrftP+JqI4GKKdrD/QSe3z+/gskv/ujz+EOfbGK2W8kpGL7cvIeCYgd//e9mv12NVdOiJQvVoN00pgc3jbGmS//qT+NI3XmYDi1jaNsiCvdaoG5tm7Nxx2Gf1zlQUOxqs8gvclDkqChZ5Bc5+OeSdKCiZDF7eSbn9u9Ar/ZxXPLCCtZuO8TsK4cyaVBHVxrn6PELnvuew0dL2fToucRGR1Bmv1F4WNVgsftwUbX3W1pWzps//sbUkd148yfrYX77Gb2qPSdQv+4r5H/eSGHigBOCcj3VuGjJQjUq/Tu1ol1sNCJCeJjgfB57GwTo7sCRUortAHH/RxvZUmlUubP9IUyE/KJS/rE4nSkvrQRg7TarJ9bjC9OAiiBQVFrG8vS9rh5UR4odHtfa5SUwXP3Kymrz+fqKbGZ+tpnXV2RXm64mjpRY+XOvuvNmb14RW+2xK6rp0GChGrW7zuoDQP9OLatN9/J3WR6r6j3kpQEdoNhRxiG7zaGgyOFREnDWKlUEi3LXGA/nNlRUW33zSy578jwDxrYD1T+o84usB3qe/TsQG3IOuUa8O63M2u+zsd7fsgXDH1/GWf/6JuD3V42DBgvVqP3hzF58cdcYJp+SyPCkNjxx8UDWPHg2c6ee4pHuSEkZv3qZGffh861xHjkHrTaL7P1HuPiFFQAYDGu3VXTNdT5jnQ3cGXvzPa7lrNpyrx676LkfABj5xDJmfbHF75KzrmaOANehOVBYwoXP/UDfhxexaNMu1/7L5/7E6f/42vPahG7erLXbDrrGq6iGSYOFatTCwoS+HVsiIsy/ZSRThnelbWw04/tX1MvfdVZv1+sbR3f3OP+qU7sSFx1BQXHFN3n3Xkz7CipeO6uxIuxg8czSisZ0wPXt3n0m3N12yWLX4SJe/GarazlYd96+6Qe6ZFmxW9vLwo3WvFc/ZO7zlTxkLn5+BeP++XWtv6+7v3y8ka+2+B7tr6qnwUI1WZclJ9KjXQvuPKs3zex1Mbq0bsazU052pYmOCKdz62Zezy8qLfcY77GvoISkGZ/z1Za9PtKXefwOlHvPLGfjeU0WuIxvHklWbgFXvVx9u0hj9fbKbdzwmq6mWVPaG0o1WU9OHux6/c1943j88zQuOSWRuJhIBnVu5So1nHhCHFt25/u6TMCK7FHhBcWewcJfG8GREgfN7PU+XLVQAZYtSh0V6WIiw6udrsTXuBClQEsWSgHQPi6GZ644mbiYSACS2rVwDYa7YVRF1VT6YxO8nn923/Z+3+PfS3/h+nmrKCj2fGC7N6x7c8StcdrZZuHwc47r2uUVpRL3UereOHtpaczwrbzcsHzLXr8BvjHSYKGUH4O7xLP+kfGkPHg20RHh3DauJ2/eONx1/L1ppxIb7b+QvnbbIZan51JQqSeTe7uCN+5rdjibNAJX7TjLAAAYA0lEQVStynJvH8mvpgdVYbHDa1defw3u9cWiTbuCvn67N2/8mM31r63mvxt2+U3b2GiwUCoArZpFusZq3DfhJMb0TuCx3w0guVtrRnRvQ2yMFSxG92rH2X07VHutT9Z5rtmRurPqSoGd4yvaSa622xh2Hjrqmq6k2BHYcrHuJZD8IofPh///vJHCLW9ZU6e4V3F5a3B38jX3FcB3GbnH3DZTU9n7CrnlrbX8af56n2mCFfR2HLJ6xe20fzclIQ0WIjJBRNJFJFNEZng5PlZE1oqIQ0QmVzq2SEQOichnocyjUjV19and+ODW0xAR19iLXu1jefnaZLJnTcLLAG2vrpj7U5V97007ldvGWcvF7s0vpqi0zOMBVeIjWJSVG85+6hs+t7/5upcs8o6W+jxvxdb9Xvc7yn0HJV/rm6fvzmfqK6t49L/ex6oEm7Oabns1Y1SqC3rHIjzMemRWFygbq5AFCxEJB2YDE4F+wBQRqbw4wTbgOuAdL5f4BzA1VPlTKpj22t1pJ7ut4Pc/9jQkx+qT20fRpU1zJrhNu3HwSAnvrNzm2j7iZV1ygMISB5l7C5j+/noe/nQTH6zJcR3LL3Lw+Ubv1SfRERWPAvfq+Ooesr6OHTpSAkDm3sBGeS/atIvb31nr9ViJo9x1PafyckOW2wjyQBr7g9V4H27/mTRYBNdwINMYk2WMKQHeAy5yT2CMyTbGbACqfEUxxiwDjr8LilK14PHfD+C2cT3p17FipPh9E05iyvCuAHx022nMcRsI2KNdiyrXcBrSJR6wei85PbXkFz76eYdr+4DbA/SZpb/gKCvnn4vT2WO3O5Qbwxs//sbbdoDp0qYZGXvzXdvuCosdPqu13Kuxdh8u4tN1FXkodZSzaNNuftlzfP9Nb3lrraskVNltb69hyMwvPfY9/3UmZ/7rG9Ir9VCTaiZmDNbD3VmyCFZJBaz7Of0fy4N2vVAJZdfZzsB2t+0cYEQI30+pOtOrfRz3TTjJY194mPDExQN54uKBrn2vXpfMDa+lMDk5kdSdeT4fkmAt/uT0vlsJASAl+4Dr9TNLM+jVPpbnlmeywZ4ssfLDv02LaNfMuZW99F2Wx7b7Y9BZDWWM4aqXf3JN6Q5WNZSznSN71iSv5/+6r5Ckts193KEnY0yVB/7StKpjVlb+at37rsNHOfGEuIB6b5UFqWThHHAZzIb/JxelB+1aoRTKkoW3MB/UspuITBORFBFJyc3N9X+CUnXszJM6sO7hc7j19J7MvnKoR2kjIkzo2qbiwdqqWSTf3DvO63UqP6ucy7CW+WhjuGFUks88VR5p7l595PxGPufbLI9AAZ5L1h4oLHF1K3W2ZazOPsgZ//yab37x/X/TfYqVEh9tIABpu/K4/e21Hu0k1ZUkKgvWw90571cwSxYNRShLFjlAF7ftRGCnj7Q1YoyZC8wFSE5ObnqfnmqQ4ptHuV5HubUVZPx9YpUHYJfWVb+Vt4uNYl+BZz3+ph1Wj6qVWQeqpAfo0yGOPh1i+WWPZzuCv4eosxrqHS/VV3lHK7riDv3bl1WOO63f7n1q+IOFJZzhNgVIaZnBVw/kO95Zy9bcQm47o2fFWun2sUAKDcFr4K5+Yavj4a1kVZ+EsmSxGugtIt1FJAq4AlgQwvdTqsEZltSGwYmt+OwPo70+KMLCpMr6Ev++4uQq6Zx8PRQjwz1LLU6XzvG9uBJYDeZ78oo4WKmRGeC8Z7+r9lynyoMQnY5U6lrr3lOrrNyQurMiyDir1QTBWXhyVpGVVtNjyykYJYvDR0pda6MHOijyWFQuWR0+WlqvxrmELFgYYxzAHcBiIA2Yb4xJFZGZInIhgIgME5Ec4FJgjoi4+tqJyHfA+8BZIpIjIueGKq9K1ZXY6Ag+vWM0A+yV/Lx5/qqhZD1+HtPHW9OtD+/ehs/+MJpHL+wf8PtEhofx998P5MQOcYzu1Y7e7WMBWPPbwWrPm/DMd4x4fFm1A/r8eeunqqWS6+etYt73v3rse21Ftqv767+XZTDp2e9dx5zVTyIVvZ9ueC2FLzbuclWVVfedPNA2i9z8Yr7P8D7R4tzvtrpeh6Jk4T6S/2BhCYMfXcLTS3+p5ozaFdJxFsaYhcaYPsaYnsaYv9v7HjbGLLBfrzbGJBpjWhhj2hpj+rudO8YYk2CMaWanWRzKvCpVX4kIYWHCHWf2JnvWJCLDwxjQuRVTT+3G478f6Pf8Hgkt6NqmOR1axrD47rG8ddMI/mdszbr11sTRSiWIt376jeXpubxcKVg8uyyDa19dBcD67Yc8jjlLHeXGeLTXvLXyNw4WWqWe/YXFTH1lJXvziigsdnh0r/XWG6qg2MHnG3Z5TN1x61truPqVla6FoNzNXl4RLKobf1JTpW4lq/32Pfnq6lwXdAS3Ug1UWJhw5YiuDEr0XSoBmHpqtypVXJWnJ+nUKsZj+4HzPHt2BcuGnEM8+Mkmn8ez9hVSVFrm6nXk5AwWJY5yj24ypWWGaW9aPbL25BXzXcY+5nybxX0fbuDMf31DoT21vPuz3VlKeX55Jre/s5bl6RU9rvbkW12Ps+zG/KMlZV5Ha1fTFl9jXgc51p9aKA0WSjV0zmDxx7OtdTnim0d6HG/hpdW4fZznMrMDE1ux5sGzibJHnU0c0DEUWeVCe7Gn6vz5ww3srDRPlXPG3sqTLhYWVy0B5Bw8wjp7qduf7d/u1VCb7O7FztKGs3PAz9sO0jzS+ls5A8S181Zx2qyvqryHr9Hr3uwvKObrdO/T1rsr8ejpZf2uR7FCg4VSDd1D5/fj7ZtG8Mez+/DqdcksuH2069iDk/py8cmdq5xzSrfWHtt/vbA/bWOj2fK3CaQ/NoHWLaKqnOOueVS4x9iKYPp03U7SfKyBXuIod/VIAl/B4ihx9lxdzsWlfsqqmM6kcntDeJhwtKSM3z+/gnR7gKGzQX2VPaaj8iyz/10feMfOa15dxXXzVvucasXJPRBW9PSy9u0+XMTcb7fW6Wy3GiyUauCiI8IZ1asdYI3j6GoPgmsXG8VNY3oQEV71v7mI8PFtp9E5vhnrHxlPx1bWxIVhYUJ0RDgtosKrnLPoj2Ncrz+5fRQAb944nPsmnMjNldpA1j50jut1oHNkBaK0rNyjF1WBl2CRujOPHfYyuM41Se7/aKPr+Idrd1BY7HB9ky92lFe5TuVBjd62nSUUf5wTRfqbXdg9mDibWJyh4ea31vD4wi1+12gPJV38SKlGaNOj5xLup8/+yV1b88OMM70e89aNt03zKN65aQQ/Ze2nT4c4AMb0TmBM7wTAGrjnSutWMvnm3jMY82TV6Sy6tGnmc1S5L/9Zvd2j8bvyeBOnfPvhn5tfTEal6UjeWbmNUke5q6qnuLSsSrCoXArwNheXv4c/wEtuf5NiRzlx1aR1r9qqvLbIfrfle+uKliyUaoRioyNcq+sFS3zzKE7r1Y57xp9YbbqXrkn22O7SprmrPcVdq2aRVfb5syh19zGlzztayjlPf1tlf9a+QleAKCotq1Kd9dFaz+lV7vug6vTnkZVKbN/8ksu/lnhO3fH3hWmu1+6lk5TsA+w4dJSX3aZacW+zqNzbyjnewj2g/La/kAGPLPYYBR9KWrJQSgXEfbS5N2N6t+PkLvGc089az+Ous3qzbMueKuluGNWdV3/4lRZRoX/8/OJj5lv38SVZ+wp56FPPHlopvx1kwjMVQcbbHFXONob12w/Rs32sq9vvn3wE0+LSMt5e+RsljnIe/e9mL8fLmb08k/Zx0Zx4glUGcY4pcQ62dF+P/eOfd1BQ7OCjtTk+3zOYNFgopbwaltSaw0dL+WVPAf07tfSb/s0bPecJvfucPtx9jjWQ8Oy+HXhmaQYPn9+P60cl8ZdJfdlx8CjPLP2FWZcM4v012/nLx9YD+9QebfjJx7Qlx6ryeA1vvvMxCM/fuuulZeWUlpVz0ewfSHbrMLBu+yH+NH8dV5/azSP9ql8PuO7Rm4JiB/9YbJVMPrrtNMCqhhr5xDLXFPi1taCUNxoslFJevX/LaUG71oDOrTx6T4ULdG3bnKcuHwLAVSO6ccWwrhhjeGfVNo9gccaJCSxPr5iM8JEL+nn9Zu7unH4d+HJz1VJNMJU4yl1L5Ka4lVRueXMNu/OKquRxhlsjuzf5bsvCurdZuC93616yqG3aZqGUqhfCw4SI8DAuGNSJy5IT+ct5fQGIi7HaNkRgyd1juX5Ud9eaH75EeekBBvDnCSdx+xk9g5LfEi+9qAAiI2rW/SvvaEWwcM49VbkXVlFpGX0e/IKXv8tyNX7X1tSDGiyUUvVK6xZRPDl5MMO6twGgd/tY3pt2Kj8/dI6rF9ZL1yRzkl2vf+Po7q5znWuX+5q7aVBiK6LCg9PwX1pW7jHZodOx9vBycp9/69UfrKlQ9lXqBZVXZC2N+9jnadQ2DRZKqXppSJd4Pr19FLeO68mpPdp6TO2eEBfN5cOsFRCiIsKYdfFAWjWLZMZEa5qSYke5ayT70ntOd503qlc7vw31gSopK+eWt7wvB1sTv+6v6NXkqwrN2XZhTahYuzRYKKXqrcFd4r0OKgSYMrwr149K4rZxPblieFfWPzKepLbWcrU7Dx1lyd1jWfTHMfSyZ9htZi9T6wwWFw3pxD8mD3Jd7/Q+CSz+41jusRvlnS4+uTPnD6o6/ckHlVYv9OXZKb6nlHf30dodftNstgf4RYaHuUZzP/tVJle/vDKg9zge2sCtlGqQYiLDeeQCz2nae7a3gsWVI7rSPi6G9nHWBInLp49zTQEyccAJPLUknVvH9STDXgxq4oATeOFqa9XCX/dZ+24/oydjeycwokdbtuYW8FmlJXC99aI6vU9ClZUBo8KFJXePZbyX8R5Du8bTLjaao6VlPntluVu77aB9zTCPnlHHMldVTWmwUEo1Gs2jIrzOWdW9XQvX607xzUidOQGArm2ac3bfDtw/sa/r+IQBHflhxpmu9g+AngmxzL95JJd5WSwqJjLM1UupT4fYKsGiW9sWrvVDKosIC2PuNcnc9d7PAd1fjj2NSUGxwzUzLlSdRTgUtBpKKdVkNY+K4OVrk13zaTm5Bwqn4d3bcG5/a8Chc6bf1s0jef364a40zkWsIsPF9btvx5Y+l0vNs7vLOks9x2LZloqBgs01WCilVP3xwlWnsPXx81zdeptFhjOiR1suGZoIQHKS1YNrWFIbXrx6KF/cNdZ17r3nVh1l7Wywvm/C8a0fUlDkfenaYNJgoZRSAQoLE8LDxNVo/vAF/QD4++8H8O29Z9A5vhmvXpfMC1edwoQBHV3pwJrmBKzR7E5XjegKQMuYinmy1j8ynnaxnuuN+JOZ631ak2DSNgullDpGbWOjPdpGYiLDXVVZZ57Uwes5zaLCyXr8PESskdjREWGEeZm/vVWzSB46vy93vbcOgAV3jOL9lBzO7NueBet2sjRtT5U10Y+WhL6BW0sWSilVS8LCBBGhWVR4lUDx7b1nsHz6OAAuGtLZNQo9sXVz/va7AZxxYnuevnwICW6rHPbr2JK46AjunxiaZXA98h7yd1BKKeVX17bNPXptRdiN5M7fTo9eWNFduKSsnI2PnsslpySGPH8aLJRSqh56+Zpkzu7bnthKU7mP6Z3AGzdYPbCc61zUhpAGCxGZICLpIpIpIjO8HB8rImtFxCEikysdu1ZEMuyfa0OZT6WUqm9O69WOl68d5rVdw7nwUo+EFlWOhUrIGrhFJByYDZwD5ACrRWSBMcZ93t5twHXA9ErntgEeAZKxpkBZY597EKWUauKGJbXm5rE9uGlMD/+JgySUJYvhQKYxJssYUwK8B1zknsAYk22M2QBUbso/F/jSGHPADhBfAhNCmFellGowIsLDuP+8vh6N3aEWymDRGdjutp1j7wv1uUoppYIslMHC2/j2QFtjAjpXRKaJSIqIpOTm5no5RSmlVDCEMljkAF3cthOBncE81xgz1xiTbIxJTkhIqHFGlVJKVS+UwWI10FtEuotIFHAFsCDAcxcD40WktYi0Bsbb+5RSStWBkAULY4wDuAPrIZ8GzDfGpIrITBG5EEBEholIDnApMEdEUu1zDwB/wwo4q4GZ9j6llFJ1QIyPtWobmuTkZJOSklLX2VBKqQZFRNYYY5L9pdMR3EoppfzSYKGUUsqvRlMNJSK5wG81PL0d4H8B3MZF77lp0HtuGo7nnrsZY/x2J200weJ4iEhKIHV2jYnec9Og99w01MY9azWUUkopvzRYKKWU8kuDhWVuXWegDug9Nw16z01DyO9Z2yyUUkr5pSULpZRSfjX5YOFvNb+GSkS6iMhyEUkTkVQRucve30ZEvrRXIPzSnnsLsTxr/x02iMjQur2DmhGRcBH5WUQ+s7e7i8hK+37/Y89ThohE29uZ9vGkusx3TYlIvIh8ICJb7M96ZBP4jO+2/01vEpF3RSSmMX7OIvKqiOwVkU1u+475sw3WqqNNOli4reY3EegHTBGRfnWbq6BxAH8yxvQFTgVut+9tBrDMGNMbWGZvg/U36G3/TANeqP0sB8VdWHOROf0v8LR9vweBG+39NwIHjTG9gKftdA3Rv4FFxpiTgMFY995oP2MR6QzcCSQbYwYA4ViTlDbGz/k1qi76dkyfrduqoyOwFqR7xBlgjpkxpsn+ACOBxW7b9wP313W+QnSvn2ItcZsOdLT3dQTS7ddzgClu6V3pGsoP1lT2y4Azgc+w1kXZB0RU/ryxJrgcab+OsNNJXd/DMd5vS+DXyvlu5J+xc2G0Nvbn9hnWypqN8nMGkoBNNf1sgSnAHLf9HumO5adJlyxoIivy2UXvk4GVQAdjzC4A+3d7O1lj+Fs8A9xHxTK9bYFDxpoBGTzvyXW/9vHDdvqGpAeQC8yzq95eFpEWNOLP2BizA/gnsA3YhfW5raFxf87ujvWzDdpn3tSDxfGs5tcgiEgs8CHwR2NMXnVJvexrMH8LETkf2GuMWeO+20tSE8CxhiICGAq8YIw5GSikolrCmwZ/z3YVykVAd6AT0AKrCqayxvQ5B8LXfQbt/pt6sDie1fzqPRGJxAoUbxtjPrJ37xGRjvbxjsBee39D/1uMAi4UkWzgPayqqGeAeBGJsNO435Prfu3jrYCGtmZKDpBjjFlpb3+AFTwa62cMcDbwqzEm1xhTCnwEnEbj/pzdHetnG7TPvKkHi+NZza9eExEBXgHSjDFPuR1aADh7RFyL1Zbh3H+N3aviVOCws7jbEBhj7jfGJBpjkrA+x6+MMVcBy4HJdrLK9+v8O0y20zeob5zGmN3AdhE50d51FrCZRvoZ27YBp4pIc/vfuPOeG+3nXMmxfrbBW3W0rhtw6voHOA/4BdgK/KWu8xPE+xqNVdzcAKyzf87Dqq9dBmTYv9vY6QWrZ9hWYCNWb5M6v48a3vs44DP7dQ9gFZAJvA9E2/tj7O1M+3iPus53De91CJBif86fAK0b+2cMPApsATYBbwLRjfFzBt7FapcpxSoh3FiTzxa4wb7/TOD6muZHR3ArpZTyq6lXQymllAqABgullFJ+abBQSinllwYLpZRSfmmwUEop5ZcGC6X8EJEyEVnn9hO02YlFJMl9VlGl6qsI/0mUavKOGmOG1HUmlKpLWrJQqoZEJFtE/ldEVtk/vez93URkmb2uwDIR6Wrv7yAiH4vIevvnNPtS4SLykr1GwxIRaWanv1NENtvXea+OblMpQIOFUoFoVqka6nK3Y3nGmOHAc1hzUWG/fsMYMwh4G3jW3v8s8I0xZjDWHE6p9v7ewGxjTH/gEHCJvX8GcLJ9nVtCdXNKBUJHcCvlh4gUGGNivezPBs40xmTZkzbuNsa0FZF9WGsOlNr7dxlj2olILpBojCl2u0YS8KWxFrNBRP4MRBpjHhORRUAB1jQenxhjCkJ8q0r5pCULpY6P8fHaVxpvit1el1HRljgJa76fU4A1brOqKlXrNFgodXwud/v9o/16BdbMtwBXAd/br5cBt4JrrfCWvi4qImFAF2PMcqwFneKBKqUbpWqLflNRyr9mIrLObXuRMcbZfTZaRFZiffGaYu+7E3hVRO7FWsnuenv/XcBcEbkRqwRxK9asot6EA2+JSCusGUWfNsYcCtodKXWMtM1CqRqy2yySjTH76jovSoWaVkMppZTyS0sWSiml/NKShVJKKb80WCillPJLg4VSSim/NFgopZTyS4OFUkopvzRYKKWU8uv/AYDwVfF8J/3AAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_values = history.history['loss']\n",
    "epochs = range(1, len(loss_values)+1)\n",
    "\n",
    "plt.plot(epochs, loss_values, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/akashsri99/deep-learning-iris-dataset-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_predictions = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.8834589e-01, 5.1165402e-01],\n",
       "       [9.4926453e-01, 5.0735451e-02],\n",
       "       [9.8586893e-01, 1.4131060e-02],\n",
       "       [1.0000000e+00, 1.5791580e-08],\n",
       "       [9.5016074e-01, 4.9839217e-02],\n",
       "       [9.2717189e-01, 7.2828084e-02],\n",
       "       [1.0000000e+00, 0.0000000e+00],\n",
       "       [4.3227158e-02, 9.5677280e-01],\n",
       "       [3.0611655e-01, 6.9388342e-01],\n",
       "       [8.3324325e-01, 1.6675675e-01]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_class = np.argmax(test_y,axis=1)\n",
    "y_pred_class = np.argmax(test_y_predictions,axis=1)\n",
    "\n",
    "y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_class[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(648,)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_test_class.shape == y_pred_class.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced classification problem\n",
    "https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/\n",
    "This happens because Machine Learning Algorithms are usually designed to improve accuracy by reducing the error. Thus, they do not take into account the class distribution / proportion or balance of classes.\n",
    "\n",
    "[A Review of Class Imbalance Problem (Shaza M. Abd Elrahman and Ajith Abraham)](http://ias04.softcomputing.net/jnic2.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98       619\n",
      "           1       0.38      0.10      0.16        29\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       648\n",
      "   macro avg       0.67      0.55      0.57       648\n",
      "weighted avg       0.93      0.95      0.94       648\n",
      "\n",
      "[[614   5]\n",
      " [ 26   3]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_class,y_pred_class))\n",
    "print(confusion_matrix(y_test_class,y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARIES\n",
      "---------\n",
      " btw, why are ou guys calling this abstraction thingy GComm when Dave has a project called GNU Comm (GComm fr short)?\n",
      "as far as I'm concerned, GComm is our internal package name... to the external world, it's GNUe Common\n",
      "but that is a good point\n",
      "pyro is an object system like what gcomm will be\n",
      "by same guys that wrote pygmy\n",
      "the python email client\n",
      "I need a production quality GNUe web shopping cart ;-)\n",
      "anything is possible I know, but ideally we need to get ideas on some sort of php and GEAS interface\n",
      "guess I'll just customize interchange.. hopefully actually have it done in a few days\n",
      "our inventory package isn't completed\n",
      "but if you have an inventory package\n",
      "you should be able to access it via geas\n",
      "I know a web interface for GNUe Forms is in the works, but I'm sure you need something relatively quickly\n",
      "madlocke was working on that, but as you probably know, he's been out of commission (sick) lately\n",
      "a new release (feature wise) is probably about 3 or 4 weeks away since the db upgrade is going to be huge\n",
      "I may make an interim bug fix/small feature release to get some of the email support down\n",
      "but we are seeing a GREAT value in DCL as a communication tool to customers\n",
      "as well as a billing tool\n",
      "and todo tool\n",
      "the todo tool works fine\n",
      "but the communication tool is flawed\n",
      "if the accounts dont have same products\n",
      "it should be fairly easy to implement who can view what\n",
      "by product\n",
      "but then the next step becomes billing (or services)\n",
      "at which point products and services need to be 'separate'\n",
      "but that was because i didnt see dcl as a communications tool\n",
      "your use of services sounds like it could be an action\n",
      "as I said, I am going to be working on this stuff (minus billing) for work, so it will come\n",
      "\n",
      "PREDICTIONS\n",
      "-----------\n",
      " well, I managed to slip past the nice men in the white coats and find a terminal\n",
      "btw, why are ou guys calling this abstraction thingy GComm when Dave has a project called GNU Comm (GComm fr short)?\n",
      "as far as I'm concerned, GComm is our internal package name... to the external world, it's GNUe Common\n",
      "I jst wrote my house rep and senators\n",
      "dude... we will have sent out over 10,000 letters using GNUe Reports mailmerge feature as of this week :)\n",
      "a new release (feature wise) is probably about 3 or 4 weeks away since the db upgrade is going to be huge\n",
      "i think registering new projects on savannah is your favourite hobby, isn't it? ;)\n",
      "and in like every one that sounds so interesting to me that i have a closer look your name appears :)\n",
      "\n",
      "ROUGE Scores\n",
      "------------\n",
      "Evaluation with Avg\n",
      "\trouge-1:\tP: 0.529412\tR: 0.529412\tF1: 0.529412\n",
      "\trouge-2:\tP: 0.396040\tR: 0.396040\tF1: 0.396040\n",
      "\trouge-3:\tP: 0.390000\tR: 0.390000\tF1: 0.390000\n",
      "\trouge-4:\tP: 0.383838\tR: 0.383838\tF1: 0.383838\n",
      "\trouge-l:\tP: 0.588610\tR: 0.588610\tF1: 0.588610\n",
      "\trouge-w:\tP: 0.410296\tR: 0.242919\tF1: 0.305163\n"
     ]
    }
   ],
   "source": [
    "from rouge_metrics import *\n",
    "from get_sentences_from_line_numbers import *\n",
    "\n",
    "predicted_line_numbers = [index_for_validation_test_split + index + 1 for index, value in enumerate(y_pred_class) if value==1]\n",
    "summaries_line_numbers = [index_for_validation_test_split + index + 1 for index, value in enumerate(y_test_class) if value==1]\n",
    "\n",
    "summaries_chat_lines = get_sentences_of_line_numbers(CHAT_LOGS, summaries_line_numbers)\n",
    "predicted_chat_lines = get_sentences_of_line_numbers(CHAT_LOGS, predicted_line_numbers)\n",
    "print(\"SUMMARIES\\n---------\\n\", summaries_chat_lines)\n",
    "print(\"PREDICTIONS\\n-----------\\n\", predicted_chat_lines)\n",
    "\n",
    "hypotheses = [predicted_chat_lines]\n",
    "references = [summaries_chat_lines]\n",
    "print(\"ROUGE Scores\\n------------\")\n",
    "print_rouge_results(get_rouge_results(hypotheses, references))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Recurrent Neural Network Instead\n",
    "It seems that adding sentence vectors as windows around the sentence does not work well with a feed forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_sentence_vectors(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "218\n",
      "(25000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 2,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 2,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 2,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 2,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 2,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(len(X_train[0]))\n",
    "print(X_test.shape)\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   19,  178,   32],\n",
       "       [   0,    0,    0, ...,   16,  145,   95],\n",
       "       [   0,    0,    0, ...,    7,  129,  113],\n",
       "       [ 687,   23,    4, ...,   21,   64, 2574],\n",
       "       [   0,    0,    0, ...,    7,   61,  113]], dtype=int32)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    1,  591,  202,   14,   31,    6,  717,   10,\n",
       "         10,    2,    2,    5,    4,  360,    7,    4,  177,    2,  394,\n",
       "        354,    4,  123,    9, 1035, 1035, 1035,   10,   10,   13,   92,\n",
       "        124,   89,  488,    2,  100,   28, 1668,   14,   31,   23,   27,\n",
       "          2,   29,  220,  468,    8,  124,   14,  286,  170,    8,  157,\n",
       "         46,    5,   27,  239,   16,  179,    2,   38,   32,   25,    2,\n",
       "        451,  202,   14,    6,  717], dtype=int32)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_36 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "imdb_model = Sequential()\n",
    "imdb_model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "imdb_model.add(LSTM(100))\n",
    "imdb_model.add(Dense(1, activation='sigmoid'))\n",
    "imdb_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(imdb_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 204s 8ms/step - loss: 0.5887 - acc: 0.6728 - val_loss: 0.5178 - val_acc: 0.7478\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 193s 8ms/step - loss: 0.3717 - acc: 0.8384 - val_loss: 0.3916 - val_acc: 0.8476\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 195s 8ms/step - loss: 0.2686 - acc: 0.8935 - val_loss: 0.3383 - val_acc: 0.8628\n"
     ]
    }
   ],
   "source": [
    "imdb_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb_loss_values = imdb_history.history['loss']\n",
    "imdb_epochs = range(1, len(imdb_loss_values)+1)\n",
    "\n",
    "plt.plot(imdb_epochs, imdb_loss_values, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16079, 156)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0.006050</td>\n",
       "      <td>0.788992</td>\n",
       "      <td>8.923999e-08</td>\n",
       "      <td>-7.957217e-09</td>\n",
       "      <td>-5.274117e-09</td>\n",
       "      <td>-1.910836e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>5.032557e-08</td>\n",
       "      <td>4.709798e-09</td>\n",
       "      <td>-7.814833e-08</td>\n",
       "      <td>3.333242e-08</td>\n",
       "      <td>-1.757239e-08</td>\n",
       "      <td>-4.967683e-09</td>\n",
       "      <td>3.275183e-08</td>\n",
       "      <td>-4.288362e-08</td>\n",
       "      <td>2.537755e-09</td>\n",
       "      <td>2.285525e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.026734</td>\n",
       "      <td>0.042729</td>\n",
       "      <td>1.138727e-09</td>\n",
       "      <td>-1.818176e-08</td>\n",
       "      <td>4.798956e-09</td>\n",
       "      <td>-5.157600e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>3.880125e-08</td>\n",
       "      <td>-1.505285e-08</td>\n",
       "      <td>-3.228048e-08</td>\n",
       "      <td>6.559005e-08</td>\n",
       "      <td>2.989873e-08</td>\n",
       "      <td>-2.563377e-08</td>\n",
       "      <td>-3.230809e-08</td>\n",
       "      <td>-1.410832e-08</td>\n",
       "      <td>3.297067e-08</td>\n",
       "      <td>8.902228e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.031086</td>\n",
       "      <td>0.051728</td>\n",
       "      <td>-4.177528e-09</td>\n",
       "      <td>-1.188295e-08</td>\n",
       "      <td>2.389462e-08</td>\n",
       "      <td>-1.349990e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>2.331976e-08</td>\n",
       "      <td>-2.124629e-08</td>\n",
       "      <td>-5.314322e-09</td>\n",
       "      <td>4.871865e-08</td>\n",
       "      <td>2.572267e-08</td>\n",
       "      <td>-1.130713e-08</td>\n",
       "      <td>-3.515117e-08</td>\n",
       "      <td>2.479115e-08</td>\n",
       "      <td>9.735289e-09</td>\n",
       "      <td>3.229467e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001743</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.027952</td>\n",
       "      <td>0.142905</td>\n",
       "      <td>-2.529559e-09</td>\n",
       "      <td>-1.465508e-08</td>\n",
       "      <td>8.994467e-09</td>\n",
       "      <td>8.784771e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.564207e-08</td>\n",
       "      <td>1.737575e-09</td>\n",
       "      <td>2.070995e-08</td>\n",
       "      <td>-6.362434e-10</td>\n",
       "      <td>-2.201059e-08</td>\n",
       "      <td>2.865113e-08</td>\n",
       "      <td>-7.716492e-09</td>\n",
       "      <td>-3.310467e-08</td>\n",
       "      <td>-9.572791e-09</td>\n",
       "      <td>-1.052344e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>0.017715</td>\n",
       "      <td>0.084493</td>\n",
       "      <td>-2.828828e-10</td>\n",
       "      <td>4.331747e-10</td>\n",
       "      <td>8.529943e-11</td>\n",
       "      <td>-2.281351e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.957617e-11</td>\n",
       "      <td>-1.465501e-10</td>\n",
       "      <td>1.223374e-10</td>\n",
       "      <td>1.708342e-10</td>\n",
       "      <td>3.674844e-10</td>\n",
       "      <td>1.448876e-10</td>\n",
       "      <td>6.751768e-10</td>\n",
       "      <td>1.813555e-10</td>\n",
       "      <td>6.316711e-10</td>\n",
       "      <td>-4.996572e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   absolute_sentence_position  sentence_length  number_of_special_terms  \\\n",
       "0                    0.000436         0.013699                      0.0   \n",
       "1                    0.000871         0.178082                      0.0   \n",
       "2                    0.001307         0.136986                      0.0   \n",
       "3                    0.001743         0.068493                      0.0   \n",
       "4                    0.002179         0.109589                      0.0   \n",
       "\n",
       "   sentiment_score  normalized_mean_tf_idf  normalized_mean_tf_isf  \\\n",
       "0           0.6249                0.006050                0.788992   \n",
       "1           0.0000                0.026734                0.042729   \n",
       "2           0.0000                0.031086                0.051728   \n",
       "3           0.0000                0.027952                0.142905   \n",
       "4           0.2263                0.017715                0.084493   \n",
       "\n",
       "              1             2             3             4  ...           141  \\\n",
       "0  8.923999e-08 -7.957217e-09 -5.274117e-09 -1.910836e-08  ...  5.032557e-08   \n",
       "1  1.138727e-09 -1.818176e-08  4.798956e-09 -5.157600e-08  ...  3.880125e-08   \n",
       "2 -4.177528e-09 -1.188295e-08  2.389462e-08 -1.349990e-08  ...  2.331976e-08   \n",
       "3 -2.529559e-09 -1.465508e-08  8.994467e-09  8.784771e-09  ...  2.564207e-08   \n",
       "4 -2.828828e-10  4.331747e-10  8.529943e-11 -2.281351e-10  ...  2.957617e-11   \n",
       "\n",
       "            142           143           144           145           146  \\\n",
       "0  4.709798e-09 -7.814833e-08  3.333242e-08 -1.757239e-08 -4.967683e-09   \n",
       "1 -1.505285e-08 -3.228048e-08  6.559005e-08  2.989873e-08 -2.563377e-08   \n",
       "2 -2.124629e-08 -5.314322e-09  4.871865e-08  2.572267e-08 -1.130713e-08   \n",
       "3  1.737575e-09  2.070995e-08 -6.362434e-10 -2.201059e-08  2.865113e-08   \n",
       "4 -1.465501e-10  1.223374e-10  1.708342e-10  3.674844e-10  1.448876e-10   \n",
       "\n",
       "            147           148           149           150  \n",
       "0  3.275183e-08 -4.288362e-08  2.537755e-09  2.285525e-08  \n",
       "1 -3.230809e-08 -1.410832e-08  3.297067e-08  8.902228e-08  \n",
       "2 -3.515117e-08  2.479115e-08  9.735289e-09  3.229467e-08  \n",
       "3 -7.716492e-09 -3.310467e-08 -9.572791e-09 -1.052344e-08  \n",
       "4  6.751768e-10  1.813555e-10  6.316711e-10 -4.996572e-10  \n",
       "\n",
       "[5 rows x 156 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A many to many RNN\n",
    "#get number of columns in training data\n",
    "embedding_vector_length = 32\n",
    "num_of_cols = train_vectors_df.shape[1]\n",
    "rnn_model = Sequential()\n",
    "# rnn_model.add(Dense(num_of_cols, activation='relu', input_shape=(num_of_cols, 1)))\n",
    "rnn_model.add(Embedding(train_vectors_df.shape[0], embedding_vector_length, input_length=num_of_cols))\n",
    "rnn_model.add(Dropout(0.2))\n",
    "rnn_model.add(LSTM(100))\n",
    "rnn_model.add(Dropout(0.2))\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 150, 32)           514528    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 150, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_51 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 567,829\n",
      "Trainable params: 567,829\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16079 samples, validate on 3988 samples\n",
      "Epoch 1/10\n",
      "16079/16079 [==============================] - 35s 2ms/step - loss: 0.1416 - acc: 0.9684 - val_loss: 0.1325 - val_acc: 0.9709\n",
      "Epoch 2/10\n",
      "16079/16079 [==============================] - 36s 2ms/step - loss: 0.1414 - acc: 0.9684 - val_loss: 0.1323 - val_acc: 0.9709\n",
      "Epoch 3/10\n",
      "16079/16079 [==============================] - 36s 2ms/step - loss: 0.1413 - acc: 0.9684 - val_loss: 0.1333 - val_acc: 0.9709\n",
      "Epoch 4/10\n",
      "16079/16079 [==============================] - 36s 2ms/step - loss: 0.1415 - acc: 0.9684 - val_loss: 0.1316 - val_acc: 0.9709\n",
      "Epoch 5/10\n",
      "16079/16079 [==============================] - 35s 2ms/step - loss: 0.1413 - acc: 0.9684 - val_loss: 0.1316 - val_acc: 0.9709\n",
      "Epoch 6/10\n",
      "16079/16079 [==============================] - 36s 2ms/step - loss: 0.1410 - acc: 0.9684 - val_loss: 0.1316 - val_acc: 0.9709\n",
      "Epoch 7/10\n",
      "16079/16079 [==============================] - 36s 2ms/step - loss: 0.1409 - acc: 0.9684 - val_loss: 0.1321 - val_acc: 0.9709\n",
      "Epoch 8/10\n",
      "16079/16079 [==============================] - 36s 2ms/step - loss: 0.1411 - acc: 0.9684 - val_loss: 0.1316 - val_acc: 0.9709\n",
      "Epoch 9/10\n",
      "16079/16079 [==============================] - 36s 2ms/step - loss: 0.1416 - acc: 0.9684 - val_loss: 0.1318 - val_acc: 0.9709\n",
      "Epoch 10/10\n",
      "16079/16079 [==============================] - 36s 2ms/step - loss: 0.1411 - acc: 0.9684 - val_loss: 0.1323 - val_acc: 0.9709\n",
      "357.93049907684326 seconds\n"
     ]
    }
   ],
   "source": [
    "# rnn_history = rnn_model.fit(train_X, train_y, epochs=3, batch_size=64)\n",
    "rnn_start = time.time()\n",
    "rnn_history = rnn_model.fit(\n",
    "    train_vectors_df, \n",
    "    train_y_nums, \n",
    "    validation_data=(validation_vectors_df, validation_y_nums), \n",
    "    epochs=10, \n",
    "    batch_size=64\n",
    ")\n",
    "rnn_end = time.time()\n",
    "print(rnn_end - rnn_start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rnn_loss_values = rnn_history.history['loss']\n",
    "rnn_epochs = range(1, len(rnn_loss_values)+1)\n",
    "\n",
    "plt.plot(rnn_epochs, rnn_loss_values, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_test_y_predictions = rnn_model.predict(validation_vectors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03626591],\n",
       "       [0.03626591],\n",
       "       [0.03626591],\n",
       "       ...,\n",
       "       [0.03626592],\n",
       "       [0.03626592],\n",
       "       [0.03626592]], dtype=float32)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_test_y_predictions[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3988,)\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "rnn_y_test_class = np.argmax(test_y,axis=1)\n",
    "rnn_y_pred_class = np.argmax(rnn_test_y_predictions,axis=1)\n",
    "\n",
    "assert y_test_class.shape == y_pred_class.shape\n",
    "print(rnn_y_pred_class.shape)\n",
    "print(rnn_y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      3872\n",
      "           1       0.00      0.00      0.00       116\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      3988\n",
      "   macro avg       0.49      0.50      0.49      3988\n",
      "weighted avg       0.94      0.97      0.96      3988\n",
      "\n",
      "[[3872    0]\n",
      " [ 116    0]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_y_nums,rnn_y_pred_class))\n",
    "print(confusion_matrix(validation_y_nums,rnn_y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16079 samples, validate on 3988 samples\n",
      "Epoch 1/100\n",
      " 9728/16079 [=================>............] - ETA: 14s - loss: 0.1351 - acc: 0.9704"
     ]
    }
   ],
   "source": [
    "# rnn_history = rnn_model.fit(train_X, train_y, epochs=3, batch_size=64)\n",
    "rnn_start = time.time()\n",
    "rnn_history = rnn_model.fit(\n",
    "    train_X, \n",
    "    train_y_nums, \n",
    "    validation_data=(validation_X, validation_y_nums), \n",
    "    epochs=100, \n",
    "    batch_size=64\n",
    ")\n",
    "rnn_end = time.time()\n",
    "print(rnn_end - rnn_start, \"seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
