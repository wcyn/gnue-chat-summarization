{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Using Keras for Word Embedding as well as the LSTM\n",
    "\n",
    "https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470\n",
    "\n",
    "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "https://keras.io/getting-started/functional-api-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "import json\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from app.services.data_preparation import get_processed_data_sets_for_model, DATA_FILES_DIR\n",
    "from app.services.model_helper import TIME_OFFSET, ModelHelper\n",
    "from app.services import keras_lstm \n",
    "\n",
    "from app.services.sentence_tokenizer import get_chat_log_sequences_and_chat_logs, MAX_CHAT_LENGTH, TOP_WORDS\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Bidirectional, Dense, Dropout, Input, LSTM, TimeDistributed, LeakyReLU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "processed_data, data_type_log_ids, train_validation_and_test_dates = get_processed_data_sets_for_model(\n",
    "    include_word_sequences=True,\n",
    "    sequence_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3572</th>\n",
       "      <td>0.997990</td>\n",
       "      <td>0.158730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.208406</td>\n",
       "      <td>0.025535</td>\n",
       "      <td>0.057232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3573</th>\n",
       "      <td>0.998492</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016537</td>\n",
       "      <td>0.685186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3574</th>\n",
       "      <td>0.998995</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.233131</td>\n",
       "      <td>0.015752</td>\n",
       "      <td>0.259289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3575</th>\n",
       "      <td>0.999497</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.656124</td>\n",
       "      <td>0.034522</td>\n",
       "      <td>0.501867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3576</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.230761</td>\n",
       "      <td>0.030084</td>\n",
       "      <td>0.077018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      absolute_sentence_position  sentence_length  number_of_special_terms  \\\n",
       "3572                    0.997990         0.158730                      0.0   \n",
       "3573                    0.998492         0.015873                      0.0   \n",
       "3574                    0.998995         0.031746                      0.0   \n",
       "3575                    0.999497         0.015873                      0.0   \n",
       "3576                    1.000000         0.095238                      0.0   \n",
       "\n",
       "      sentiment_score  normalized_mean_tf_idf  normalized_mean_tf_isf  \n",
       "3572         0.208406                0.025535                0.057232  \n",
       "3573         0.000000                0.016537                0.685186  \n",
       "3574         0.233131                0.015752                0.259289  \n",
       "3575         0.656124                0.034522                0.501867  \n",
       "3576         0.230761                0.030084                0.077018  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data[\"validation_features_X\"].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       absolute_sentence_position  sentence_length  number_of_special_terms  \\\n",
      "13728                    0.992537         0.082192                 0.027027   \n",
      "13729                    0.994403         0.000000                 0.000000   \n",
      "13730                    0.996269         0.438356                 0.027027   \n",
      "13731                    0.998134         0.027397                 0.027027   \n",
      "13732                    1.000000         0.041096                 0.000000   \n",
      "\n",
      "       sentiment_score  normalized_mean_tf_idf  normalized_mean_tf_isf  \n",
      "13728         0.680659                0.007117                0.100894  \n",
      "13729         0.481478                0.000000                0.000000  \n",
      "13730         0.714451                0.006696                0.017713  \n",
      "13731         0.481478                0.004437                0.322041  \n",
      "13732         0.000000                0.004600                0.209270  \n",
      "(3405, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001182</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005692</td>\n",
       "      <td>0.368636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002364</td>\n",
       "      <td>0.084507</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007438</td>\n",
       "      <td>0.114519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006591</td>\n",
       "      <td>0.260825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004728</td>\n",
       "      <td>0.098592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011325</td>\n",
       "      <td>0.086599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005910</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160904</td>\n",
       "      <td>0.006633</td>\n",
       "      <td>0.665037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   absolute_sentence_position  sentence_length  number_of_special_terms  \\\n",
       "0                    0.001182         0.028169                 0.142857   \n",
       "1                    0.002364         0.084507                 0.285714   \n",
       "2                    0.003546         0.028169                 0.000000   \n",
       "3                    0.004728         0.098592                 0.000000   \n",
       "4                    0.005910         0.014085                 0.000000   \n",
       "\n",
       "   sentiment_score  normalized_mean_tf_idf  normalized_mean_tf_isf  \n",
       "0         0.000000                0.005692                0.368636  \n",
       "1         0.000000                0.007438                0.114519  \n",
       "2         0.000000                0.006591                0.260825  \n",
       "3         0.000000                0.011325                0.086599  \n",
       "4         0.160904                0.006633                0.665037  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_X = processed_data[\"train_features_X\"]\n",
    "validation_features_X = processed_data[\"validation_features_X\"]\n",
    "test_features_X = processed_data[\"test_features_X\"]\n",
    "print(train_features_X.tail())\n",
    "\n",
    "assert train_features_X.shape[1] == test_features_X.shape[1] \n",
    "assert test_features_X.shape[1] == validation_features_X.shape[1] \n",
    "print(test_features_X.shape)\n",
    "test_features_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13733, 100)\n",
      "(3405, 100)\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    4 1359   18  108   15    3 4900  918   12  115\n",
      "   786   14]]\n",
      "3405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hey reinhard\\n', 'did jonas ever hack on GNUe?\\n']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_X, train_chat_logs = processed_data[\"train_features_X\"]\n",
    "# validation_X, validation_chat_logs = get_chat_log_sequences_and_chat_logs(data_type_log_ids[\"validation\"])\n",
    "# test_X, test_chat_logs = get_chat_log_sequences_and_chat_logs(data_type_log_ids[\"test\"])\n",
    "\n",
    "train_sequences_X, train_chat_logs = (processed_data[\"train_sequences_X\"], processed_data[\"train_chat_logs\"])\n",
    "validation_sequences_X, validation_chat_logs = (processed_data[\"validation_sequences_X\"], \n",
    "                                                processed_data[\"validation_chat_logs\"])\n",
    "test_sequences_X, test_chat_logs = (processed_data[\"test_sequences_X\"], processed_data[\"test_chat_logs\"])\n",
    "print(train_sequences_X.shape)\n",
    "print(test_sequences_X.shape)\n",
    "print(train_sequences_X[:1])\n",
    "print(len(test_sequences_X))\n",
    "test_chat_logs[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3405, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = processed_data[\"train_y\"]\n",
    "validation_y = processed_data[\"validation_y\"]\n",
    "test_y = processed_data[\"test_y\"]\n",
    "print(test_y.shape)\n",
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[144080, 144081, 144082, 144083, 144084]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_type_log_ids[\"test\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_labels = [0 if list(label) == [1, 0] else 1 for label in train_y]\n",
    "train_y_labels[60: 80]\n",
    "np.unique(train_y_labels)\n",
    "# j = [1,0]\n",
    "# j == [1,0]\n",
    "# list(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_helper = ModelHelper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/wcyn/anaconda3/envs/gnue-irc/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Sentence input: meant to receive sequences of `max_chat_length` integers, between 1 and `top_words`.\n",
    "main_sentence_input = Input(shape=(MAX_CHAT_LENGTH,), dtype='int32', name='main_sentence_input')\n",
    "\n",
    "# This embedding layer will encode the input sequence\n",
    "# into a sequence of dense 512-dimensional vectors.\n",
    "x = Embedding(output_dim=512, input_dim=TOP_WORDS, input_length=MAX_CHAT_LENGTH)(main_sentence_input)\n",
    "\n",
    "# An LSTM will transform the vector sequence into a single vector,\n",
    "# containing information about the entire sequence\n",
    "lstm_out = LSTM(32)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.51756237, 14.73497854])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    np.unique(train_y_labels),\n",
    "    train_y_labels\n",
    ")\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "BATCH_SIZE = 64\n",
    "OPTIMIZER = \"adam\"\n",
    "OUTPUT_ACTIVATION = \"sigmoid\"\n",
    "# Here we insert the auxiliary loss, allowing the LSTM and Embedding layer \n",
    "# to be trained smoothly even though the main loss will be much higher in the model.\n",
    "auxiliary_output = Dense(2, activation=OUTPUT_ACTIVATION, name='aux_output')(lstm_out)\n",
    "\n",
    "# At this point, we feed into the model our auxiliary input data by \n",
    "# concatenating it with the LSTM output\n",
    "num_of_feature_columns = test_features_X.shape[1]\n",
    "sentence_features_input = Input(shape=(num_of_feature_columns,), name='sentence_features_input')\n",
    "merged_input_and_output = keras.layers.concatenate([lstm_out, sentence_features_input])\n",
    "\n",
    "# We stack a deep densely-connected network on top\n",
    "x = Dense(64, activation='relu')(merged_input_and_output)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# And finally we add the main logistic regression layer\n",
    "main_output = Dense(2, activation=OUTPUT_ACTIVATION, name='main_output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Define a model with two inputs and two outputs\n",
    "merged_model = Model(\n",
    "    inputs=[main_sentence_input, sentence_features_input], \n",
    "    outputs=[main_output, auxiliary_output]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EPOCHS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-20ab9904ef36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m checkpointer, checkpoints_dir = (\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_checkpointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoints_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EPOCHS' is not defined"
     ]
    }
   ],
   "source": [
    "checkpointer, checkpoints_dir = (\n",
    "    model_helper.fetch_checkpointer(EPOCHS, BATCH_SIZE, OPTIMIZER),\n",
    "    model_helper.checkpoints_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "We compile the model and assign a weight of 0.2 to the auxiliary loss. To specify different loss_weights or loss for each different output, you can use a list or a dictionary. Here we pass a single loss as the loss argument, so the same loss will be used on all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_sentence_input (InputLayer (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 512)     5120000     main_sentence_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 32)           69760       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sentence_features_input (InputL (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 38)           0           lstm_1[0][0]                     \n",
      "                                                                 sentence_features_input[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           2496        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           4160        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 64)           4160        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 2)            130         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "aux_output (Dense)              (None, 2)            66          lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5,200,772\n",
      "Trainable params: 5,200,772\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "merged_model.compile(\n",
    "    optimizer=OPTIMIZER, \n",
    "    #optimizer=\"adam\",\n",
    "    loss='categorical_crossentropy',\n",
    "    loss_weights=[.9, 0.7],\n",
    "    #sample_weight_mode=\"temporal\"\n",
    ")\n",
    "merged_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13733 samples, validate on 3577 samples\n",
      "Epoch 1/500\n",
      "13733/13733 [==============================] - 46s 3ms/step - loss: 0.0251 - main_output_loss: 0.0153 - aux_output_loss: 0.0162 - val_loss: 0.5367 - val_main_output_loss: 0.3882 - val_aux_output_loss: 0.2676\n",
      "Epoch 2/500\n",
      "13733/13733 [==============================] - 45s 3ms/step - loss: 0.0157 - main_output_loss: 0.0090 - aux_output_loss: 0.0109 - val_loss: 0.5271 - val_main_output_loss: 0.3773 - val_aux_output_loss: 0.2678\n",
      "Epoch 3/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0130 - main_output_loss: 0.0080 - aux_output_loss: 0.0083 - val_loss: 0.7509 - val_main_output_loss: 0.5900 - val_aux_output_loss: 0.3142\n",
      "Epoch 4/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0096 - main_output_loss: 0.0055 - aux_output_loss: 0.0066 - val_loss: 0.6541 - val_main_output_loss: 0.5112 - val_aux_output_loss: 0.2772\n",
      "\n",
      "Epoch 00004: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-04.hdf5\n",
      "Epoch 5/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0115 - main_output_loss: 0.0069 - aux_output_loss: 0.0075 - val_loss: 0.5986 - val_main_output_loss: 0.4425 - val_aux_output_loss: 0.2862\n",
      "Epoch 6/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0079 - main_output_loss: 0.0047 - aux_output_loss: 0.0054 - val_loss: 0.8236 - val_main_output_loss: 0.6541 - val_aux_output_loss: 0.3356\n",
      "Epoch 7/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0074 - main_output_loss: 0.0044 - aux_output_loss: 0.0050 - val_loss: 0.9177 - val_main_output_loss: 0.7399 - val_aux_output_loss: 0.3597\n",
      "Epoch 8/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0063 - main_output_loss: 0.0036 - aux_output_loss: 0.0044 - val_loss: 0.8455 - val_main_output_loss: 0.6608 - val_aux_output_loss: 0.3583\n",
      "Epoch 9/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0065 - main_output_loss: 0.0038 - aux_output_loss: 0.0044 - val_loss: 1.1053 - val_main_output_loss: 0.9251 - val_aux_output_loss: 0.3896\n",
      "Epoch 10/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0063 - main_output_loss: 0.0038 - aux_output_loss: 0.0041 - val_loss: 0.9760 - val_main_output_loss: 0.7949 - val_aux_output_loss: 0.3723\n",
      "Epoch 11/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0060 - main_output_loss: 0.0036 - aux_output_loss: 0.0039 - val_loss: 1.1042 - val_main_output_loss: 0.9306 - val_aux_output_loss: 0.3810\n",
      "Epoch 12/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0060 - main_output_loss: 0.0037 - aux_output_loss: 0.0039 - val_loss: 1.1440 - val_main_output_loss: 0.9498 - val_aux_output_loss: 0.4130\n",
      "Epoch 13/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0060 - main_output_loss: 0.0037 - aux_output_loss: 0.0038 - val_loss: 1.2754 - val_main_output_loss: 1.0700 - val_aux_output_loss: 0.4462\n",
      "Epoch 14/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0058 - main_output_loss: 0.0035 - aux_output_loss: 0.0038 - val_loss: 1.1133 - val_main_output_loss: 0.9116 - val_aux_output_loss: 0.4184\n",
      "\n",
      "Epoch 00014: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-14.hdf5\n",
      "Epoch 15/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0057 - main_output_loss: 0.0035 - aux_output_loss: 0.0037 - val_loss: 1.2227 - val_main_output_loss: 1.0201 - val_aux_output_loss: 0.4352\n",
      "Epoch 16/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0056 - main_output_loss: 0.0034 - aux_output_loss: 0.0036 - val_loss: 1.2231 - val_main_output_loss: 1.0184 - val_aux_output_loss: 0.4379\n",
      "Epoch 17/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0057 - main_output_loss: 0.0035 - aux_output_loss: 0.0036 - val_loss: 1.3268 - val_main_output_loss: 1.1221 - val_aux_output_loss: 0.4527\n",
      "Epoch 18/500\n",
      "13733/13733 [==============================] - 39s 3ms/step - loss: 0.0056 - main_output_loss: 0.0035 - aux_output_loss: 0.0036 - val_loss: 1.2366 - val_main_output_loss: 1.0312 - val_aux_output_loss: 0.4407\n",
      "Epoch 19/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0055 - main_output_loss: 0.0034 - aux_output_loss: 0.0036 - val_loss: 1.3062 - val_main_output_loss: 1.0966 - val_aux_output_loss: 0.4561\n",
      "Epoch 20/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0055 - main_output_loss: 0.0033 - aux_output_loss: 0.0036 - val_loss: 1.3789 - val_main_output_loss: 1.1640 - val_aux_output_loss: 0.4733\n",
      "Epoch 21/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0059 - main_output_loss: 0.0037 - aux_output_loss: 0.0037 - val_loss: 1.2615 - val_main_output_loss: 1.0374 - val_aux_output_loss: 0.4683\n",
      "Epoch 22/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0055 - main_output_loss: 0.0034 - aux_output_loss: 0.0035 - val_loss: 1.4101 - val_main_output_loss: 1.1935 - val_aux_output_loss: 0.4800\n",
      "Epoch 23/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0056 - main_output_loss: 0.0035 - aux_output_loss: 0.0035 - val_loss: 1.7218 - val_main_output_loss: 1.4404 - val_aux_output_loss: 0.6078\n",
      "Epoch 24/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0217 - main_output_loss: 0.0150 - aux_output_loss: 0.0117 - val_loss: 0.5608 - val_main_output_loss: 0.3359 - val_aux_output_loss: 0.3693\n",
      "\n",
      "Epoch 00024: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-24.hdf5\n",
      "Epoch 25/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0117 - main_output_loss: 0.0075 - aux_output_loss: 0.0071 - val_loss: 0.6795 - val_main_output_loss: 0.4237 - val_aux_output_loss: 0.4259\n",
      "Epoch 26/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0063 - main_output_loss: 0.0037 - aux_output_loss: 0.0043 - val_loss: 0.6904 - val_main_output_loss: 0.5112 - val_aux_output_loss: 0.3290\n",
      "Epoch 27/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0067 - main_output_loss: 0.0042 - aux_output_loss: 0.0041 - val_loss: 0.6390 - val_main_output_loss: 0.4307 - val_aux_output_loss: 0.3590\n",
      "Epoch 28/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0055 - main_output_loss: 0.0034 - aux_output_loss: 0.0036 - val_loss: 0.6831 - val_main_output_loss: 0.4866 - val_aux_output_loss: 0.3502\n",
      "Epoch 29/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0054 - main_output_loss: 0.0033 - aux_output_loss: 0.0035 - val_loss: 0.6973 - val_main_output_loss: 0.4953 - val_aux_output_loss: 0.3593\n",
      "Epoch 30/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0054 - main_output_loss: 0.0033 - aux_output_loss: 0.0034 - val_loss: 0.7253 - val_main_output_loss: 0.5168 - val_aux_output_loss: 0.3716\n",
      "Epoch 31/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0053 - main_output_loss: 0.0033 - aux_output_loss: 0.0034 - val_loss: 0.7386 - val_main_output_loss: 0.5258 - val_aux_output_loss: 0.3791\n",
      "Epoch 32/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0052 - main_output_loss: 0.0032 - aux_output_loss: 0.0033 - val_loss: 0.7193 - val_main_output_loss: 0.5001 - val_aux_output_loss: 0.3845\n",
      "Epoch 33/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0054 - main_output_loss: 0.0033 - aux_output_loss: 0.0034 - val_loss: 0.7524 - val_main_output_loss: 0.5274 - val_aux_output_loss: 0.3968\n",
      "Epoch 34/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0054 - main_output_loss: 0.0033 - aux_output_loss: 0.0034 - val_loss: 0.7641 - val_main_output_loss: 0.5378 - val_aux_output_loss: 0.4002\n",
      "\n",
      "Epoch 00034: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-34.hdf5\n",
      "Epoch 35/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0053 - main_output_loss: 0.0033 - aux_output_loss: 0.0034 - val_loss: 0.7889 - val_main_output_loss: 0.5580 - val_aux_output_loss: 0.4096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0054 - main_output_loss: 0.0034 - aux_output_loss: 0.0034 - val_loss: 0.7966 - val_main_output_loss: 0.5653 - val_aux_output_loss: 0.4113\n",
      "Epoch 37/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0054 - main_output_loss: 0.0033 - aux_output_loss: 0.0034 - val_loss: 0.8180 - val_main_output_loss: 0.5846 - val_aux_output_loss: 0.4169\n",
      "Epoch 38/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0053 - main_output_loss: 0.0033 - aux_output_loss: 0.0033 - val_loss: 0.8197 - val_main_output_loss: 0.5833 - val_aux_output_loss: 0.4210\n",
      "Epoch 39/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0052 - main_output_loss: 0.0032 - aux_output_loss: 0.0033 - val_loss: 0.8201 - val_main_output_loss: 0.5829 - val_aux_output_loss: 0.4222\n",
      "Epoch 40/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0054 - main_output_loss: 0.0033 - aux_output_loss: 0.0034 - val_loss: 0.8369 - val_main_output_loss: 0.5987 - val_aux_output_loss: 0.4258\n",
      "Epoch 41/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0053 - main_output_loss: 0.0033 - aux_output_loss: 0.0034 - val_loss: 0.8502 - val_main_output_loss: 0.6078 - val_aux_output_loss: 0.4332\n",
      "Epoch 42/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0052 - main_output_loss: 0.0032 - aux_output_loss: 0.0033 - val_loss: 0.8884 - val_main_output_loss: 0.6448 - val_aux_output_loss: 0.4401\n",
      "Epoch 43/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0053 - main_output_loss: 0.0033 - aux_output_loss: 0.0033 - val_loss: 0.8860 - val_main_output_loss: 0.6432 - val_aux_output_loss: 0.4388\n",
      "Epoch 44/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0052 - main_output_loss: 0.0033 - aux_output_loss: 0.0033 - val_loss: 0.9363 - val_main_output_loss: 0.6815 - val_aux_output_loss: 0.4613\n",
      "\n",
      "Epoch 00044: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-44.hdf5\n",
      "Epoch 45/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0053 - main_output_loss: 0.0033 - aux_output_loss: 0.0033 - val_loss: 0.8705 - val_main_output_loss: 0.6166 - val_aux_output_loss: 0.4508\n",
      "Epoch 46/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0052 - main_output_loss: 0.0032 - aux_output_loss: 0.0033 - val_loss: 0.9293 - val_main_output_loss: 0.6682 - val_aux_output_loss: 0.4685\n",
      "Epoch 47/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0052 - main_output_loss: 0.0033 - aux_output_loss: 0.0033 - val_loss: 0.8844 - val_main_output_loss: 0.6297 - val_aux_output_loss: 0.4538\n",
      "Epoch 48/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0055 - main_output_loss: 0.0034 - aux_output_loss: 0.0035 - val_loss: 0.9137 - val_main_output_loss: 0.6534 - val_aux_output_loss: 0.4652\n",
      "Epoch 49/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0051 - main_output_loss: 0.0032 - aux_output_loss: 0.0032 - val_loss: 0.8879 - val_main_output_loss: 0.6229 - val_aux_output_loss: 0.4675\n",
      "Epoch 50/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0053 - main_output_loss: 0.0033 - aux_output_loss: 0.0033 - val_loss: 0.9152 - val_main_output_loss: 0.6537 - val_aux_output_loss: 0.4670\n",
      "Epoch 51/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0053 - main_output_loss: 0.0033 - aux_output_loss: 0.0033 - val_loss: 0.9387 - val_main_output_loss: 0.6775 - val_aux_output_loss: 0.4700\n",
      "Epoch 52/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0051 - main_output_loss: 0.0032 - aux_output_loss: 0.0033 - val_loss: 0.9117 - val_main_output_loss: 0.6512 - val_aux_output_loss: 0.4651\n",
      "Epoch 53/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0054 - main_output_loss: 0.0034 - aux_output_loss: 0.0033 - val_loss: 0.9608 - val_main_output_loss: 0.6983 - val_aux_output_loss: 0.4748\n",
      "Epoch 54/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0052 - main_output_loss: 0.0032 - aux_output_loss: 0.0032 - val_loss: 0.9328 - val_main_output_loss: 0.6685 - val_aux_output_loss: 0.4730\n",
      "\n",
      "Epoch 00054: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-54.hdf5\n",
      "Epoch 55/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0052 - main_output_loss: 0.0033 - aux_output_loss: 0.0033 - val_loss: 0.9532 - val_main_output_loss: 0.6826 - val_aux_output_loss: 0.4842\n",
      "Epoch 56/500\n",
      "13733/13733 [==============================] - 38s 3ms/step - loss: 0.0052 - main_output_loss: 0.0033 - aux_output_loss: 0.0032 - val_loss: 1.0014 - val_main_output_loss: 0.7353 - val_aux_output_loss: 0.4852\n",
      "Epoch 57/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0052 - main_output_loss: 0.0032 - aux_output_loss: 0.0033 - val_loss: 1.0124 - val_main_output_loss: 0.7473 - val_aux_output_loss: 0.4856\n",
      "Epoch 58/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0051 - main_output_loss: 0.0032 - aux_output_loss: 0.0032 - val_loss: 1.0288 - val_main_output_loss: 0.7631 - val_aux_output_loss: 0.4886\n",
      "Epoch 59/500\n",
      "13733/13733 [==============================] - 39s 3ms/step - loss: 0.0051 - main_output_loss: 0.0032 - aux_output_loss: 0.0032 - val_loss: 1.0200 - val_main_output_loss: 0.7482 - val_aux_output_loss: 0.4952\n",
      "Epoch 60/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0051 - main_output_loss: 0.0032 - aux_output_loss: 0.0033 - val_loss: 1.0144 - val_main_output_loss: 0.7390 - val_aux_output_loss: 0.4990\n",
      "Epoch 61/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0051 - main_output_loss: 0.0032 - aux_output_loss: 0.0032 - val_loss: 0.9842 - val_main_output_loss: 0.7076 - val_aux_output_loss: 0.4961\n",
      "Epoch 62/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0051 - main_output_loss: 0.0032 - aux_output_loss: 0.0032 - val_loss: 1.0350 - val_main_output_loss: 0.7456 - val_aux_output_loss: 0.5199\n",
      "Epoch 63/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0051 - main_output_loss: 0.0032 - aux_output_loss: 0.0032 - val_loss: 1.0455 - val_main_output_loss: 0.7573 - val_aux_output_loss: 0.5198\n",
      "Epoch 64/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0051 - main_output_loss: 0.0031 - aux_output_loss: 0.0032 - val_loss: 1.0406 - val_main_output_loss: 0.7478 - val_aux_output_loss: 0.5250\n",
      "\n",
      "Epoch 00064: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-64.hdf5\n",
      "Epoch 65/500\n",
      "13733/13733 [==============================] - 39s 3ms/step - loss: 0.0051 - main_output_loss: 0.0032 - aux_output_loss: 0.0032 - val_loss: 1.0466 - val_main_output_loss: 0.7569 - val_aux_output_loss: 0.5220\n",
      "Epoch 66/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0050 - main_output_loss: 0.0031 - aux_output_loss: 0.0032 - val_loss: 1.0728 - val_main_output_loss: 0.7655 - val_aux_output_loss: 0.5483\n",
      "Epoch 67/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0051 - main_output_loss: 0.0032 - aux_output_loss: 0.0032 - val_loss: 1.0569 - val_main_output_loss: 0.7637 - val_aux_output_loss: 0.5279\n",
      "Epoch 68/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0050 - main_output_loss: 0.0031 - aux_output_loss: 0.0032 - val_loss: 1.0175 - val_main_output_loss: 0.7151 - val_aux_output_loss: 0.5342\n",
      "Epoch 69/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0051 - main_output_loss: 0.0032 - aux_output_loss: 0.0032 - val_loss: 1.0972 - val_main_output_loss: 0.7906 - val_aux_output_loss: 0.5511\n",
      "Epoch 70/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0052 - main_output_loss: 0.0033 - aux_output_loss: 0.0032 - val_loss: 1.0284 - val_main_output_loss: 0.7332 - val_aux_output_loss: 0.5264\n",
      "Epoch 71/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0052 - main_output_loss: 0.0033 - aux_output_loss: 0.0032 - val_loss: 1.0795 - val_main_output_loss: 0.7744 - val_aux_output_loss: 0.5465\n",
      "Epoch 72/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0050 - main_output_loss: 0.0031 - aux_output_loss: 0.0032 - val_loss: 1.0498 - val_main_output_loss: 0.7521 - val_aux_output_loss: 0.5328\n",
      "Epoch 73/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0051 - main_output_loss: 0.0032 - aux_output_loss: 0.0032 - val_loss: 1.0499 - val_main_output_loss: 0.7476 - val_aux_output_loss: 0.5387\n",
      "Epoch 74/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0050 - main_output_loss: 0.0031 - aux_output_loss: 0.0032 - val_loss: 1.0469 - val_main_output_loss: 0.7321 - val_aux_output_loss: 0.5543\n",
      "\n",
      "Epoch 00074: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-74.hdf5\n",
      "Epoch 75/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0050 - main_output_loss: 0.0031 - aux_output_loss: 0.0032 - val_loss: 1.1119 - val_main_output_loss: 0.8024 - val_aux_output_loss: 0.5568\n",
      "Epoch 76/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0050 - main_output_loss: 0.0031 - aux_output_loss: 0.0032 - val_loss: 1.1027 - val_main_output_loss: 0.7790 - val_aux_output_loss: 0.5738\n",
      "Epoch 77/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0050 - main_output_loss: 0.0031 - aux_output_loss: 0.0031 - val_loss: 1.1399 - val_main_output_loss: 0.8251 - val_aux_output_loss: 0.5676\n",
      "Epoch 78/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0050 - main_output_loss: 0.0030 - aux_output_loss: 0.0032 - val_loss: 1.0755 - val_main_output_loss: 0.7539 - val_aux_output_loss: 0.5672\n",
      "Epoch 79/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0049 - main_output_loss: 0.0030 - aux_output_loss: 0.0032 - val_loss: 1.0430 - val_main_output_loss: 0.7179 - val_aux_output_loss: 0.5671\n",
      "Epoch 80/500\n",
      "13733/13733 [==============================] - 39s 3ms/step - loss: 0.0050 - main_output_loss: 0.0031 - aux_output_loss: 0.0031 - val_loss: 0.9626 - val_main_output_loss: 0.6589 - val_aux_output_loss: 0.5280\n",
      "Epoch 81/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0050 - main_output_loss: 0.0031 - aux_output_loss: 0.0032 - val_loss: 1.2536 - val_main_output_loss: 0.9652 - val_aux_output_loss: 0.5500\n",
      "Epoch 82/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0164 - main_output_loss: 0.0121 - aux_output_loss: 0.0079 - val_loss: 0.7344 - val_main_output_loss: 0.4604 - val_aux_output_loss: 0.4571\n",
      "Epoch 83/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0147 - main_output_loss: 0.0094 - aux_output_loss: 0.0089 - val_loss: 0.5526 - val_main_output_loss: 0.3138 - val_aux_output_loss: 0.3859\n",
      "Epoch 84/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0080 - main_output_loss: 0.0051 - aux_output_loss: 0.0049 - val_loss: 0.7057 - val_main_output_loss: 0.4586 - val_aux_output_loss: 0.4185\n",
      "\n",
      "Epoch 00084: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-84.hdf5\n",
      "Epoch 85/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0056 - main_output_loss: 0.0035 - aux_output_loss: 0.0036 - val_loss: 0.7142 - val_main_output_loss: 0.4832 - val_aux_output_loss: 0.3989\n",
      "Epoch 86/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0050 - main_output_loss: 0.0030 - aux_output_loss: 0.0032 - val_loss: 0.7363 - val_main_output_loss: 0.5017 - val_aux_output_loss: 0.4068\n",
      "Epoch 87/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0049 - main_output_loss: 0.0030 - aux_output_loss: 0.0032 - val_loss: 0.7551 - val_main_output_loss: 0.5153 - val_aux_output_loss: 0.4162\n",
      "Epoch 88/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0032 - val_loss: 0.7679 - val_main_output_loss: 0.5237 - val_aux_output_loss: 0.4236\n",
      "Epoch 89/500\n",
      "13733/13733 [==============================] - 39s 3ms/step - loss: 0.0049 - main_output_loss: 0.0030 - aux_output_loss: 0.0032 - val_loss: 0.7935 - val_main_output_loss: 0.5455 - val_aux_output_loss: 0.4322\n",
      "Epoch 90/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0049 - main_output_loss: 0.0029 - aux_output_loss: 0.0032 - val_loss: 0.8088 - val_main_output_loss: 0.5565 - val_aux_output_loss: 0.4399\n",
      "Epoch 91/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0032 - val_loss: 0.8169 - val_main_output_loss: 0.5611 - val_aux_output_loss: 0.4456\n",
      "Epoch 92/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0049 - main_output_loss: 0.0030 - aux_output_loss: 0.0032 - val_loss: 0.8339 - val_main_output_loss: 0.5737 - val_aux_output_loss: 0.4537\n",
      "Epoch 93/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0032 - val_loss: 0.8444 - val_main_output_loss: 0.5815 - val_aux_output_loss: 0.4585\n",
      "Epoch 94/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0032 - val_loss: 0.8595 - val_main_output_loss: 0.5941 - val_aux_output_loss: 0.4641\n",
      "\n",
      "Epoch 00094: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-94.hdf5\n",
      "Epoch 95/500\n",
      "13733/13733 [==============================] - 38s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 0.8621 - val_main_output_loss: 0.5927 - val_aux_output_loss: 0.4695\n",
      "Epoch 96/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0049 - main_output_loss: 0.0029 - aux_output_loss: 0.0032 - val_loss: 0.8845 - val_main_output_loss: 0.6116 - val_aux_output_loss: 0.4771\n",
      "Epoch 97/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0032 - val_loss: 0.8896 - val_main_output_loss: 0.6142 - val_aux_output_loss: 0.4812\n",
      "Epoch 98/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0031 - val_loss: 0.9001 - val_main_output_loss: 0.6215 - val_aux_output_loss: 0.4869\n",
      "Epoch 99/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0032 - val_loss: 0.9108 - val_main_output_loss: 0.6322 - val_aux_output_loss: 0.4883\n",
      "Epoch 100/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0032 - val_loss: 0.9128 - val_main_output_loss: 0.6313 - val_aux_output_loss: 0.4923\n",
      "Epoch 101/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0031 - val_loss: 0.9282 - val_main_output_loss: 0.6439 - val_aux_output_loss: 0.4981\n",
      "Epoch 102/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0049 - main_output_loss: 0.0029 - aux_output_loss: 0.0032 - val_loss: 0.9400 - val_main_output_loss: 0.6485 - val_aux_output_loss: 0.5090\n",
      "Epoch 103/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0031 - val_loss: 0.9430 - val_main_output_loss: 0.6505 - val_aux_output_loss: 0.5108\n",
      "Epoch 104/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0032 - val_loss: 0.9670 - val_main_output_loss: 0.6701 - val_aux_output_loss: 0.5200\n",
      "\n",
      "Epoch 00104: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-104.hdf5\n",
      "Epoch 105/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0031 - val_loss: 0.9708 - val_main_output_loss: 0.6719 - val_aux_output_loss: 0.5230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 0.9755 - val_main_output_loss: 0.6733 - val_aux_output_loss: 0.5278\n",
      "Epoch 107/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0031 - val_loss: 0.9810 - val_main_output_loss: 0.6802 - val_aux_output_loss: 0.5269\n",
      "Epoch 108/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0032 - val_loss: 0.9974 - val_main_output_loss: 0.6954 - val_aux_output_loss: 0.5308\n",
      "Epoch 109/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0049 - main_output_loss: 0.0030 - aux_output_loss: 0.0032 - val_loss: 1.0264 - val_main_output_loss: 0.7147 - val_aux_output_loss: 0.5474\n",
      "Epoch 110/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0032 - val_loss: 1.0275 - val_main_output_loss: 0.7190 - val_aux_output_loss: 0.5434\n",
      "Epoch 111/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0048 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 1.0453 - val_main_output_loss: 0.7355 - val_aux_output_loss: 0.5476\n",
      "Epoch 112/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0031 - val_loss: 1.0390 - val_main_output_loss: 0.7272 - val_aux_output_loss: 0.5494\n",
      "Epoch 113/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0049 - main_output_loss: 0.0030 - aux_output_loss: 0.0032 - val_loss: 1.0608 - val_main_output_loss: 0.7445 - val_aux_output_loss: 0.5582\n",
      "Epoch 114/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0050 - main_output_loss: 0.0031 - aux_output_loss: 0.0031 - val_loss: 1.0483 - val_main_output_loss: 0.7259 - val_aux_output_loss: 0.5643\n",
      "\n",
      "Epoch 00114: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-114.hdf5\n",
      "Epoch 115/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0050 - main_output_loss: 0.0030 - aux_output_loss: 0.0033 - val_loss: 1.0485 - val_main_output_loss: 0.7284 - val_aux_output_loss: 0.5613\n",
      "Epoch 116/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0031 - val_loss: 1.0441 - val_main_output_loss: 0.7290 - val_aux_output_loss: 0.5544\n",
      "Epoch 117/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0032 - val_loss: 1.0825 - val_main_output_loss: 0.7592 - val_aux_output_loss: 0.5704\n",
      "Epoch 118/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0031 - val_loss: 1.1008 - val_main_output_loss: 0.7798 - val_aux_output_loss: 0.5700\n",
      "Epoch 119/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0031 - val_loss: 1.0914 - val_main_output_loss: 0.7690 - val_aux_output_loss: 0.5704\n",
      "Epoch 120/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0031 - val_loss: 1.1078 - val_main_output_loss: 0.7850 - val_aux_output_loss: 0.5732\n",
      "Epoch 121/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0049 - main_output_loss: 0.0030 - aux_output_loss: 0.0032 - val_loss: 1.1536 - val_main_output_loss: 0.8198 - val_aux_output_loss: 0.5939\n",
      "Epoch 122/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0031 - val_loss: 1.1268 - val_main_output_loss: 0.7890 - val_aux_output_loss: 0.5953\n",
      "Epoch 123/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0031 - val_loss: 1.1505 - val_main_output_loss: 0.7974 - val_aux_output_loss: 0.6185\n",
      "Epoch 124/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0048 - main_output_loss: 0.0028 - aux_output_loss: 0.0032 - val_loss: 1.0887 - val_main_output_loss: 0.7639 - val_aux_output_loss: 0.5732\n",
      "\n",
      "Epoch 00124: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-124.hdf5\n",
      "Epoch 125/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 1.0983 - val_main_output_loss: 0.7720 - val_aux_output_loss: 0.5764\n",
      "Epoch 126/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 1.0563 - val_main_output_loss: 0.7271 - val_aux_output_loss: 0.5742\n",
      "Epoch 127/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0047 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.0667 - val_main_output_loss: 0.7322 - val_aux_output_loss: 0.5825\n",
      "Epoch 128/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 1.0586 - val_main_output_loss: 0.7227 - val_aux_output_loss: 0.5831\n",
      "Epoch 129/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0048 - main_output_loss: 0.0028 - aux_output_loss: 0.0032 - val_loss: 1.1330 - val_main_output_loss: 0.7955 - val_aux_output_loss: 0.5958\n",
      "Epoch 130/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 1.0917 - val_main_output_loss: 0.7535 - val_aux_output_loss: 0.5908\n",
      "Epoch 131/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 1.0954 - val_main_output_loss: 0.7488 - val_aux_output_loss: 0.6021\n",
      "Epoch 132/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.0748 - val_main_output_loss: 0.7219 - val_aux_output_loss: 0.6073\n",
      "Epoch 133/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 1.1118 - val_main_output_loss: 0.7582 - val_aux_output_loss: 0.6134\n",
      "Epoch 134/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.0827 - val_main_output_loss: 0.7231 - val_aux_output_loss: 0.6171\n",
      "\n",
      "Epoch 00134: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-134.hdf5\n",
      "Epoch 135/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 1.2009 - val_main_output_loss: 0.8420 - val_aux_output_loss: 0.6330\n",
      "Epoch 136/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0047 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.2181 - val_main_output_loss: 0.8535 - val_aux_output_loss: 0.6427\n",
      "Epoch 137/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1226 - val_main_output_loss: 0.7664 - val_aux_output_loss: 0.6184\n",
      "Epoch 138/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 1.1247 - val_main_output_loss: 0.7686 - val_aux_output_loss: 0.6186\n",
      "Epoch 139/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.1235 - val_main_output_loss: 0.7560 - val_aux_output_loss: 0.6330\n",
      "Epoch 140/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.0596 - val_main_output_loss: 0.7173 - val_aux_output_loss: 0.5915\n",
      "Epoch 141/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0031 - val_loss: 1.1764 - val_main_output_loss: 0.8038 - val_aux_output_loss: 0.6471\n",
      "Epoch 142/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0032 - val_loss: 1.0511 - val_main_output_loss: 0.6924 - val_aux_output_loss: 0.6114\n",
      "Epoch 143/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0085 - main_output_loss: 0.0066 - aux_output_loss: 0.0036 - val_loss: 0.7039 - val_main_output_loss: 0.4798 - val_aux_output_loss: 0.3886\n",
      "Epoch 144/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0180 - main_output_loss: 0.0115 - aux_output_loss: 0.0109 - val_loss: 0.8750 - val_main_output_loss: 0.5084 - val_aux_output_loss: 0.5964\n",
      "\n",
      "Epoch 00144: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-144.hdf5\n",
      "Epoch 145/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0082 - main_output_loss: 0.0053 - aux_output_loss: 0.0049 - val_loss: 0.7721 - val_main_output_loss: 0.4508 - val_aux_output_loss: 0.5234\n",
      "Epoch 146/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0048 - main_output_loss: 0.0028 - aux_output_loss: 0.0032 - val_loss: 0.9268 - val_main_output_loss: 0.5934 - val_aux_output_loss: 0.5610\n",
      "Epoch 147/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0047 - main_output_loss: 0.0027 - aux_output_loss: 0.0032 - val_loss: 0.9370 - val_main_output_loss: 0.6122 - val_aux_output_loss: 0.5515\n",
      "Epoch 148/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 0.9549 - val_main_output_loss: 0.6350 - val_aux_output_loss: 0.5477\n",
      "Epoch 149/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 0.9670 - val_main_output_loss: 0.6489 - val_aux_output_loss: 0.5472\n",
      "Epoch 150/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 0.9786 - val_main_output_loss: 0.6635 - val_aux_output_loss: 0.5450\n",
      "Epoch 151/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 0.9901 - val_main_output_loss: 0.6755 - val_aux_output_loss: 0.5459\n",
      "Epoch 152/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.0025 - val_main_output_loss: 0.6884 - val_aux_output_loss: 0.5471\n",
      "Epoch 153/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.0140 - val_main_output_loss: 0.7015 - val_aux_output_loss: 0.5467\n",
      "Epoch 154/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.0228 - val_main_output_loss: 0.7116 - val_aux_output_loss: 0.5461\n",
      "\n",
      "Epoch 00154: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-154.hdf5\n",
      "Epoch 155/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0047 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.0303 - val_main_output_loss: 0.7205 - val_aux_output_loss: 0.5455\n",
      "Epoch 156/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.0443 - val_main_output_loss: 0.7341 - val_aux_output_loss: 0.5480\n",
      "Epoch 157/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.0426 - val_main_output_loss: 0.7340 - val_aux_output_loss: 0.5457\n",
      "Epoch 158/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.0508 - val_main_output_loss: 0.7423 - val_aux_output_loss: 0.5467\n",
      "Epoch 159/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.0602 - val_main_output_loss: 0.7523 - val_aux_output_loss: 0.5473\n",
      "Epoch 160/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.0555 - val_main_output_loss: 0.7492 - val_aux_output_loss: 0.5447\n",
      "Epoch 161/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.0836 - val_main_output_loss: 0.7741 - val_aux_output_loss: 0.5528\n",
      "Epoch 162/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.0938 - val_main_output_loss: 0.7852 - val_aux_output_loss: 0.5531\n",
      "Epoch 163/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.0858 - val_main_output_loss: 0.7747 - val_aux_output_loss: 0.5550\n",
      "Epoch 164/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.0819 - val_main_output_loss: 0.7715 - val_aux_output_loss: 0.5536\n",
      "\n",
      "Epoch 00164: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-164.hdf5\n",
      "Epoch 165/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.0907 - val_main_output_loss: 0.7794 - val_aux_output_loss: 0.5560\n",
      "Epoch 166/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.0870 - val_main_output_loss: 0.7757 - val_aux_output_loss: 0.5555\n",
      "Epoch 167/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.1007 - val_main_output_loss: 0.7904 - val_aux_output_loss: 0.5562\n",
      "Epoch 168/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1322 - val_main_output_loss: 0.8222 - val_aux_output_loss: 0.5602\n",
      "Epoch 169/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1126 - val_main_output_loss: 0.8006 - val_aux_output_loss: 0.5601\n",
      "Epoch 170/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.1360 - val_main_output_loss: 0.8187 - val_aux_output_loss: 0.5702\n",
      "Epoch 171/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0032 - val_loss: 1.1332 - val_main_output_loss: 0.8205 - val_aux_output_loss: 0.5639\n",
      "Epoch 172/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.1345 - val_main_output_loss: 0.8223 - val_aux_output_loss: 0.5635\n",
      "Epoch 173/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.1252 - val_main_output_loss: 0.8150 - val_aux_output_loss: 0.5596\n",
      "Epoch 174/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.1384 - val_main_output_loss: 0.8272 - val_aux_output_loss: 0.5627\n",
      "\n",
      "Epoch 00174: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-174.hdf5\n",
      "Epoch 175/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1250 - val_main_output_loss: 0.8108 - val_aux_output_loss: 0.5647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 1.1750 - val_main_output_loss: 0.8651 - val_aux_output_loss: 0.5663\n",
      "Epoch 177/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0049 - main_output_loss: 0.0030 - aux_output_loss: 0.0032 - val_loss: 1.1381 - val_main_output_loss: 0.8369 - val_aux_output_loss: 0.5498\n",
      "Epoch 178/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0032 - val_loss: 1.1419 - val_main_output_loss: 0.8374 - val_aux_output_loss: 0.5546\n",
      "Epoch 179/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0032 - val_loss: 1.1564 - val_main_output_loss: 0.8501 - val_aux_output_loss: 0.5590\n",
      "Epoch 180/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0032 - val_loss: 1.1796 - val_main_output_loss: 0.8652 - val_aux_output_loss: 0.5727\n",
      "Epoch 181/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.2090 - val_main_output_loss: 0.8993 - val_aux_output_loss: 0.5708\n",
      "Epoch 182/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0032 - val_loss: 1.2052 - val_main_output_loss: 0.8911 - val_aux_output_loss: 0.5760\n",
      "Epoch 183/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.2021 - val_main_output_loss: 0.8869 - val_aux_output_loss: 0.5770\n",
      "Epoch 184/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2339 - val_main_output_loss: 0.9141 - val_aux_output_loss: 0.5875\n",
      "\n",
      "Epoch 00184: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-184.hdf5\n",
      "Epoch 185/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2433 - val_main_output_loss: 0.9257 - val_aux_output_loss: 0.5858\n",
      "Epoch 186/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2694 - val_main_output_loss: 0.9538 - val_aux_output_loss: 0.5872\n",
      "Epoch 187/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.2966 - val_main_output_loss: 0.9750 - val_aux_output_loss: 0.5987\n",
      "Epoch 188/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.2567 - val_main_output_loss: 0.9366 - val_aux_output_loss: 0.5911\n",
      "Epoch 189/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0047 - main_output_loss: 0.0027 - aux_output_loss: 0.0032 - val_loss: 1.2769 - val_main_output_loss: 0.9544 - val_aux_output_loss: 0.5970\n",
      "Epoch 190/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 1.2912 - val_main_output_loss: 0.9694 - val_aux_output_loss: 0.5981\n",
      "Epoch 191/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0049 - main_output_loss: 0.0029 - aux_output_loss: 0.0032 - val_loss: 1.3685 - val_main_output_loss: 1.0349 - val_aux_output_loss: 0.6245\n",
      "Epoch 192/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.3196 - val_main_output_loss: 0.9903 - val_aux_output_loss: 0.6118\n",
      "Epoch 193/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.3252 - val_main_output_loss: 0.9967 - val_aux_output_loss: 0.6117\n",
      "Epoch 194/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.3115 - val_main_output_loss: 0.9767 - val_aux_output_loss: 0.6179\n",
      "\n",
      "Epoch 00194: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-194.hdf5\n",
      "Epoch 195/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 1.3064 - val_main_output_loss: 0.9761 - val_aux_output_loss: 0.6112\n",
      "Epoch 196/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0032 - val_loss: 1.3317 - val_main_output_loss: 0.9905 - val_aux_output_loss: 0.6289\n",
      "Epoch 197/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.3330 - val_main_output_loss: 0.9887 - val_aux_output_loss: 0.6331\n",
      "Epoch 198/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2622 - val_main_output_loss: 0.9211 - val_aux_output_loss: 0.6189\n",
      "Epoch 199/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0047 - main_output_loss: 0.0027 - aux_output_loss: 0.0032 - val_loss: 1.3445 - val_main_output_loss: 0.9906 - val_aux_output_loss: 0.6471\n",
      "Epoch 200/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.2541 - val_main_output_loss: 0.9038 - val_aux_output_loss: 0.6295\n",
      "Epoch 201/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.3033 - val_main_output_loss: 0.9468 - val_aux_output_loss: 0.6445\n",
      "Epoch 202/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2528 - val_main_output_loss: 0.8990 - val_aux_output_loss: 0.6338\n",
      "Epoch 203/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.3028 - val_main_output_loss: 0.9390 - val_aux_output_loss: 0.6539\n",
      "Epoch 204/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2810 - val_main_output_loss: 0.9209 - val_aux_output_loss: 0.6460\n",
      "\n",
      "Epoch 00204: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-204.hdf5\n",
      "Epoch 205/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.3234 - val_main_output_loss: 0.9609 - val_aux_output_loss: 0.6551\n",
      "Epoch 206/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2665 - val_main_output_loss: 0.9025 - val_aux_output_loss: 0.6489\n",
      "Epoch 207/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.2352 - val_main_output_loss: 0.8723 - val_aux_output_loss: 0.6431\n",
      "Epoch 208/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0052 - main_output_loss: 0.0034 - aux_output_loss: 0.0032 - val_loss: 1.2866 - val_main_output_loss: 0.9141 - val_aux_output_loss: 0.6626\n",
      "Epoch 209/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0052 - main_output_loss: 0.0032 - aux_output_loss: 0.0033 - val_loss: 1.3098 - val_main_output_loss: 0.9310 - val_aux_output_loss: 0.6741\n",
      "Epoch 210/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0047 - main_output_loss: 0.0027 - aux_output_loss: 0.0032 - val_loss: 1.2758 - val_main_output_loss: 0.8809 - val_aux_output_loss: 0.6900\n",
      "Epoch 211/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0047 - main_output_loss: 0.0027 - aux_output_loss: 0.0032 - val_loss: 1.2636 - val_main_output_loss: 0.8669 - val_aux_output_loss: 0.6906\n",
      "Epoch 212/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.2565 - val_main_output_loss: 0.8655 - val_aux_output_loss: 0.6823\n",
      "Epoch 213/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0051 - main_output_loss: 0.0032 - aux_output_loss: 0.0031 - val_loss: 1.2803 - val_main_output_loss: 0.8846 - val_aux_output_loss: 0.6917\n",
      "Epoch 214/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0049 - main_output_loss: 0.0030 - aux_output_loss: 0.0031 - val_loss: 1.2242 - val_main_output_loss: 0.8380 - val_aux_output_loss: 0.6715\n",
      "\n",
      "Epoch 00214: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-214.hdf5\n",
      "Epoch 215/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0070 - main_output_loss: 0.0051 - aux_output_loss: 0.0034 - val_loss: 1.2304 - val_main_output_loss: 0.7344 - val_aux_output_loss: 0.8136\n",
      "Epoch 216/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0204 - main_output_loss: 0.0126 - aux_output_loss: 0.0129 - val_loss: 0.6934 - val_main_output_loss: 0.4076 - val_aux_output_loss: 0.4664\n",
      "Epoch 217/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0095 - main_output_loss: 0.0057 - aux_output_loss: 0.0062 - val_loss: 0.6431 - val_main_output_loss: 0.3938 - val_aux_output_loss: 0.4125\n",
      "Epoch 218/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0079 - main_output_loss: 0.0050 - aux_output_loss: 0.0049 - val_loss: 0.6678 - val_main_output_loss: 0.4230 - val_aux_output_loss: 0.4102\n",
      "Epoch 219/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0049 - main_output_loss: 0.0029 - aux_output_loss: 0.0032 - val_loss: 0.7785 - val_main_output_loss: 0.5269 - val_aux_output_loss: 0.4347\n",
      "Epoch 220/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0032 - val_loss: 0.8084 - val_main_output_loss: 0.5576 - val_aux_output_loss: 0.4379\n",
      "Epoch 221/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0047 - main_output_loss: 0.0027 - aux_output_loss: 0.0032 - val_loss: 0.8332 - val_main_output_loss: 0.5833 - val_aux_output_loss: 0.4403\n",
      "Epoch 222/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0032 - val_loss: 0.8510 - val_main_output_loss: 0.6014 - val_aux_output_loss: 0.4425\n",
      "Epoch 223/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 0.8300 - val_main_output_loss: 0.5780 - val_aux_output_loss: 0.4426\n",
      "Epoch 224/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 0.8567 - val_main_output_loss: 0.6055 - val_aux_output_loss: 0.4454\n",
      "\n",
      "Epoch 00224: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-224.hdf5\n",
      "Epoch 225/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 0.8744 - val_main_output_loss: 0.6214 - val_aux_output_loss: 0.4502\n",
      "Epoch 226/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 0.8933 - val_main_output_loss: 0.6396 - val_aux_output_loss: 0.4538\n",
      "Epoch 227/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 0.9059 - val_main_output_loss: 0.6509 - val_aux_output_loss: 0.4572\n",
      "Epoch 228/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 0.9167 - val_main_output_loss: 0.6599 - val_aux_output_loss: 0.4611\n",
      "Epoch 229/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 0.9307 - val_main_output_loss: 0.6736 - val_aux_output_loss: 0.4635\n",
      "Epoch 230/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 0.9383 - val_main_output_loss: 0.6799 - val_aux_output_loss: 0.4663\n",
      "Epoch 231/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 0.9542 - val_main_output_loss: 0.6932 - val_aux_output_loss: 0.4719\n",
      "Epoch 232/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 0.9614 - val_main_output_loss: 0.6991 - val_aux_output_loss: 0.4746\n",
      "Epoch 233/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 0.9712 - val_main_output_loss: 0.7067 - val_aux_output_loss: 0.4788\n",
      "Epoch 234/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 0.9754 - val_main_output_loss: 0.7115 - val_aux_output_loss: 0.4787\n",
      "\n",
      "Epoch 00234: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-234.hdf5\n",
      "Epoch 235/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 0.9889 - val_main_output_loss: 0.7241 - val_aux_output_loss: 0.4818\n",
      "Epoch 236/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 0.9941 - val_main_output_loss: 0.7262 - val_aux_output_loss: 0.4865\n",
      "Epoch 237/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 0.9998 - val_main_output_loss: 0.7307 - val_aux_output_loss: 0.4889\n",
      "Epoch 238/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.0078 - val_main_output_loss: 0.7383 - val_aux_output_loss: 0.4905\n",
      "Epoch 239/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.0175 - val_main_output_loss: 0.7455 - val_aux_output_loss: 0.4950\n",
      "Epoch 240/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.0272 - val_main_output_loss: 0.7530 - val_aux_output_loss: 0.4994\n",
      "Epoch 241/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.0346 - val_main_output_loss: 0.7594 - val_aux_output_loss: 0.5015\n",
      "Epoch 242/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.0297 - val_main_output_loss: 0.7525 - val_aux_output_loss: 0.5035\n",
      "Epoch 243/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.0504 - val_main_output_loss: 0.7717 - val_aux_output_loss: 0.5084\n",
      "Epoch 244/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.0613 - val_main_output_loss: 0.7810 - val_aux_output_loss: 0.5119\n",
      "\n",
      "Epoch 00244: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-244.hdf5\n",
      "Epoch 245/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.0634 - val_main_output_loss: 0.7827 - val_aux_output_loss: 0.5129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.0684 - val_main_output_loss: 0.7852 - val_aux_output_loss: 0.5168\n",
      "Epoch 247/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.0909 - val_main_output_loss: 0.8062 - val_aux_output_loss: 0.5219\n",
      "Epoch 248/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.1015 - val_main_output_loss: 0.8139 - val_aux_output_loss: 0.5272\n",
      "Epoch 249/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1053 - val_main_output_loss: 0.8168 - val_aux_output_loss: 0.5288\n",
      "Epoch 250/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.1225 - val_main_output_loss: 0.8324 - val_aux_output_loss: 0.5333\n",
      "Epoch 251/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1251 - val_main_output_loss: 0.8323 - val_aux_output_loss: 0.5372\n",
      "Epoch 252/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.1255 - val_main_output_loss: 0.8303 - val_aux_output_loss: 0.5403\n",
      "Epoch 253/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1736 - val_main_output_loss: 0.8788 - val_aux_output_loss: 0.5467\n",
      "Epoch 254/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1804 - val_main_output_loss: 0.8830 - val_aux_output_loss: 0.5509\n",
      "\n",
      "Epoch 00254: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-254.hdf5\n",
      "Epoch 255/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.1895 - val_main_output_loss: 0.8896 - val_aux_output_loss: 0.5556\n",
      "Epoch 256/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 1.1823 - val_main_output_loss: 0.8802 - val_aux_output_loss: 0.5574\n",
      "Epoch 257/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1760 - val_main_output_loss: 0.8730 - val_aux_output_loss: 0.5576\n",
      "Epoch 258/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1905 - val_main_output_loss: 0.8841 - val_aux_output_loss: 0.5640\n",
      "Epoch 259/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1847 - val_main_output_loss: 0.8761 - val_aux_output_loss: 0.5660\n",
      "Epoch 260/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2088 - val_main_output_loss: 0.8974 - val_aux_output_loss: 0.5730\n",
      "Epoch 261/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2075 - val_main_output_loss: 0.8922 - val_aux_output_loss: 0.5779\n",
      "Epoch 262/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.2438 - val_main_output_loss: 0.9182 - val_aux_output_loss: 0.5963\n",
      "Epoch 263/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0032 - val_loss: 1.2496 - val_main_output_loss: 0.9221 - val_aux_output_loss: 0.5996\n",
      "Epoch 264/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0049 - main_output_loss: 0.0031 - aux_output_loss: 0.0031 - val_loss: 1.1944 - val_main_output_loss: 0.8712 - val_aux_output_loss: 0.5861\n",
      "\n",
      "Epoch 00264: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-264.hdf5\n",
      "Epoch 265/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1984 - val_main_output_loss: 0.8768 - val_aux_output_loss: 0.5847\n",
      "Epoch 266/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2017 - val_main_output_loss: 0.8760 - val_aux_output_loss: 0.5904\n",
      "Epoch 267/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1922 - val_main_output_loss: 0.8617 - val_aux_output_loss: 0.5953\n",
      "Epoch 268/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1865 - val_main_output_loss: 0.8537 - val_aux_output_loss: 0.5974\n",
      "Epoch 269/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2163 - val_main_output_loss: 0.8794 - val_aux_output_loss: 0.6069\n",
      "Epoch 270/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0046 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 1.2237 - val_main_output_loss: 0.8869 - val_aux_output_loss: 0.6079\n",
      "Epoch 271/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2126 - val_main_output_loss: 0.8708 - val_aux_output_loss: 0.6127\n",
      "Epoch 272/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2077 - val_main_output_loss: 0.8625 - val_aux_output_loss: 0.6164\n",
      "Epoch 273/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2300 - val_main_output_loss: 0.8811 - val_aux_output_loss: 0.6242\n",
      "Epoch 274/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2557 - val_main_output_loss: 0.9060 - val_aux_output_loss: 0.6290\n",
      "\n",
      "Epoch 00274: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-274.hdf5\n",
      "Epoch 275/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.2556 - val_main_output_loss: 0.9003 - val_aux_output_loss: 0.6361\n",
      "Epoch 276/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0032 - val_loss: 1.2083 - val_main_output_loss: 0.8485 - val_aux_output_loss: 0.6353\n",
      "Epoch 277/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0049 - main_output_loss: 0.0028 - aux_output_loss: 0.0033 - val_loss: 2.2296 - val_main_output_loss: 1.7816 - val_aux_output_loss: 0.8945\n",
      "Epoch 278/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0095 - main_output_loss: 0.0061 - aux_output_loss: 0.0058 - val_loss: 0.9066 - val_main_output_loss: 0.5749 - val_aux_output_loss: 0.5560\n",
      "Epoch 279/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0064 - main_output_loss: 0.0039 - aux_output_loss: 0.0042 - val_loss: 1.0304 - val_main_output_loss: 0.7190 - val_aux_output_loss: 0.5475\n",
      "Epoch 280/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0052 - main_output_loss: 0.0029 - aux_output_loss: 0.0036 - val_loss: 1.1593 - val_main_output_loss: 0.8154 - val_aux_output_loss: 0.6077\n",
      "Epoch 281/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0048 - main_output_loss: 0.0028 - aux_output_loss: 0.0033 - val_loss: 1.1017 - val_main_output_loss: 0.8061 - val_aux_output_loss: 0.5374\n",
      "Epoch 282/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1824 - val_main_output_loss: 0.8889 - val_aux_output_loss: 0.5462\n",
      "Epoch 283/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1802 - val_main_output_loss: 0.8873 - val_aux_output_loss: 0.5452\n",
      "Epoch 284/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1811 - val_main_output_loss: 0.8877 - val_aux_output_loss: 0.5460\n",
      "\n",
      "Epoch 00284: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-284.hdf5\n",
      "Epoch 285/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1829 - val_main_output_loss: 0.8887 - val_aux_output_loss: 0.5471\n",
      "Epoch 286/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1855 - val_main_output_loss: 0.8905 - val_aux_output_loss: 0.5486\n",
      "Epoch 287/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.1786 - val_main_output_loss: 0.8848 - val_aux_output_loss: 0.5461\n",
      "Epoch 288/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.1811 - val_main_output_loss: 0.8861 - val_aux_output_loss: 0.5480\n",
      "Epoch 289/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1834 - val_main_output_loss: 0.8873 - val_aux_output_loss: 0.5496\n",
      "Epoch 290/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0032 - val_loss: 1.1938 - val_main_output_loss: 0.8950 - val_aux_output_loss: 0.5546\n",
      "Epoch 291/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 1.1709 - val_main_output_loss: 0.8745 - val_aux_output_loss: 0.5483\n",
      "Epoch 292/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1765 - val_main_output_loss: 0.8798 - val_aux_output_loss: 0.5496\n",
      "Epoch 293/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1847 - val_main_output_loss: 0.8874 - val_aux_output_loss: 0.5515\n",
      "Epoch 294/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1886 - val_main_output_loss: 0.8910 - val_aux_output_loss: 0.5525\n",
      "\n",
      "Epoch 00294: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-294.hdf5\n",
      "Epoch 295/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1851 - val_main_output_loss: 0.8851 - val_aux_output_loss: 0.5550\n",
      "Epoch 296/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1922 - val_main_output_loss: 0.8916 - val_aux_output_loss: 0.5569\n",
      "Epoch 297/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1885 - val_main_output_loss: 0.8884 - val_aux_output_loss: 0.5556\n",
      "Epoch 298/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0047 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.1867 - val_main_output_loss: 0.8859 - val_aux_output_loss: 0.5564\n",
      "Epoch 299/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.2518 - val_main_output_loss: 0.9398 - val_aux_output_loss: 0.5800\n",
      "Epoch 300/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2199 - val_main_output_loss: 0.9122 - val_aux_output_loss: 0.5699\n",
      "Epoch 301/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2011 - val_main_output_loss: 0.8947 - val_aux_output_loss: 0.5656\n",
      "Epoch 302/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2092 - val_main_output_loss: 0.9018 - val_aux_output_loss: 0.5681\n",
      "Epoch 303/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.1959 - val_main_output_loss: 0.8912 - val_aux_output_loss: 0.5627\n",
      "Epoch 304/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1955 - val_main_output_loss: 0.8892 - val_aux_output_loss: 0.5647\n",
      "\n",
      "Epoch 00304: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-304.hdf5\n",
      "Epoch 305/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2095 - val_main_output_loss: 0.9029 - val_aux_output_loss: 0.5669\n",
      "Epoch 306/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2092 - val_main_output_loss: 0.9014 - val_aux_output_loss: 0.5685\n",
      "Epoch 307/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1985 - val_main_output_loss: 0.8921 - val_aux_output_loss: 0.5651\n",
      "Epoch 308/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1984 - val_main_output_loss: 0.8880 - val_aux_output_loss: 0.5702\n",
      "Epoch 309/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.1971 - val_main_output_loss: 0.8853 - val_aux_output_loss: 0.5719\n",
      "Epoch 310/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.2143 - val_main_output_loss: 0.8990 - val_aux_output_loss: 0.5789\n",
      "Epoch 311/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2048 - val_main_output_loss: 0.8898 - val_aux_output_loss: 0.5771\n",
      "Epoch 312/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2221 - val_main_output_loss: 0.9065 - val_aux_output_loss: 0.5804\n",
      "Epoch 313/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2543 - val_main_output_loss: 0.9306 - val_aux_output_loss: 0.5953\n",
      "Epoch 314/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2311 - val_main_output_loss: 0.9111 - val_aux_output_loss: 0.5873\n",
      "\n",
      "Epoch 00314: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-314.hdf5\n",
      "Epoch 315/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2278 - val_main_output_loss: 0.9043 - val_aux_output_loss: 0.5913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 316/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2439 - val_main_output_loss: 0.9186 - val_aux_output_loss: 0.5960\n",
      "Epoch 317/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2449 - val_main_output_loss: 0.9183 - val_aux_output_loss: 0.5978\n",
      "Epoch 318/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 1.2352 - val_main_output_loss: 0.9068 - val_aux_output_loss: 0.5987\n",
      "Epoch 319/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.2410 - val_main_output_loss: 0.9112 - val_aux_output_loss: 0.6013\n",
      "Epoch 320/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.2437 - val_main_output_loss: 0.9112 - val_aux_output_loss: 0.6051\n",
      "Epoch 321/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2493 - val_main_output_loss: 0.9136 - val_aux_output_loss: 0.6100\n",
      "Epoch 322/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2623 - val_main_output_loss: 0.9258 - val_aux_output_loss: 0.6129\n",
      "Epoch 323/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2467 - val_main_output_loss: 0.9065 - val_aux_output_loss: 0.6154\n",
      "Epoch 324/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2695 - val_main_output_loss: 0.9279 - val_aux_output_loss: 0.6206\n",
      "\n",
      "Epoch 00324: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-324.hdf5\n",
      "Epoch 325/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2711 - val_main_output_loss: 0.9258 - val_aux_output_loss: 0.6255\n",
      "Epoch 326/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2875 - val_main_output_loss: 0.9419 - val_aux_output_loss: 0.6282\n",
      "Epoch 327/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2826 - val_main_output_loss: 0.9338 - val_aux_output_loss: 0.6318\n",
      "Epoch 328/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2681 - val_main_output_loss: 0.9144 - val_aux_output_loss: 0.6359\n",
      "Epoch 329/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.2870 - val_main_output_loss: 0.9323 - val_aux_output_loss: 0.6400\n",
      "Epoch 330/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.2842 - val_main_output_loss: 0.9271 - val_aux_output_loss: 0.6425\n",
      "Epoch 331/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.3112 - val_main_output_loss: 0.9487 - val_aux_output_loss: 0.6534\n",
      "Epoch 332/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2963 - val_main_output_loss: 0.9296 - val_aux_output_loss: 0.6568\n",
      "Epoch 333/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.3063 - val_main_output_loss: 0.9378 - val_aux_output_loss: 0.6604\n",
      "Epoch 334/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.3064 - val_main_output_loss: 0.9353 - val_aux_output_loss: 0.6636\n",
      "\n",
      "Epoch 00334: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-334.hdf5\n",
      "Epoch 335/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.3088 - val_main_output_loss: 0.9353 - val_aux_output_loss: 0.6673\n",
      "Epoch 336/500\n",
      "13733/13733 [==============================] - 43s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.3112 - val_main_output_loss: 0.9356 - val_aux_output_loss: 0.6702\n",
      "Epoch 337/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.3304 - val_main_output_loss: 0.9545 - val_aux_output_loss: 0.6733\n",
      "Epoch 338/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.3216 - val_main_output_loss: 0.9429 - val_aux_output_loss: 0.6757\n",
      "Epoch 339/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.3385 - val_main_output_loss: 0.9586 - val_aux_output_loss: 0.6797\n",
      "Epoch 340/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.3520 - val_main_output_loss: 0.9701 - val_aux_output_loss: 0.6841\n",
      "Epoch 341/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.3577 - val_main_output_loss: 0.9742 - val_aux_output_loss: 0.6870\n",
      "Epoch 342/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.3529 - val_main_output_loss: 0.9649 - val_aux_output_loss: 0.6921\n",
      "Epoch 343/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.3725 - val_main_output_loss: 0.9840 - val_aux_output_loss: 0.6956\n",
      "Epoch 344/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.3477 - val_main_output_loss: 0.9567 - val_aux_output_loss: 0.6952\n",
      "\n",
      "Epoch 00344: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-344.hdf5\n",
      "Epoch 345/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0032 - val_loss: 1.3923 - val_main_output_loss: 0.9951 - val_aux_output_loss: 0.7096\n",
      "Epoch 346/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0065 - main_output_loss: 0.0041 - aux_output_loss: 0.0041 - val_loss: 1.5176 - val_main_output_loss: 1.2018 - val_aux_output_loss: 0.6228\n",
      "Epoch 347/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0135 - main_output_loss: 0.0093 - aux_output_loss: 0.0073 - val_loss: 0.9238 - val_main_output_loss: 0.6177 - val_aux_output_loss: 0.5254\n",
      "Epoch 348/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0075 - main_output_loss: 0.0051 - aux_output_loss: 0.0041 - val_loss: 0.8794 - val_main_output_loss: 0.5565 - val_aux_output_loss: 0.5408\n",
      "Epoch 349/500\n",
      "13733/13733 [==============================] - 43s 3ms/step - loss: 0.0065 - main_output_loss: 0.0041 - aux_output_loss: 0.0040 - val_loss: 0.7368 - val_main_output_loss: 0.4654 - val_aux_output_loss: 0.4542\n",
      "Epoch 350/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0054 - main_output_loss: 0.0032 - aux_output_loss: 0.0036 - val_loss: 0.6891 - val_main_output_loss: 0.4205 - val_aux_output_loss: 0.4437\n",
      "Epoch 351/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0047 - main_output_loss: 0.0027 - aux_output_loss: 0.0033 - val_loss: 0.7890 - val_main_output_loss: 0.4882 - val_aux_output_loss: 0.4994\n",
      "Epoch 352/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0032 - val_loss: 0.7957 - val_main_output_loss: 0.4969 - val_aux_output_loss: 0.4979\n",
      "Epoch 353/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0032 - val_loss: 0.8027 - val_main_output_loss: 0.5049 - val_aux_output_loss: 0.4977\n",
      "Epoch 354/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0032 - val_loss: 0.8142 - val_main_output_loss: 0.5166 - val_aux_output_loss: 0.4989\n",
      "\n",
      "Epoch 00354: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-354.hdf5\n",
      "Epoch 355/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0032 - val_loss: 0.8194 - val_main_output_loss: 0.5218 - val_aux_output_loss: 0.4996\n",
      "Epoch 356/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0032 - val_loss: 0.8232 - val_main_output_loss: 0.5255 - val_aux_output_loss: 0.5004\n",
      "Epoch 357/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0032 - val_loss: 0.8322 - val_main_output_loss: 0.5349 - val_aux_output_loss: 0.5012\n",
      "Epoch 358/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0032 - val_loss: 0.8346 - val_main_output_loss: 0.5376 - val_aux_output_loss: 0.5011\n",
      "Epoch 359/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0032 - val_loss: 0.8390 - val_main_output_loss: 0.5415 - val_aux_output_loss: 0.5024\n",
      "Epoch 360/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 0.8445 - val_main_output_loss: 0.5472 - val_aux_output_loss: 0.5028\n",
      "Epoch 361/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0032 - val_loss: 0.8477 - val_main_output_loss: 0.5491 - val_aux_output_loss: 0.5050\n",
      "Epoch 362/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 0.8597 - val_main_output_loss: 0.5619 - val_aux_output_loss: 0.5058\n",
      "Epoch 363/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 0.8685 - val_main_output_loss: 0.5701 - val_aux_output_loss: 0.5077\n",
      "Epoch 364/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 0.8670 - val_main_output_loss: 0.5673 - val_aux_output_loss: 0.5092\n",
      "\n",
      "Epoch 00364: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-364.hdf5\n",
      "Epoch 365/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 0.8749 - val_main_output_loss: 0.5728 - val_aux_output_loss: 0.5133\n",
      "Epoch 366/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 0.8693 - val_main_output_loss: 0.5659 - val_aux_output_loss: 0.5142\n",
      "Epoch 367/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 0.8768 - val_main_output_loss: 0.5739 - val_aux_output_loss: 0.5148\n",
      "Epoch 368/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 0.8848 - val_main_output_loss: 0.5822 - val_aux_output_loss: 0.5153\n",
      "Epoch 369/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 0.8956 - val_main_output_loss: 0.5930 - val_aux_output_loss: 0.5169\n",
      "Epoch 370/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 0.9032 - val_main_output_loss: 0.6000 - val_aux_output_loss: 0.5188\n",
      "Epoch 371/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 0.9074 - val_main_output_loss: 0.6042 - val_aux_output_loss: 0.5195\n",
      "Epoch 372/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 0.9080 - val_main_output_loss: 0.6037 - val_aux_output_loss: 0.5209\n",
      "Epoch 373/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 0.9130 - val_main_output_loss: 0.6085 - val_aux_output_loss: 0.5220\n",
      "Epoch 374/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 0.9260 - val_main_output_loss: 0.6221 - val_aux_output_loss: 0.5230\n",
      "\n",
      "Epoch 00374: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-374.hdf5\n",
      "Epoch 375/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 0.9173 - val_main_output_loss: 0.6112 - val_aux_output_loss: 0.5246\n",
      "Epoch 376/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 0.9196 - val_main_output_loss: 0.6130 - val_aux_output_loss: 0.5255\n",
      "Epoch 377/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 0.9416 - val_main_output_loss: 0.6360 - val_aux_output_loss: 0.5274\n",
      "Epoch 378/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 0.9410 - val_main_output_loss: 0.6339 - val_aux_output_loss: 0.5293\n",
      "Epoch 379/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0043 - main_output_loss: 0.0024 - aux_output_loss: 0.0031 - val_loss: 0.9566 - val_main_output_loss: 0.6500 - val_aux_output_loss: 0.5309\n",
      "Epoch 380/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 0.9696 - val_main_output_loss: 0.6625 - val_aux_output_loss: 0.5334\n",
      "Epoch 381/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 0.9369 - val_main_output_loss: 0.6317 - val_aux_output_loss: 0.5261\n",
      "Epoch 382/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 0.9415 - val_main_output_loss: 0.6364 - val_aux_output_loss: 0.5269\n",
      "Epoch 383/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 0.9403 - val_main_output_loss: 0.6327 - val_aux_output_loss: 0.5299\n",
      "Epoch 384/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 0.9570 - val_main_output_loss: 0.6499 - val_aux_output_loss: 0.5316\n",
      "\n",
      "Epoch 00384: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-384.hdf5\n",
      "Epoch 385/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 0.9744 - val_main_output_loss: 0.6672 - val_aux_output_loss: 0.5342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 386/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 0.9836 - val_main_output_loss: 0.6754 - val_aux_output_loss: 0.5367\n",
      "Epoch 387/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 0.9864 - val_main_output_loss: 0.6766 - val_aux_output_loss: 0.5392\n",
      "Epoch 388/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.0006 - val_main_output_loss: 0.6904 - val_aux_output_loss: 0.5418\n",
      "Epoch 389/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 0.9837 - val_main_output_loss: 0.6696 - val_aux_output_loss: 0.5444\n",
      "Epoch 390/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 0.9960 - val_main_output_loss: 0.6822 - val_aux_output_loss: 0.5457\n",
      "Epoch 391/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 0.9994 - val_main_output_loss: 0.6859 - val_aux_output_loss: 0.5458\n",
      "Epoch 392/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 0.9987 - val_main_output_loss: 0.6827 - val_aux_output_loss: 0.5490\n",
      "Epoch 393/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.0064 - val_main_output_loss: 0.6888 - val_aux_output_loss: 0.5521\n",
      "Epoch 394/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0032 - val_loss: 1.0251 - val_main_output_loss: 0.7080 - val_aux_output_loss: 0.5541\n",
      "\n",
      "Epoch 00394: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-394.hdf5\n",
      "Epoch 395/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.0118 - val_main_output_loss: 0.6891 - val_aux_output_loss: 0.5594\n",
      "Epoch 396/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.0377 - val_main_output_loss: 0.7155 - val_aux_output_loss: 0.5625\n",
      "Epoch 397/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.0569 - val_main_output_loss: 0.7262 - val_aux_output_loss: 0.5762\n",
      "Epoch 398/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.0433 - val_main_output_loss: 0.7102 - val_aux_output_loss: 0.5774\n",
      "Epoch 399/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.0651 - val_main_output_loss: 0.7321 - val_aux_output_loss: 0.5802\n",
      "Epoch 400/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.0920 - val_main_output_loss: 0.7598 - val_aux_output_loss: 0.5831\n",
      "Epoch 401/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1289 - val_main_output_loss: 0.7991 - val_aux_output_loss: 0.5853\n",
      "Epoch 402/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.0982 - val_main_output_loss: 0.7619 - val_aux_output_loss: 0.5892\n",
      "Epoch 403/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1008 - val_main_output_loss: 0.7620 - val_aux_output_loss: 0.5929\n",
      "Epoch 404/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1051 - val_main_output_loss: 0.7645 - val_aux_output_loss: 0.5957\n",
      "\n",
      "Epoch 00404: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-404.hdf5\n",
      "Epoch 405/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1104 - val_main_output_loss: 0.7691 - val_aux_output_loss: 0.5974\n",
      "Epoch 406/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.1465 - val_main_output_loss: 0.8062 - val_aux_output_loss: 0.6014\n",
      "Epoch 407/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1572 - val_main_output_loss: 0.8180 - val_aux_output_loss: 0.6014\n",
      "Epoch 408/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0044 - main_output_loss: 0.0024 - aux_output_loss: 0.0031 - val_loss: 1.1245 - val_main_output_loss: 0.7840 - val_aux_output_loss: 0.5984\n",
      "Epoch 409/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1325 - val_main_output_loss: 0.7939 - val_aux_output_loss: 0.5970\n",
      "Epoch 410/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0032 - val_loss: 1.1304 - val_main_output_loss: 0.7845 - val_aux_output_loss: 0.6061\n",
      "Epoch 411/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0031 - val_loss: 1.1726 - val_main_output_loss: 0.8264 - val_aux_output_loss: 0.6126\n",
      "Epoch 412/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1509 - val_main_output_loss: 0.7991 - val_aux_output_loss: 0.6166\n",
      "Epoch 413/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1582 - val_main_output_loss: 0.8041 - val_aux_output_loss: 0.6208\n",
      "Epoch 414/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1535 - val_main_output_loss: 0.7969 - val_aux_output_loss: 0.6232\n",
      "\n",
      "Epoch 00414: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-414.hdf5\n",
      "Epoch 415/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1851 - val_main_output_loss: 0.8286 - val_aux_output_loss: 0.6277\n",
      "Epoch 416/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1962 - val_main_output_loss: 0.8381 - val_aux_output_loss: 0.6313\n",
      "Epoch 417/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0043 - main_output_loss: 0.0024 - aux_output_loss: 0.0031 - val_loss: 1.2165 - val_main_output_loss: 0.8578 - val_aux_output_loss: 0.6349\n",
      "Epoch 418/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 1.1895 - val_main_output_loss: 0.8243 - val_aux_output_loss: 0.6395\n",
      "Epoch 419/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.1862 - val_main_output_loss: 0.8197 - val_aux_output_loss: 0.6406\n",
      "Epoch 420/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1821 - val_main_output_loss: 0.8129 - val_aux_output_loss: 0.6436\n",
      "Epoch 421/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1981 - val_main_output_loss: 0.8283 - val_aux_output_loss: 0.6466\n",
      "Epoch 422/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2113 - val_main_output_loss: 0.8417 - val_aux_output_loss: 0.6483\n",
      "Epoch 423/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2082 - val_main_output_loss: 0.8338 - val_aux_output_loss: 0.6541\n",
      "Epoch 424/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2358 - val_main_output_loss: 0.8602 - val_aux_output_loss: 0.6594\n",
      "\n",
      "Epoch 00424: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-424.hdf5\n",
      "Epoch 425/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2364 - val_main_output_loss: 0.8590 - val_aux_output_loss: 0.6618\n",
      "Epoch 426/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2551 - val_main_output_loss: 0.8767 - val_aux_output_loss: 0.6659\n",
      "Epoch 427/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0043 - main_output_loss: 0.0024 - aux_output_loss: 0.0031 - val_loss: 1.2609 - val_main_output_loss: 0.8816 - val_aux_output_loss: 0.6677\n",
      "Epoch 428/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2355 - val_main_output_loss: 0.8510 - val_aux_output_loss: 0.6709\n",
      "Epoch 429/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 1.2465 - val_main_output_loss: 0.8604 - val_aux_output_loss: 0.6745\n",
      "Epoch 430/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2591 - val_main_output_loss: 0.8716 - val_aux_output_loss: 0.6780\n",
      "Epoch 431/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0043 - main_output_loss: 0.0024 - aux_output_loss: 0.0031 - val_loss: 1.2639 - val_main_output_loss: 0.8762 - val_aux_output_loss: 0.6790\n",
      "Epoch 432/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0048 - main_output_loss: 0.0028 - aux_output_loss: 0.0032 - val_loss: 1.2898 - val_main_output_loss: 0.8961 - val_aux_output_loss: 0.6905\n",
      "Epoch 433/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0032 - val_loss: 1.3176 - val_main_output_loss: 0.9214 - val_aux_output_loss: 0.6977\n",
      "Epoch 434/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 1.1693 - val_main_output_loss: 0.7868 - val_aux_output_loss: 0.6589\n",
      "\n",
      "Epoch 00434: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-434.hdf5\n",
      "Epoch 435/500\n",
      "13733/13733 [==============================] - 39s 3ms/step - loss: 0.0049 - main_output_loss: 0.0029 - aux_output_loss: 0.0033 - val_loss: 1.1450 - val_main_output_loss: 0.7838 - val_aux_output_loss: 0.6280\n",
      "Epoch 436/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0032 - val_loss: 1.1617 - val_main_output_loss: 0.7844 - val_aux_output_loss: 0.6511\n",
      "Epoch 437/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.0785 - val_main_output_loss: 0.7306 - val_aux_output_loss: 0.6014\n",
      "Epoch 438/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0062 - main_output_loss: 0.0038 - aux_output_loss: 0.0040 - val_loss: 1.1427 - val_main_output_loss: 0.8394 - val_aux_output_loss: 0.5532\n",
      "Epoch 439/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0072 - main_output_loss: 0.0051 - aux_output_loss: 0.0037 - val_loss: 1.3598 - val_main_output_loss: 0.9546 - val_aux_output_loss: 0.7152\n",
      "Epoch 440/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0050 - main_output_loss: 0.0029 - aux_output_loss: 0.0035 - val_loss: 0.9271 - val_main_output_loss: 0.6081 - val_aux_output_loss: 0.5427\n",
      "Epoch 441/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0055 - main_output_loss: 0.0033 - aux_output_loss: 0.0037 - val_loss: 1.1446 - val_main_output_loss: 0.7538 - val_aux_output_loss: 0.6659\n",
      "Epoch 442/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1556 - val_main_output_loss: 0.7658 - val_aux_output_loss: 0.6663\n",
      "Epoch 443/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1576 - val_main_output_loss: 0.7675 - val_aux_output_loss: 0.6670\n",
      "Epoch 444/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1604 - val_main_output_loss: 0.7705 - val_aux_output_loss: 0.6671\n",
      "\n",
      "Epoch 00444: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-444.hdf5\n",
      "Epoch 445/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1617 - val_main_output_loss: 0.7721 - val_aux_output_loss: 0.6668\n",
      "Epoch 446/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1608 - val_main_output_loss: 0.7712 - val_aux_output_loss: 0.6669\n",
      "Epoch 447/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1629 - val_main_output_loss: 0.7730 - val_aux_output_loss: 0.6675\n",
      "Epoch 448/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.1697 - val_main_output_loss: 0.7796 - val_aux_output_loss: 0.6686\n",
      "Epoch 449/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1699 - val_main_output_loss: 0.7799 - val_aux_output_loss: 0.6685\n",
      "Epoch 450/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1728 - val_main_output_loss: 0.7826 - val_aux_output_loss: 0.6692\n",
      "Epoch 451/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1730 - val_main_output_loss: 0.7825 - val_aux_output_loss: 0.6698\n",
      "Epoch 452/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1719 - val_main_output_loss: 0.7809 - val_aux_output_loss: 0.6701\n",
      "Epoch 453/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1722 - val_main_output_loss: 0.7809 - val_aux_output_loss: 0.6705\n",
      "Epoch 454/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1576 - val_main_output_loss: 0.7645 - val_aux_output_loss: 0.6708\n",
      "\n",
      "Epoch 00454: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-454.hdf5\n",
      "Epoch 455/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.1645 - val_main_output_loss: 0.7715 - val_aux_output_loss: 0.6716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 456/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1684 - val_main_output_loss: 0.7753 - val_aux_output_loss: 0.6724\n",
      "Epoch 457/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1775 - val_main_output_loss: 0.7848 - val_aux_output_loss: 0.6731\n",
      "Epoch 458/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.1877 - val_main_output_loss: 0.7958 - val_aux_output_loss: 0.6735\n",
      "Epoch 459/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.1872 - val_main_output_loss: 0.7942 - val_aux_output_loss: 0.6749\n",
      "Epoch 460/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2025 - val_main_output_loss: 0.8107 - val_aux_output_loss: 0.6756\n",
      "Epoch 461/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0053 - main_output_loss: 0.0033 - aux_output_loss: 0.0033 - val_loss: 1.2059 - val_main_output_loss: 0.8146 - val_aux_output_loss: 0.6754\n",
      "Epoch 462/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0047 - main_output_loss: 0.0029 - aux_output_loss: 0.0031 - val_loss: 1.2083 - val_main_output_loss: 0.8134 - val_aux_output_loss: 0.6803\n",
      "Epoch 463/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2140 - val_main_output_loss: 0.8191 - val_aux_output_loss: 0.6812\n",
      "Epoch 464/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2249 - val_main_output_loss: 0.8300 - val_aux_output_loss: 0.6827\n",
      "\n",
      "Epoch 00464: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-464.hdf5\n",
      "Epoch 465/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2228 - val_main_output_loss: 0.8276 - val_aux_output_loss: 0.6828\n",
      "Epoch 466/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2374 - val_main_output_loss: 0.8426 - val_aux_output_loss: 0.6844\n",
      "Epoch 467/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2349 - val_main_output_loss: 0.8391 - val_aux_output_loss: 0.6853\n",
      "Epoch 468/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0032 - val_loss: 1.2354 - val_main_output_loss: 0.8383 - val_aux_output_loss: 0.6870\n",
      "Epoch 469/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 1.2402 - val_main_output_loss: 0.8419 - val_aux_output_loss: 0.6893\n",
      "Epoch 470/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2474 - val_main_output_loss: 0.8490 - val_aux_output_loss: 0.6904\n",
      "Epoch 471/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2549 - val_main_output_loss: 0.8564 - val_aux_output_loss: 0.6917\n",
      "Epoch 472/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2603 - val_main_output_loss: 0.8617 - val_aux_output_loss: 0.6926\n",
      "Epoch 473/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2444 - val_main_output_loss: 0.8431 - val_aux_output_loss: 0.6938\n",
      "Epoch 474/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2496 - val_main_output_loss: 0.8481 - val_aux_output_loss: 0.6947\n",
      "\n",
      "Epoch 00474: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-474.hdf5\n",
      "Epoch 475/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2545 - val_main_output_loss: 0.8525 - val_aux_output_loss: 0.6960\n",
      "Epoch 476/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2715 - val_main_output_loss: 0.8702 - val_aux_output_loss: 0.6976\n",
      "Epoch 477/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2743 - val_main_output_loss: 0.8717 - val_aux_output_loss: 0.6998\n",
      "Epoch 478/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2766 - val_main_output_loss: 0.8731 - val_aux_output_loss: 0.7012\n",
      "Epoch 479/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2721 - val_main_output_loss: 0.8671 - val_aux_output_loss: 0.7026\n",
      "Epoch 480/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2840 - val_main_output_loss: 0.8790 - val_aux_output_loss: 0.7042\n",
      "Epoch 481/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2863 - val_main_output_loss: 0.8795 - val_aux_output_loss: 0.7069\n",
      "Epoch 482/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.2664 - val_main_output_loss: 0.8576 - val_aux_output_loss: 0.7065\n",
      "Epoch 483/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.2856 - val_main_output_loss: 0.8777 - val_aux_output_loss: 0.7081\n",
      "Epoch 484/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0024 - aux_output_loss: 0.0031 - val_loss: 1.3028 - val_main_output_loss: 0.8957 - val_aux_output_loss: 0.7095\n",
      "\n",
      "Epoch 00484: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-484.hdf5\n",
      "Epoch 485/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0043 - main_output_loss: 0.0024 - aux_output_loss: 0.0031 - val_loss: 1.3287 - val_main_output_loss: 0.9235 - val_aux_output_loss: 0.7108\n",
      "Epoch 486/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0043 - main_output_loss: 0.0024 - aux_output_loss: 0.0031 - val_loss: 1.3439 - val_main_output_loss: 0.9391 - val_aux_output_loss: 0.7125\n",
      "Epoch 487/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.3601 - val_main_output_loss: 0.9556 - val_aux_output_loss: 0.7144\n",
      "Epoch 488/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0047 - main_output_loss: 0.0028 - aux_output_loss: 0.0031 - val_loss: 1.3839 - val_main_output_loss: 0.9807 - val_aux_output_loss: 0.7161\n",
      "Epoch 489/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0042 - main_output_loss: 0.0023 - aux_output_loss: 0.0031 - val_loss: 1.3454 - val_main_output_loss: 0.9599 - val_aux_output_loss: 0.6878\n",
      "Epoch 490/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0048 - main_output_loss: 0.0029 - aux_output_loss: 0.0031 - val_loss: 1.3694 - val_main_output_loss: 0.9887 - val_aux_output_loss: 0.6850\n",
      "Epoch 491/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0046 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.3826 - val_main_output_loss: 0.9982 - val_aux_output_loss: 0.6918\n",
      "Epoch 492/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.4469 - val_main_output_loss: 1.0652 - val_aux_output_loss: 0.6975\n",
      "Epoch 493/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0051 - main_output_loss: 0.0033 - aux_output_loss: 0.0031 - val_loss: 1.3893 - val_main_output_loss: 1.0010 - val_aux_output_loss: 0.6977\n",
      "Epoch 494/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.3956 - val_main_output_loss: 1.0027 - val_aux_output_loss: 0.7046\n",
      "\n",
      "Epoch 00494: saving model to ../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam/model-494.hdf5\n",
      "Epoch 495/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.4005 - val_main_output_loss: 1.0085 - val_aux_output_loss: 0.7040\n",
      "Epoch 496/500\n",
      "13733/13733 [==============================] - 40s 3ms/step - loss: 0.0044 - main_output_loss: 0.0025 - aux_output_loss: 0.0031 - val_loss: 1.4197 - val_main_output_loss: 1.0282 - val_aux_output_loss: 0.7062\n",
      "Epoch 497/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.4041 - val_main_output_loss: 1.0084 - val_aux_output_loss: 0.7093\n",
      "Epoch 498/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.3907 - val_main_output_loss: 0.9921 - val_aux_output_loss: 0.7112\n",
      "Epoch 499/500\n",
      "13733/13733 [==============================] - 41s 3ms/step - loss: 0.0045 - main_output_loss: 0.0026 - aux_output_loss: 0.0031 - val_loss: 1.3939 - val_main_output_loss: 0.9948 - val_aux_output_loss: 0.7122\n",
      "Epoch 500/500\n",
      "13733/13733 [==============================] - 42s 3ms/step - loss: 0.0046 - main_output_loss: 0.0027 - aux_output_loss: 0.0031 - val_loss: 1.4165 - val_main_output_loss: 1.0174 - val_aux_output_loss: 0.7156\n",
      "20577.239824056625  seconds\n"
     ]
    }
   ],
   "source": [
    "# Train the model by passing it lists of input arrays and target arrays 19960.996418952942  seconds\n",
    "# 25183.798012971878  seconds Second time, with 30 minute break in between\n",
    "start = time.time()\n",
    "merged_model_history = merged_model.fit(\n",
    "    [train_sequences_X, train_features_X], \n",
    "    [train_y, train_y],\n",
    "    #validation_split=0.2,\n",
    "    validation_data=[\n",
    "        [validation_sequences_X, validation_features_X], \n",
    "        [validation_y, validation_y]\n",
    "    ],\n",
    "    epochs=EPOCHS, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpointer]\n",
    "    #class_weight=[class_weights, class_weights]\n",
    ")\n",
    "end = time.time()\n",
    "time_taken_in_minutes = (end-start)//60\n",
    "print(end - start, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAELCAYAAADz6wBxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztvXmYVNW1uP2uquqJhgaBlqk1gKAMKopIgmPUOGYgMSZCrsYY70+TLybxGpNgbqZrTGKGG42JJnqNUzROmERUhASHGByQQWYEGkRoaKBpoKGBHmt9f5xT1VVNdVX1UF27mvU+Tz996tQ+p9Y+VWevs4a9tqgqhmEYhtFRAtkWwDAMw8htTJEYhmEYncIUiWEYhtEpTJEYhmEYncIUiWEYhtEpTJEYhmEYnSKjikRELhGRtSJSLiIzErxfICJP+e8vEJHh/v4BIvKqiNSKyO9bHZMvIveLyDoReU9EPpvJPhiGYRjJCWXqxCISBO4BLgQqgIUiMktVV8c0uw7Yo6qjRGQa8AvgSqAO+AFwov8Xy38DO1X1eBEJAP0z1QfDMAwjNZm0SCYD5aq6UVUbgCeBqa3aTAUe8bdnAheIiKjqAVWdj6dQWvNl4OcAqhpW1V2ZEd8wDMNIh0wqkmHAlpjXFf6+hG1UtQmoAQa0dUIR6edv/kRElojIMyIyqOtENgzDMNpLxlxbgCTY17oeSzptYgkBZcAbqnqziNwM/Bq4+rAPF7keuB6guLj4tDFjxqQltGEYhuGxePHiXapamqpdJhVJBXBMzOsyYFsbbSpEJAT0BXYnOWc1cBD4m//6Gbw4y2Go6v3A/QCTJk3SRYsWtVd+wzCMIxoR+SCddpl0bS0ERovICBHJB6YBs1q1mQVc429fAbyiSapI+u89D3zU33UBsLqt9oZhGEbmyZhFoqpNInIjMBcIAg+q6ioRuQ1YpKqzgD8BfxaRcjxLZFrkeBHZBJQA+SLyaeAiP+Pru/4xdwFVwLWZ6oNhGIaRGjkSysiba8swDKP9iMhiVZ2Uql0mYySGYRhxNDY2UlFRQV1dosx+I1sUFhZSVlZGXl5eh443RWIYRrdRUVFBnz59GD58OCKJkjaN7kZVqa6upqKighEjRnToHFZryzCMbqOuro4BAwaYEnEIEWHAgAGdshJNkRiG0a2YEnGPzn4npkiS8Mibm3h+WeupL4ZhGEYspkiS8NjbH/DSyspsi2EYRhdRXV3NKaecwimnnMLgwYMZNmxY9HVDQ0Na57j22mtZu3Zt0jb33HMPjz/+eFeIzFlnncXSpUu75FyZwoLtSQiIEA5nWwrDMLqKAQMGRAflH//4x/Tu3Ztbbrklro2qoqoEAomfsx966KGUn/O1r32t88LmEGaRJEEEwkfAPBvDONIpLy/nxBNP5Ctf+QoTJ06ksrKS66+/nkmTJjF+/Hhuu+22aNuIhdDU1ES/fv2YMWMGEyZMYMqUKezcuROA73//+9x1113R9jNmzGDy5MmccMIJvPnmmwAcOHCAz372s0yYMIHp06czadKktC2PQ4cOcc0113DSSScxceJEXn/9dQBWrFjB6aefzimnnMLJJ5/Mxo0b2b9/P5deeikTJkzgxBNPZObMmV156QCzSJIiIkkrSBqG0XH+5/lVrN62r0vPOW5oCT/65PgOHbt69Woeeugh/vjHPwJwxx130L9/f5qamjjvvPO44oorGDduXNwxNTU1nHvuudxxxx3cfPPNPPjgg8yYcdgafqgq77zzDrNmzeK2225jzpw5/O53v2Pw4ME8++yzLFu2jIkTJ6Yt6913301+fj4rVqxg1apVXHbZZaxfv557772XW265hSuvvJL6+npUleeee47hw4fz0ksvRWXuaswiSYLg/QAMw+j5HHfccZx++unR10888QQTJ05k4sSJrFmzhtWrDy/rV1RUxKWXXgrAaaedxqZNmxKe+/LLLz+szfz585k2zasKNWHCBMaPT18Bzp8/n6uv9oqejx8/nqFDh1JeXs4ZZ5zB7bffzi9/+Uu2bNlCYWEhJ598MnPmzGHGjBm88cYb9O3bN+3PSRezSJIQCEBH9MiV973FsH5F/ObKU7peKMPoIXTUcsgUxcXF0e3169fz29/+lnfeeYd+/fpx1VVXJZxnkZ+fH90OBoM0NTUlPHdBQcFhbTrzkNrWsVdffTVTpkzhxRdf5MILL+SRRx7hnHPOYdGiRcyePZtvf/vbfOITn+B73/tehz87EWaRJCEg0qEYyYL3d/PXd7dmQCLDMLqDffv20adPH0pKSqisrGTu3Lld/hlnnXUWTz/9NODFNhJZPG1xzjnnRLPC1qxZQ2VlJaNGjWLjxo2MGjWKb37zm3z84x9n+fLlbN26ld69e3P11Vdz8803s2TJki7vi1kkSRAgbJ4twzjimDhxIuPGjePEE09k5MiRnHnmmV3+GV//+tf54he/yMknn8zEiRM58cQT23Q7XXzxxdE6WGeffTYPPvggN9xwAyeddBJ5eXk8+uij5Ofn85e//IUnnniCvLw8hg4dyu23386bb77JjBkzCAQC5OfnR2NAXYlV/03Cp+95g5KiPB798uR2HTd8xosAbLrj4+3+TMPoyaxZs4axY8dmWwwnaGpqoqmpicLCQtavX89FF13E+vXrCYWy83yf6Lux6r9dQEAs2G4YRmaora3lggsuoKmpCVXlvvvuy5oS6Sy5KXU3IR2MkRiGYaSiX79+LF68ONtidAkWbE+CZ5FkWwrD6FmYle8enf1OTJEkwSwSw+haCgsLqa6uNmXiEJH1SAoLCzt8DnNtJcGytgyjaykrK6OiooKqqqpsi2LEEFkhsaOYIklCQIRm0ySG0WXk5eV1eBU+w13MtZUEK9poGIaRGlMkSQhY0UbDMIyUZFSRiMglIrJWRMpF5LCSmCJSICJP+e8vEJHh/v4BIvKqiNSKyO/bOPcsEVmZWfnNIjEMw0hFxhSJiASBe4BLgXHAdBEZ16rZdcAeVR0F3An8wt9fB/wAuIUEiMjlQG0m5G71ORZsNwzDSEEmLZLJQLmqblTVBuBJYGqrNlOBR/ztmcAFIiKqekBV5+MplDhEpDdwM3B75kT3CHh15DP9MYZhGDlNJhXJMGBLzOsKf1/CNqraBNQAA1Kc9yfA/wIHkzUSketFZJGILOpoqmHALBLDMIyUZFKRSIJ9rYfldNq0NBY5BRilqn9L9eGqer+qTlLVSaWlpamaJ/48LEZiGIaRikwqkgrgmJjXZcC2ttqISAjoC+xOcs4pwGkisgmYDxwvIq91kbyHISLm2TIMw0hBJhXJQmC0iIwQkXxgGjCrVZtZwDX+9hXAK5qkdoKq/kFVh6rqcOAsYJ2qfrTLJfexrC3DMIzUZGxmu6o2iciNwFwgCDyoqqtE5DZgkarOAv4E/FlEyvEskWmR432rowTIF5FPAxepavpLiHUBgUSON8MwDCOOjJZIUdXZwOxW+34Ys10HfK6NY4enOPcm4MROC5mEji61axiGcSRhM9uT4Lm2si2FYRiG25giSYIXbDdNYhiGkQxTJEkIWNaWYRhGSkyRJMHmkRiGYaTGFEkSApJkdqRhGIYBmCJJii21axiGkRpTJEkQsZqNhmEYqTBFkgQLthuGYaTGFEkSLNhuGIaRGlMkSTCLxDAMIzWmSJIQCJhFYhiGkQpTJEmxha0MwzBSYYokCV71X9MkhmEYyTBFkgRbatcwDCM1pkiSYAtbGYZhpMYUSRIsa8uI8PG7/83YH8zJthiG4SQZXdiqJ2AWiQGwatu+bItgGM5iFkkSAmJVGw3DMFJhiiQJAYuRGIZhpMQUSRJsqV3DMIzUmCJJQkAENd+WYRhGUjKqSETkEhFZKyLlIjIjwfsFIvKU//4CERnu7x8gIq+KSK2I/D6mfS8ReVFE3hORVSJyR4blN4vEMAwjBRlTJCISBO4BLgXGAdNFZFyrZtcBe1R1FHAn8At/fx3wA+CWBKf+taqOAU4FzhSRSzMhP0TWI8ltTdIcVnbuq8u2GIZh9GAyaZFMBspVdaOqNgBPAlNbtZkKPOJvzwQuEBFR1QOqOh9PoURR1YOq+qq/3QAsAcoy1YFAD1jY6uez1zD5Zy+z+0BDtkUxDKOHkklFMgzYEvO6wt+XsI2qNgE1wIB0Ti4i/YBPAi93WtK2PoPcX2r3lfd2ApgiMQwjY2RSkUiCfa1H5XTaHH5ikRDwBHC3qm5so831IrJIRBZVVVWlFDYRgR4wjSTgVZ7MeYVoGIa7ZFKRVADHxLwuA7a11cZXDn2B3Wmc+35gvare1VYDVb1fVSep6qTS0tJ2CR5B/BIpuRwnCYqnSJota8AwjAyRSUWyEBgtIiNEJB+YBsxq1WYWcI2/fQXwiqYYtUXkdjyFc1MXy5vgs7z/OaxHzCIxDCPjZKzWlqo2iciNwFwgCDyoqqtE5DZgkarOAv4E/FlEyvEskWmR40VkE1AC5IvIp4GLgH3AfwPvAUvEG+l/r6oPZKIPAV+T5PIQ7OsRwuHsymEYRs8lo0UbVXU2MLvVvh/GbNcBn2vj2OFtnDZRXCUjRAdhVYLd97FdStDvRLNZJIZhZAib2Z4E3+LJabdQwGIkhmFkGFMkSegJMZKIReJawsCeAw18UH0g22IYhtEFmCJJghAZhLMsSCeIuOdcs0gu+M2/OPdXr2VbDMMwugBTJEmIjZHkKlHXlmN9sAmShtFzMEWShJ6RtZX7VpVhGG5jiiQJ0gGLxLVYRDRryzHXlmEYPQdTJEmQDjzNx7Z1QakELP3XMIwMY4okCYFo1lY7LJLYbQfG7mB0QqIDwhiG0SMxRZKEyBTE9ozBsUrHhaHbXFuGYWQaUyRJCHRyDoYL2V6BHjCp0jAMtzFFkoSWme3pH+Oaa6tlZnuWBTEMo8diiiQJEddWu2IkscF2B5xbAf8btmC7YRiZwhRJEjoyjyRWebgwdrdYJGaSGIaRGUyRJKFj80gSb2eLSLC9qdkBYQzD6JGYIklCoJNFG10IcNsKiYZhZBpTJEnoSBn5+BhJ9olknjWZIjEMI0OYIklCS7A9/WPiYyTZH7zNIjEMI9OYIklCRwoexrZ1Yew2i8QwjExjiiQJkdTZDsc6HBi7W9YjsawtwzAygymSJEQWtmpXjCRm24Vge8SqMouka3DBXWkYrmGKJAnRpXbbcYyrtbZcLdqYawOzo5fRMLJKRhWJiFwiImtFpFxEZiR4v0BEnvLfXyAiw/39A0TkVRGpFZHftzrmNBFZ4R9zt0RSqzIjP9CZ6r/ZH3Vct0gcFatNXPhODcM1MqZIRCQI3ANcCowDpovIuFbNrgP2qOoo4E7gF/7+OuAHwC0JTv0H4HpgtP93SddL79GReSSuBdsjuJq15YL7rz04ehkNI6tk0iKZDJSr6kZVbQCeBKa2ajMVeMTfnglcICKiqgdUdT6eQokiIkOAElV9S71Hw0eBT2eqA4EOFG2MNUlcqLUVkcFViyTH9IgT36lhuEYmFckwYEvM6wp/X8I2qtoE1AADUpyzIsU5u4yW9Uja49pya0ZiRHSzSLqGHBPXMLqFTCqSRLGL1rdhOm061F5ErheRRSKyqKqqKskpk3xYD5hHEvHpu1prK9cG5lyT1zC6g0wqkgrgmJjXZcC2ttqISAjoC+xOcc6yFOcEQFXvV9VJqjqptLS0naJ7BDpStDFuO/ujTkQCV+eRuHCN2kOuWVCG0R1kUpEsBEaLyAgRyQemAbNatZkFXONvXwG8oknSYlS1EtgvIh/xs7W+CDzX9aJ7dMwiaWnshkXi/Xc1RuKoWG2SY+IaRrcQytSJVbVJRG4E5gJB4EFVXSUitwGLVHUW8CfgzyJSjmeJTIscLyKbgBIgX0Q+DVykqquBrwIPA0XAS/5fRohmbXVw+HAhVTQiu8VIuoZck9cwuoOMKRIAVZ0NzG6174cx23XA59o4dngb+xcBJ3adlG3Tsh5J+se4ttSu68F2F65Re8g1eQ2jO7CZ7UnodBl5BwadlhiJA8IkwAWrrT3kmryG0R2YIklCh6r/xpaRd8CjbjGSrsX0iGEcjimSJLSsR9Ix35YLg2REdrNIugaLkRjG4ZgiSULUImnHMa7V2oqI0NjsZvqvo/qtTXJMXMPoFtJSJCJynIgU+NsfFZFviEi/zIqWfaLzSNox2rm21G7EvebqgO2C+689OPBskJO8vq6Kd95PNkXMyGXStUieBZpFZBReyu4I4C8Zk8oVOpS15dZSuxERXJAlEY6K1SauXkfX+eKD7/D5+97KthhGhkhXkYT9WlifAe5S1f8ChmROLDdocW3lftaWq759V+Vqi9yS1jC6h3QVSaOITMebhf6Cvy8vMyK5Q0eytmJxYdCJziNxQZgE5JgeyTnFZxjdQbqK5FpgCvBTVX1fREYAj2VOLDeQTtbacmHQibhiXHXJuHCN2kOOiWsY3UJaM9v90iTfABCRo4A+qnpHJgVzgUBHYiSxS+06MOi47tpyVKw2cfU6GkY2STdr6zURKRGR/sAy4CER+U1mRcs++cEgAA1N6afOxpeRz/6g4/48kmxL0D5yTV7D6A7SdW31VdV9wOXAQ6p6GvCxzInlBkX53uU51NjcoeNdGHRaLJKsitEmLijb9pBj4hpGt5CuIgn5y9x+npZge4+nMM+zSOoa0lckrg00rqf/5oIiiXNXOpFCYRhuka4iuQ2vHPwGVV0oIiOB9ZkTyw2KfEXSHoskdqBxYZB0vmhjtgVIA9dWvTQM10g32P4M8EzM643AZzMllCv0yvcuT7sUiWvzSNTxme0uXKQUuFb2xjBcI91ge5mI/E1EdorIDhF5VkTKUh+Z2xSE/BhJe1xbMdtOWCS+CC7IkghXFVwssdcuF+Q1jO4mXdfWQ3jL4g4FhgHP+/t6NIGAUJgXoK5dFkmsPz37tNTackGaw3FUrDjiZcwBgQ2jm0lXkZSq6kOq2uT/PQyUZlAuZyjKC+Z21lbEInGz+K+zCi6W+LhXFgUxDEdJV5HsEpGrRCTo/10FVGdSMFcoygt22LXlgj/dfdeWm3LF4lrcyzBcI11F8mW81N/tQCVwBV7ZlB5PYX77LBJ3y8i7II2Ha7P/U+HaJFPDcI20FImqblbVT6lqqaoeraqfxpuc2OMpygu2K0YSqz7as45JpmixSLIrRyy59oQfvzRAFgUxDEfpzAqJN3eZFA5TlBfkYAcnJLow5kRntjukScJxCQnuyNUWYbNIDCMpnVEkkrKByCUislZEykVkRoL3C0TkKf/9BSIyPOa9W/39a0Xk4pj9/yUiq0RkpYg8ISKFnehDSora69qK3XZgzGmZR+KAMD7xA3P25EgXF2JdhuEynVEkSe8uEQkC9wCXAuOA6SIyrlWz64A9qjoKuBP4hX/sOGAaMB64BLjXD/IPw6tCPElVTwSCfruMUdjeYHuc2yb7A5CLrq34eRkOCdYGZpEYRnKSKhIR2S8i+xL87cebU5KMyUC5qm5U1QbgSWBqqzZTgUf87ZnABSIi/v4nVbVeVd8Hyv3zgTcbv0hEQkAvYFuafe0Q7Y2RxPnTMyFQO3GxREquxUjINXkNo5tJqkhUtY+qliT466OqqcqrDAO2xLyu8PclbOMv5VsDDGjrWFXdCvwa2IyXPVajqv9IIUen8CYkdqyMvAuDjosLW8XFSBySqy1cq59mGK7RGddWKhLFUFrfhW21SbjfX1RrKjACzyIq9ue0HP7hIteLyCIRWVRVVdUOseMJBgI0dfBp3oVBx8Uy8rlWciRWxhwQ1zC6nUwqkgrgmJjXZRzuhoq28V1VfYHdSY79GPC+qlapaiPwV+CMRB+uqver6iRVnVRa2vFJ+KGA0NyOaeHOZW35QjQ7oNQixA3MDsnVFppjFpRhdDeZVCQLgdEiMkJE8vGC4rNatZkFXONvXwG8ot6dOguY5md1jQBGA+/gubQ+IiK9/FjKBcCaDPaBUFDaZZG45gaJSODSABibipxzFkkOyGsY3U1aZeQ7gqo2iciNeOuYBIEHVXWViNwGLFLVWcCfgD+LSDmeJTLNP3aViDwNrAaagK+pajOwQERmAkv8/e8C92eqDxCxSNqhSOLyf7tenvbiYhn53I6RZFEQw3CUjCkSAFWdDcxute+HMdt1wOfaOPanwE8T7P8R8KOulbRtOhMjcWGyXdS15dAImHMxhxxzxRlGd5NJ11aPoDMWiQsVd12vteWSXG2RaxMoDaO7MUWSgqCvSNJ9EnUuRqLx/10g1wbm+LlBOSCwY5gV1/MxRZKCUMDLRE7XKnE2a8uhETvnYiQWbO8Uds16PqZIUhAMeook3ThJbCsXCiW66NqKVyRZFCRNck1e13Dpt2dkBlMkKWi/RdLSzoW5Gy66tnJtfY9ck9c1HHieMjKMKZIUBAPeJepI5pYLN5CbExJz6wnfNXdlrhFXycCFm8LockyRpCAv2E6LJGbbhZvGTddW7LY7crWFawkUuYxdv56JKZIUBH3XVlNzerm8sfeJCwHuWNeWK4Ht2OvihkTJcW2Saa4Rdszda3Q9pkhSEImRpO/acuvpNc5Cyr44QO7Vrsq19VNcw0rM9HxMkaQgEiPpSPqvC4OOi5P/cm8eScx2DsjrGnEWSS584Ua7MUWSgohFsr+uKa32sbdJmt6wjBIvjxs3ce4F291TxrmExtwHdv16JqZIUhCJkVx297/Zua8uZfu4GIkDN42LboVccxVZ1lbnyLX1Z4z2Y4okBRGLBKCyJh1F4pj/38FB2zX3XyriXVvuy+salv7b8zFFkoJgjCKJ3W4L11xJcfI4MgjmgvKIJddcca6Ra+neRvsxRZKCULBFeQQktSKJxQlFEuuWcSBmA7k3sMRbUNmTI1dxrdqD0fWYIklBJGsLIB094lqBPxcn08UqWBdK7aciPkbixjXMJVyM0xldiymSFOTFuLOamlPfBbEDjQtPX64F/6FVHCmLcqSLubY6h6X/9nxMkaQgNi7SlM7js6Mz28FRi8QRmdIl1+R1gVzL0jPajymSFMTGSNKZ3e5aho+LT9PNcTI5IlQSbPDrHK65e42uxxRJCmJjJGm5tuIskkxI1HFcsJAg9wYWF626XMJcWz0fUyQpCLXTteVyjMSVQTDetZVFQdLESqR0jlzL0jPaT0YViYhcIiJrRaRcRGYkeL9ARJ7y318gIsNj3rvV379WRC6O2d9PRGaKyHsiskZEpmSyD8H2BttjbxoHRsm49cazLw4Qr2BzYWCxmdmdw2IkPZ+MKRIRCQL3AJcC44DpIjKuVbPrgD2qOgq4E/iFf+w4YBowHrgEuNc/H8BvgTmqOgaYAKzJVB+gtUXSvhiJCzeNa2XtIfeytuJdcbkgsVuoKeIeTyYtkslAuapuVNUG4Elgaqs2U4FH/O2ZwAUiIv7+J1W1XlXfB8qBySJSApwD/AlAVRtUdW8G+9DKIknDteXY5CvXFBvEx45yY2B2z6qL8NjbH/Dmhl3ZFiMpYQcfZoyuJZOKZBiwJeZ1hb8vYRtVbQJqgAFJjh0JVAEPici7IvKAiBRnRnyPUGywvZ03gROuLQefBuMWtnJEpmTETahzzIb6/t9X8oX/W5BtMZJirq2eTyYVSaJ54K1/RW21aWt/CJgI/EFVTwUOAIfFXgBE5HoRWSQii6qqqtKXuhXx6b/pBNtbcGHgVm2Zke/KTZxrA4uVSOkcuZalZ7SfTCqSCuCYmNdlwLa22ohICOgL7E5ybAVQoaqRR7CZeIrlMFT1flWdpKqTSktLO9yJ2BhJYxrBducmJNLSB1cG7VwLXrs4FyeXsPTfnk8mFclCYLSIjBCRfLzg+axWbWYB1/jbVwCvqOeLmQVM87O6RgCjgXdUdTuwRURO8I+5AFidwT7ExUjSuQlcq22lqtFik67cxPGuLTdkSoaLKdS5hF2/nk8oUydW1SYRuRGYCwSBB1V1lYjcBixS1Vl4QfM/i0g5niUyzT92lYg8jackmoCvqWqzf+qvA4/7ymkjcG2m+gCtYiRpBdtbtl24aSIWST3uPE3n2hN+XAp1FuXIVXLNlWm0n4wpEgBVnQ3MbrXvhzHbdcDn2jj2p8BPE+xfCkzqWknbJhhsn2vLtZntqhBwzbWVY0uvxoqYzsOEEU/8hMTsyWFkDpvZnoJQu11bLTiRtYVG++CMayuH55EcbGhuu6GREIuR9HxMkaQgNkbSmE7WlmvzSLSlD67cw+Fwbrk6YmXcV9eYRUlyEzXXVo/HFEkK4iyStNYjacGFm0a1ZWVHVwLbubbQUayItXVNWZMjV4lzbZlnsEdiiiQFIsL8754HQGOOTkh02rWVA5okVsb9pkjaTa5ZoEb7MUWSBmVH9SIUkHZnbaUz7STTKC0JA47okVYDSxYFSZPY77S23h1F4sKDSjpY9d+ejymSNAkGJM0nereevlQh6Jxry61rlIpI+m+v/CD7HYqRtLdkT7awGEnPxxRJmuQFA+1O/3XhiVHRaLDdheA/tFqPxIFrlIrIZSspzHPKtZUrg7LFSHo+pkjSJBSUdtXaEnEjJuFk1pZjmW2piFy3kqKQU64tF35f6ZBr33dHeODfG/nxrFXZFiNrZHRCYk8iFJD01iPxmwRFnBi4lZasLVeeYGOvSy64ZyLXraQwjx376rMsTQu5MigfCfNI3tpQzQe7D2ZbjKxhFkmahAKB9ILtvk0SDIgTA7eqkhf0vuZ0VnjsDiKDSSggaaVUZ5vIdTuqOJ/a+iZnYk25cO0g3t17qIdO6KxvCvdYJZkOpkjSxHNtpW+RhNIOzmcWVSgIeV9zoyPlPSJxkYJQICcskohLs29RHs1hpb7JjeuYixbJocaeqkianbm/soEpkjTx0n/Tn5AYcMUiAQry3FIkkQEwPxRIK+6UbSJJFkV53mrPrii/XEhUgHhXZk+1SBrMIjHSIRRMb9CLuD1ccm0VhLwB0JUn6cj9FgoGcuLmi7g0C/MCca+zTS5aJD21Vll9Uzi99Yp6KKZI0iRdiyS2vQuDpAL5QbcsknBYCQjktfOaZouIBRKxSFwZMFz4faWDHhGurTDNOWBdZwpTJGmSbowkQjAgTuTMq3ouJIBGRyySZvXmtgSDbijbVEQtkvw+N4YfAAAgAElEQVSIa8uR65gjC4TFB9vdSZ/uSuobm3PioShTmCJJk6K8IAfSmEMQn/6b/R9WWDUabG9wxSLxV20MBXIl2N4qRuLIgBGrSFyxkhIRFyPpwRZJLvyWM4UpkjQ55qhebE4jTzya/hsUN3zYsRaJI4ON59oSgoH0Jnlmm9bBdmdchDG/L5ev45EQI2loCjv9HWQaUyRpMnxgMZU1dSmzTlrSfwNOZNUoLYrElWB7c9hz/bU37pQtIr7vQseytmL1mSsyJSKiSArzAtSZRdIjMUWSJsMHFgOktEoiiiQgbpQkUd+NlB8MOPUkHRAv7pQLMZKIRVLoWhp1zLVzWSFH7oneBaEeaZGEw0pDcxjV3EmA6GpMkaTJ8AG9APig+kDSdpGfUSjgRmqrAgLkBcWZYHtYlUBACOZMjCRMMCDOVggAd1KSExGxSHrlh3pkjCQ29nikurdMkaTJUb3yAag5lLyMeCR7JhAQdtXW05DlwVvVKyCZHwo4E2xvDitBEWdSpFPR1OwtDhaKKBJHBovmuBiJu9cxIlpxQahHTkiMdRm78pDR3WRUkYjIJSKyVkTKRWRGgvcLROQp//0FIjI85r1b/f1rReTiVscFReRdEXkhk/LH0rvAq2+Zqvpr5GcUDMDO/fXc9NS7GZYsOYoiIn4ZfDcGwLDiWyTijEzJaGz26pXl+VWUXUlayBXXVsQi6V0Q7JEWSX1TS59cVuiZJGOKRESCwD3ApcA4YLqIjGvV7Dpgj6qOAu4EfuEfOw6YBowHLgHu9c8X4ZvAmkzJnojiiCJJcz2KukZvgJy9YnvGZEqHsEZcWwFngu3RCYk5EiNpDocJBWMsEkcG7dhsqEZHrKREaIxrqyfGSOobYy0Sd7+HTJJJi2QyUK6qG1W1AXgSmNqqzVTgEX97JnCBiIi//0lVrVfV94Fy/3yISBnwceCBDMp+GPmhAPmhALWpJlT597YzK+mpt+58QSi9hbm6g2b1XFu5EiNpDHuurci6Lq64tmIVmivKLRGRr7h3QYi6nqhIYl1bOfB7zgSZVCTDgC0xryv8fQnbqGoTUAMMSHHsXcB3gG6/m/sUhFJaJJF5JJGV9CKpt9nCc235Kzy6YpH4wfbciZGECQUC5AV9ReLIoJ0r80gig2txQZCDjc1Oz8LvCObayqwikQT7Wl/lttok3C8inwB2qurilB8ucr2ILBKRRVVVVamlTYPigtQr5EXukYgJX5BtReK7tlwKtsdPSHT/xmtqVs+1FXAs2J4jMZJIwkm/Xvk0h9UZy7iraGgy11YmR7kK4JiY12XAtrbaiEgI6AvsTnLsmcCnRGQTnqvsfBF5LNGHq+r9qjpJVSeVlpZ2vjd4pnmqMimtb5GsKxLwLRJ3AtvN2jIhMRcK3TWG/WB70LFge45YJJGBtm9RHtDzSsmbayuzimQhMFpERohIPl7wfFarNrOAa/ztK4BX1LN7ZwHT/KyuEcBo4B1VvVVVy1R1uH++V1T1qgz2IY7ehSHmrdnJdQ8vbPNmaG21R0q4ZwtVRfCytrKdihwhMiExmEMz211M/42tnBAb8HWNwxRJD8vcsvTfDCoSP+ZxIzAXL8PqaVVdJSK3icin/GZ/AgaISDlwMzDDP3YV8DSwGpgDfE1Vs/7riyiPl9/bycJNuxO20VY2SeQpNltELBIXXVt5wRwJtjdr1IKKvHaB2GvncjZUQ3MzwYBEU+gP9rAKwPWNsTESN+6x7iaUyZOr6mxgdqt9P4zZrgM+18axPwV+muTcrwGvdYWc6RI7q/07M5fznUtO4PKJZdF9K7fWMG/1jrhjsp1yG42RODSPpDnsl5HPoWC759pyLP03VpE4/JTf0BQmPxigyC/D77LS6whmkdjM9naxLyZja/u+Om5+ehlV++uj+z7xu/m8utYL7D99wxSO7d8rqzdNJDtGRDyLxCnXll+0MQee4JrCfrA96Fb6b2yMxOV1PhqawhTkBaLVk3ta4cYGi5GYImkPj355MjecM5KHrj2dL585AoDX1yXOCBs+sBeXnTQkq2Z8ZJyJpv868rTkzWwnZyySxuYweYEAeQG3yvE354hrq963SHodERaJGw8Z3U1GXVs9jXOOL+Wc470MsHNGl/LC8m1865ll3PXyOr567qi4toJQnB+ksVk90z4L2VuRYca1YHtsra1ceIJrDiuhQKDFInFksMiVdT4iv/+Ia6vnBdtb+pMLD0aZwCySDhIMCA9+6XQAtuw+xPf+tiLu/eKCYMuNk+ZN3tAU5tdz11Jd67nLNlTVdmpNkxbXFhTkBeJ+8NkkrvqvI0/3yWhsbu3ackPm2GvnckptfbOvSPLadz/kCrEWSaMjv43uxhRJJzhxWF9uueh4Pjb26Lj9d1x+Er3yQ9H6XFv2eGuYrKio4Q+vbeDWvy6nsuYQL6/Zwba9h6LHvbSykt+/Ws6d89axufogF/zvv/j1P9Z2WL4WiwSO6pXHnoONTiy2FfZLpOQF3YyRvFm+i/+Nue5NYT/YHnBrPZJYi8Tlp/yGqGvLux9clrUjxFr6uTAvKhOYa6uT3Hj+aAA+9fv5LK+o4Q//MZFLTxoCwCnH9CM/FGDqPW/wn2eP4L5/bYwe9/yyyugs+fxQgP698ikp8r6O2Su280Z5NQD3vraB/XVN3HrZmOiN2Jo//msDq7ft47ap4+nnl7uH+BjJgOICmsPKvrrGuDbZoDlmZruLroCvP/Eu1QcamHrKUEYd3SdaRj4QEALiTmZOrD5z3bVVkBeMWiQuy9oRYi39WUu3cf6YQWkdt6+ukZ/PXsP3LhtLn8K8TsuxbMtemlWZeOxRnT5XezGLpIt4+NrJXHfWCM4aPTC6b+yQEp6/8SwmD+8fVSKTPnQUn51YFldqpaEpzPZ9dazbUQvA7gMNvL+rJdX4z29/wE9eWM3r66q46cl3owF+VWVz9UHueOk9Zi3bxuMLNsfJFI7J2hrQ21Meu2obMtD79hEOe8F2V2Mkxx3dG/AUejis1NY3RVN/Qw7NfWmOVtUNup+1FZP+29OytmIng/596baUaxZFePTNTTzxzhYenL+pS+SYes8bXH7vm11yrvZiFkkX0b84nx98onWVfDhhcB/+cNVELrrzdT5z6jBuvWwszWHlpGElFBeEeHNDNX0KQ3xs7CBWbK1h78EGnlq4JZpq/LXzjqPmUCOPvb2ZJ97x6lj+fek2hvQtRIBtNXWAZ9X8au5ajivtzbnHl7Jlz0GO7e+t6ijiyQeekuoM5Tv3M6C4gKOKO27VhFXJCwQIBgKo+hMUA9mduBlLpKxNZc0hfvLiair2HGLCMf0AyAuIM8H2Zl8O15ewrW9qpld+iLygZ4X2uAmJrZJYDtQ3RWfxJyPou0pr6x2pFN4JTJF0A/165TP/u+dHZ7kHA8KX/PThz01qKSkWyQi79dKx3P3Kehqawnz74jE0NYcZfXQfGprCnHpsP+5+pfywtOMrTivjLws285XHWupZPvilSQAERBhQXAAQDeR3hLrGZj72m9c5cVgJL3z97A6fpzkyjyQmeJ3vkCKJVG7ec6CRV97bCsCW3V6cyy2LxPvfu9DtJWwbmsP0CwUQEUoKQ2k/secKrZNYUhV2jVCY5ymSOofL26SLKZJuoj3pv4GAcNPHjo++DgUDXHPG8Ojr300/lbkrt/N+9QH+tbaKG84dyYXjBjGgOJ/fvVIebfflhxdFrZ0+hd5XXX2ggTc37GL80L7MXbmdK04rS2kNLNhYzYjSYlZU1ACwcuu+6Hsrt9awc39d2n5haCmREopZ3yPfIS9rZC2ZvYcaGNy3kM27D7J2+36ge4tfPvHOZm796wqW/eiihE+4kcSJPo5bJJFgO8DA3gXs2p9992pX0tAUjnPT7k9z8btIfLAnuPpMkeQgfYvy+PzpniXz3UvGRPd/66ITmD75WIb2K+KBf2/k9hfX8PPLT2LU0b2jmSXf//vKuHOVFIUYM7iEXgXBqPvh3c17OX14f4IBobq2nivvf5sTBvWhd2HLz6Vqfz0DivP5xO/mA/DeTy6hMC+9ApVhv/pvy0JRnX/C37b3EAver+Yzp5albpyCiFtx78FG+vXKY/NuKPEH8lA3pizf968NgOdiS6RIIjGS3oUhdh9w9yk/MrMdfEXSCavYReqbwhQXtFha6S5qF1E4nS1vs/tAAzv21UVfq3rLa3cnpkh6GEP7FQHwn2eP5PKJZdHYSH4owLcuPJ7//ee6uPZfeWxJm+e6ePwg1u/0EgDW7vCeyL90xnAefnMTX31sMYs+2BNte8+r5XzrohNSylfX2Mzqyn2cV1IatUiau2BgvupPC9hYdYDzTjg6YVZaU3OYq/60gMsnlvH5GHdiIiIDwZ6DDRSEggzsnc/Mr0wBIC8k1HXTfJyIgk0U16prbOaOl94DoLR3Aau37XMu1hQhziLpU8CKir1Zliiee14tRwT+v4+OSt04BlVlzsrtVB9ooCgvGFUk6bq2Iu12dzIB5sr73orep+AptnQf6roKd/wJRpfTv1VA/OsXjObc40s5f8zRTJ98LB8e0T/u/bFDSjg7Juts7qodbKw6wDBfOQHMuHQMl4wfzLtbvMHg2P69KCkM8btXynlpRSWVNYf47bz1PL2wZYHLXbX10cmRv315Pc1hZU3lfoLRsuwdUySxK+1trPKy3D6oPpiw7fPLt/H2xt38q42SNhEam8NRn/Weg43sqq1n6inD+NCAYgDGDi5h4fu7Kd+5n5/NXtNlgXdVZb2vrCNEXB+JMu0irjaAM0YNZM/BRlZX7jus3V3z1vHO+4krVXcXDc0tlR0GFOd3Webg3FXb2dPJ5BGAx97+gL+/uzWttj9/aQ13v7yebXsPMWfldr76+BLeeX83/Xq1WIypVlFt3W7n/roULdtm94GGOCUCpFwzKROYRXKE8ciXJ8e9PuW2f7D3YCNzbjqbMYNLAO9pt66xmbmrttMrP8QnJwxl5uIKggEozAvy+y+cSrMqlXvr6FuUR2FekPN+/RpffTzeunl9fRU799fzzvu7+cypwxgxsJi3NnjzYwpCAYr9dNCNVbWU9ilgX10jJYV5LNhYzdB+RRzjZ50lYt2O/Xz54YX89DMncfaoFuW3qfpANMMqQjis/OE1z01UvqOWOSsreXbJVv541Wks2FjNmCElUaUbcTcMLilk+746GvDcMRE+NnYQ/1i9g4/95nUA3ijfxV/+8yP07RXvelr8wW5mLd3Gjz45Ps5KiCi/eWt2cu7xpdEEjLvmree3L69n1o1ncnKZJ39UkfiFQd/eWE0wIJw+vD/l/uDx6VOGcq6fpPHWhmpOHNY3+lkVew5y17z1bK4+yGT/oWH9jv3s3F/PmTHXLBkrKmr4+9Kt/PdlY9Oydg41NJMXbFm7Bbz02IgiKe1TQG19E3WNzUmfmldurWH80JI2XTQbq2q54c+LuWT8YP549Wlp9SUR1bX1VNbUse9QMOoS2lBVy7zVO/h/Z488rM/PLt7K4L4F/KaVZd+/OJ+3bj2fKT9/JalFUl1bT3FBiMK8ILV+9tqWPYei1mTV/npmLq7ghnMO/+xETPzJPw/bd6C+mQG90+l912GK5Ajn6Rum8Pq6qqgSAU9ZFOYFufL0Y6P7rjitJfYQCgYIAcMHFkf3zfzqFH47bz3PLK5g4rH9GFnam5mLK6Lv/y3mie+Y/kU88uXJ9C/O5+g+73HdI4sYO6QPCzft4dsXn8Cv5q6lT0GIxT+4kK17D3Hz00upbwwzdkgJHxnZn5Glvbn75fVU7DnEd2cu5+sXtLgkvvnkUh596wO27jnE508/hqs+fCw3PvEu63bUMrRvIWt37I+6855dUsF3Zi4nPxjgS2cO5+2N1dx15SkADDuqiO2+33lw3xZFctnJQ/jVP9ZGqz6v2raPCbf9gxvOGclxpb257/UN9C3KY3lFDU1h5cUV2ynKD3Du8aWs31HLmsp9fOqUoTz29mZOPbYf/Yry6JUf4sUVlQCsqdzHyWX9qK1vYqf/Gbtq62loCnPjX5bQtyiPl7/1UTZU1RIKCL/63ATyggGG9Sti+daaqJzba+qiT9kbYuYkff2Jd3lv+37uv/o0igtCfFB9kC982PueF27azb/XVfG180dFF2T7wXMrWbplL3+a/z7//s55ccp95dYarnnwHX7/hYlMOW4AqsrYH87h4ycP4Z4vTIy2q4+xSAb685l27KvjQwOKqa6tp6E5zJC+RcxbvYOtew8x6uje/McDC/jJp0/k6o98CFXlpqeWcsHYQXxqwlDAU+AAc1Zt597Xytvtlnrg3xvp1yufo/t43+2Bhmb2HmzkqOJ87nm1nL8u2UpBKBDNrgQvLrirtj5hum5BKMCgPoVA28F2VeW02+dx1qiBPPafH45aJA1NYbbVHKLsqF7c9NS7vFFezVmjBnJSWd+E54nQVpWKVdtquOmpd/nuJWPYuvcQn5wwNDoPKlOYIjnCOX5QH44f1KfT5yk7qhe/+twErjz9GEaW9qZfUR4nDi3hx8+vPqztl88cER2Q7p5+KtPuf5uFm7x4y6/meqVJ9tc3Me6Hc+LcXqsr9/HsEk85FYQCHN2ngO376vjvv8UnECz2Yzd3v7yeu19eD8DJZX2ZPLw/D8x/P9ruJ75sDc1h7n/dmzD66XveADzFeVxpMZM+1J/L/EoF4M3ZeOaGKVTW1HHC4D48/Mb73P1KOff5x58wqA/rd9ZG5d5VW88Jg/rw2Nstk0Uj2+9uPjxW8N1nV1AQCkblBq+6wb2+RbWrtoFFm3bz8pqdDB9YHB0gThrWl+eXbeOGc0ayqfoAN/7l3Zbrtq2GA/VNfPXxJbznu8Rmr6jkzQ3V7KqtZ8pxA3ht7U7+x78eC97fzbihJXzrohOi2X4A/1i9g2Vb9rJjXx2fOHkIzy+rpPpAA797ZT2nDz8q6mJ5cXklZxz3AXf+cx3/+K9zvWC7r5gi1uL88l0MKinkkt/+m6r99Tz4pUn856OLAPjKuccBngV29Uc+xLodtTy3dBvPLd3GlJEDKO1TwNsbW9x1v5yzlk+ePJS6xmZEhLKjinjlvZ2MLC1mzOASnl+2jV219VzrK4VwWLn9xTUAfOeSlrje1r2HOKo4P5pM8diCzXGKJOJOTJSuG/QrH/QuCEUVyZsbdvHtZ5bzi8+ezI59dVGrcL6vBGvrm6LVHTbtOsjgksJoRYsXV1QybmhJNCGlvqmZ/3l+NeGw8vPLT0JEqNhz6DA5AO57fSNLt+zlyvvfpjAvwDnHl8ZZ1ZlAYv3MPZVJkybpokWLsi3GEcnjCz5g36EmfjHnPaaMHMAvrziZof2KojcIeE96p/90HseVFnPeCUczv3wXpX0KyA8GePm9nVx20mCK80M8E2PhANx55QQ+MnIAU37+CgD//s55VNbU8e7mPby2tor3tu/jzFED+expZZw+vD8Vew5y05NL+f0XJrK6ch8/nrWKIX0LCQWEZRU19ClsGQTak4WmqqzYWsP2mjrOHl3Kuh37ufGJJVx/9khOGFzCmCF9OPnH/wDgZ585ie/9bQXTTj+GvGAAReOUTCz/fdlYXli+jWUVNYe9VxAK8Lvpp3LR+MGAl+H1cz/43h5E4peH7l0QirpmRh/dm217D9HQHE5ZOn9g73yG9itieStZTxrWlxVba/jN5ydw+cQyVJXz//df1DU2U1mTPDYwZnAfnrvxTL7/t5Vx3/2SH1zI5+97K+reA7honOdyjO3DwN4F3Hzh8dGCqm/MOJ83y3cxd9V25q3ZCXgJJfPW7KQ5rFw8fhB/vOo0Pn/fW9EHmzk3nc3uAw2UFObx9sbqqAKK8LGxRzNvzU6G9i3kzVsv4CM/e5nG5jAzLh3DrX9dQZO/iFtzWPnilA/x6FsfADB98rHMXbWdY/v3YumWvdxy0fFU7DnEkzGxxQllfblr2qmMGFjMva+V88s53kPW8AG9eOEbZ7NgYzXXPZJ8XPvilA9x29QTk7ZJhogsVtVJKduZIjG6g70HG6Ius0S8u3kPx/TvddiT04H6JkJBb233hZt2c+aogfxlwWZ+8sJqXvv2Ryk7qhdXPbCA/XWNPHfjWe2S6VBDM01hL8Nl/Y5axg7pw5cfXkjZUb34yac7fvMl4qE33mdI30IuHj846kKJcMszy5i5uIKbLzw+6nv/83WTOXt0KUs27+H6Rxezq7ae7102hl21Dbz63k4e/NLpcW6mQw3NLNy0m90HGlj0wW4uGDuIP7y6ga9fMIpH3tzE0i010bTb804o5dW1VZw/5mh21dazvKKGXvlBTi7ry8PXTmbemh3kBQPMeHY5ew428oUPH8vA4nzufqWcsqOKeP7Gs5hyx8vUNYb5/sfHUpAX5Ad+Wvm4ISWsrtzHsH5F9C4IsXbHfgaXFPKv73w0apW8Ub6L/3hgAQAXjhvEMUf14sUV23jua2fxzzU7oudqi75FedQcauSTE4YybkgJMxdvYUPVgbg2l544mJdWbo/bV1IYilucLsLF4wfRuyCPZ5dURBXfuceX8vbGak49tl/U+inKCx428fMv/+/DfOH/FpAfDLDup5dy/aOLogotHa49czjlO2v593rPSolkRSbijOMG0Cs/yLw1O/n6+aMIBoS75q1P2BY8y/+mC0dT0ok6XqZIYjBF0vOorq1ngK90YleCzEWa/XpefYvyqGts5oPqg5wwuMXd2BXzAlSVsMLm3Qc5uk8BSzbv4fTh/dlYdYAH5m/kZ5856TAl/9KKSr76+BJuOGckt142Nu697/99BY+9vZl5N5/LqKN78/bGavbXNXH+mKOZt2YHZ48eSFi9jKiPnzTksMSJ19bu5PV1u/jBJ8YiInGpy5EJer+eu5aH39zEjEvHMHlEf256cik3XXg833jCc9v9+nMTuOK0Mt7eWM2LyysZM6QPVfvreX7ZNubcdA5bdh8krMrO/fUUhII8/OYmVJV/ravi7NEDmb3CUzR3XH4Sn590DN95djmzlm6joTnMl84YTv/ifO6at46wJlZC//yvcxg9qA+PL/iAMYP7cNqH+qOqfFB9kJmLK/j3+ipuuvB41lTuY3P1Qdbt2M+MS8fStyiPzbsPMnPxFn55xQRE4EfPraIgFOCHnxzHz2avYeueQ1xx2jHcNW8d2/fVce0Zw7n2zBEcVZzPN598lxeXVybNdvzY2EE8cE3K8T8lpkhiMEViGO1HVXlheSXnjC49LCutsTnMsi17mTS8fxtHdw376xoPq4w7b/UOvvXMMmbdeGY0Lbs9RJTWwk27Kd9Zy7TTj4kq6rc2VDP9/97m55efxPTJx9LUHKYprFTW1HHer1/jonGDGNqvyIsNTT+1S/qYjEMNzTSr0rugJVZVc7CRrzy2mLc2Vkddk585dRjTJx/L5BGeC7dvUV6XVBQ2RRKDKRLD6Flkcvb2tr2HGFRSGBfHAy94fsKgPlFLOJs0h5W/v7uVvYca+ckLq/nCh4/lZ585qcs/J11FYllbhmHkHJl0Yw6NmYAbyxnHpTf3pjsIBoTPnlZGQ1OYnfvq+OpHj8uqPBlNLhaRS0RkrYiUi8iMBO8XiMhT/vsLRGR4zHu3+vvXisjF/r5jRORVEVkjIqtE5JuZlN8wDMNl8kMBbr1sbNYXq8uYIhGRIHAPcCkwDpguIq0X7LgO2KOqo4A7gV/4x44DpgHjgUuAe/3zNQHfUtWxwEeAryU4p2EYhtGNZNIimQyUq+pGVW0AngSmtmozFXjE354JXCCezToVeFJV61X1faAcmKyqlaq6BEBV9wNrgGEZ7INhGIaRgkwqkmHAlpjXFRw+6EfbqGoTUAMMSOdY3w12KrCgC2U2DMMw2kkmFUmiaFjrFLG22iQ9VkR6A88CN6nq4SVPvTbXi8giEVlUVZW84qthGIbRcTKpSCqA2IUfyoBtbbURkRDQF9id7FgRycNTIo+r6l/b+nBVvV9VJ6nqpNLS0k52xTAMw2iLTCqShcBoERkhIvl4wfNZrdrMAq7xt68AXlFvYsssYJqf1TUCGA2848dP/gSsUdXfZFB2wzAMI00yNo9EVZtE5EZgLhAEHlTVVSJyG7BIVWfhKYU/i0g5niUyzT92lYg8DazGy9T6mqo2i8hZwNXAChFZ6n/U91R1dqb6YRiGYSTHZrYbhmEYCbESKTGISBXwQQcPHwjs6kJxcgHr85GB9fnIoDN9/pCqpgwyHxGKpDOIyKJ0NHJPwvp8ZGB9PjLojj5ndv1FwzAMo8djisQwDMPoFKZIUnN/tgXIAtbnIwPr85FBxvtsMRLDMAyjU5hFYhiGYXQKUyRtkGotlVxGRB4UkZ0isjJmX38R+aeIrPf/H+XvFxG5278Oy0VkYvYk7xhtrWPTw/tcKCLviMgyv8//4+8f4a/9s95fCyjf39/m2kC5hogEReRdEXnBf92j+ywim0RkhYgsFZFF/r5u/W2bIklAmmup5DIP463zEssM4GVVHQ287L8G7xqM9v+uB/7QTTJ2JW2tY9OT+1wPnK+qE4BTgEtE5CN4a/7c6fd5D96aQNDG2kA5yjfxlpiIcCT0+TxVPSUmzbd7f9uqan+t/oApwNyY17cCt2Zbri7u43BgZczrtcAQf3sIsNbfvg+Ynqhdrv4BzwEXHil9BnoBS4AP401MC/n7o79zvFJGU/ztkN9Osi17B/pahjdwng+8gFdJvKf3eRMwsNW+bv1tm0WSmHTWUulpDFLVSgD//9H+/h51LVqtY9Oj++y7eJYCO4F/AhuAveqt/QPx/WprbaBc4y7gO0DYfz2Ant9nBf4hIotF5Hp/X7f+tjNWtDHHSWctlSOFHnMtWq9j4xWTTtw0wb6c67OqNgOniEg/4G/A2ETN/P8532cR+QSwU1UXi8hHI7sTNO0xffY5U1W3icjRwD9F5L0kbTPSZ7NIEpPOWio9jR0iMgTA/7/T398jrkUb69j06PxVn30AAAMnSURBVD5HUNW9wGt48aF+4q39A/H9amttoFziTOBTIrIJb2nv8/EslJ7cZ1R1m/9/J94Dw2S6+bdtiiQx6ayl0tOIXRvmGrw4QmT/F/1sj48ANRGTOVcQaXMdm57c51LfEkFEioCP4QWgX8Vb+wcO73OitYFyBlW9VVXLVHU43j37iqr+Bz24zyJSLCJ9ItvARcBKuvu3ne1Akat/wGXAOjy/8n9nW54u7tsTQCXQiPeEch2eb/hlYL3/v7/fVvAy2DYAK4BJ2Za/A/09C898Xw4s9f8u6+F9Phl41+/zSuCH/v6RwDtAOfAMUODvL/Rfl/vvj8x2HzrZ/48CL/T0Pvt9W+b/rYqMVd3927aZ7YZhGEanMNeWYRiG0SlMkRiGYRidwhSJYRiG0SlMkRiGYRidwhSJYRiG0SlMkRhGBxGRZr/iauSvy6pEi8hwianObBguYyVSDKPjHFLVU7IthGFkG7NIDKOL8deH+IW/Hsg7IjLK3/8hEXnZXwfiZRE51t8/SET+5q8dskxEzvBPFRSR//PXE/mHP0MdEfmGiKz2z/NklrppGFFMkRhGxylq5dq6Mua9fao6Gfg9Xr0n/O1HVfVk4HHgbn//3cC/1Fs7ZCLeDGXw1oy4R1XHA3uBz/r7ZwCn+uf5SqY6ZxjpYjPbDaODiEitqvZOsH8T3qJSG/1ikdtVdYCI7MJb+6HR31+pqgNFpAooU9X6mHMMB/6p3sJEiMh3gTxVvV1E5gC1wN+Bv6tqbYa7ahhJMYvEMDKDtrHdVptE1MdsN9MS0/w4Xr2k04DFMZVtDSMrmCIxjMxwZcz/t/ztN/Gq0gL8BzDf334Z+CpEF6MqaeukIhIAjlHVV/EWcOoHHGYVGUZ3Yk8yhtFxivwVCCPMUdVICnCBiCzAe1ib7u/7BvCgiHwbqAKu9fd/E7hfRK7Dszy+iledORFB4DER6YtXyfVO9dYbMYysYTESw+hi/BjJJFXdlW1ZDKM7MNeWYRiG0SnMIjEMwzA6hVkkhmEYRqcwRWIYhmF0ClMkhmEYRqcwRWIYhmF0ClMkhmEYRqcwRWIYhmF0iv8fxAj1V30MnTUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_model_loss_values = merged_model_history.history['main_output_loss']\n",
    "validation_loss = merged_model_history.history['val_main_output_loss']\n",
    "merged_model_epochs = merged_model_history.epoch\n",
    "plt.plot(merged_model_epochs, merged_model_loss_values, label='Training Loss')\n",
    "# plt.plot(merged_model_epochs, validation_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute '__dict__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-999c9de5d703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reset validation data and model since they are not serializable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmerged_model_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmerged_model_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_model_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute '__dict__'"
     ]
    }
   ],
   "source": [
    "# Reset validation data and model since they are not serializable\n",
    "merged_model_history.validation_data = []\n",
    "merged_model_history.model = str(merged_model_history.model.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model_history.params[\"steps\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in merged_model_history.__dict__.items():\n",
    "    if v is None:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model params and history\n",
    "model_helper.save_model_details(merged_model_history.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../feature_extraction/data_files/1157354_checkpoints_hybrid_500e_64b_adam\n"
     ]
    }
   ],
   "source": [
    "print(checkpoints_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../feature_extraction/data_files/models/1157354df_500e_64bs_adam_342min_merged_hybrid_model.h5'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current_time = time.strftime(\"%Y-%m-%d-(%H_%M_%S)\")\n",
    "merged_model_filename = \"{}df_{}e_{}bs_{}_{}min\".format(\n",
    "    model_helper.time_diff, \n",
    "    EPOCHS, \n",
    "    BATCH_SIZE, \n",
    "    OPTIMIZER,\n",
    "    int(time_taken_in_minutes)\n",
    ")\n",
    "MODELS_DIR = join(DATA_FILES_DIR, \"models\")\n",
    "merged_model_filename = join(MODELS_DIR,  merged_model_filename + '_merged_hybrid_model.h5')\n",
    "merged_model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "merged_model.save(merged_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../app/services/chat_data_files/models/model-20-30ts.hdf5'"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_filename = join(\"..\", \"app\", \"services\", \"chat_data_files\", \"models\", \"model-20-30ts.hdf5\")\n",
    "merged_model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "loaded_merged_model = load_model(merged_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [0.07915102856518555,\n",
       "  0.07785702332633482,\n",
       "  0.07869851134659143,\n",
       "  0.08587305130099064,\n",
       "  0.07944610574576136,\n",
       "  0.08379199592902244,\n",
       "  0.08611314915285152,\n",
       "  0.08901916948724606,\n",
       "  0.10006969877882442,\n",
       "  0.1009041471881982,\n",
       "  0.1152629942487668,\n",
       "  0.1220929700628234,\n",
       "  0.12912517389904724,\n",
       "  0.1305865225839121,\n",
       "  0.13466589530432108,\n",
       "  0.14942991028089472,\n",
       "  0.1651692309260189,\n",
       "  0.16492769434258028,\n",
       "  0.1719237150976732,\n",
       "  0.16288661176052652,\n",
       "  0.16609621758764515,\n",
       "  0.1510059082495798,\n",
       "  0.16816525713610164,\n",
       "  0.1498307111300541,\n",
       "  0.17852680625064365,\n",
       "  0.20201748268348643,\n",
       "  0.17721288413266506,\n",
       "  0.1756857912846874,\n",
       "  0.14651428166774813,\n",
       "  0.15986811945044394,\n",
       "  0.1873215591583473,\n",
       "  0.2088484848489408,\n",
       "  0.22697884456259848,\n",
       "  0.1623426314136055,\n",
       "  0.2205839607641226,\n",
       "  0.16203574694228728,\n",
       "  0.1902153641797825,\n",
       "  0.204400097629727,\n",
       "  0.20702640807167594,\n",
       "  0.17062273937908098,\n",
       "  0.1994460961902967,\n",
       "  0.1764396487962689,\n",
       "  0.22806846329289873,\n",
       "  0.35590813045726083,\n",
       "  0.19515684659222585,\n",
       "  0.1632880290135674,\n",
       "  0.23037649188445855,\n",
       "  0.2033107211427471,\n",
       "  0.2275538857395892,\n",
       "  0.27511610613127213,\n",
       "  0.21729990258831097,\n",
       "  0.18580596989939074,\n",
       "  0.2278669531289712,\n",
       "  0.2632967236235442,\n",
       "  0.22330367033803866,\n",
       "  0.18579715415623183,\n",
       "  0.24362568750769534,\n",
       "  0.17613217292849276,\n",
       "  0.24972954504654998,\n",
       "  0.21442132758719243,\n",
       "  0.18679489756150064,\n",
       "  0.22547571102991962,\n",
       "  0.20791699904277058,\n",
       "  0.21169316220464163,\n",
       "  0.1826707381550207,\n",
       "  0.24261305542901754,\n",
       "  0.18908331079186175,\n",
       "  0.29127356111545083,\n",
       "  0.25190054075500823,\n",
       "  0.23909404523085376,\n",
       "  0.18475517984672335,\n",
       "  0.22570876878609963,\n",
       "  0.2617209771278853,\n",
       "  0.24117601621320103,\n",
       "  0.2073908593710788,\n",
       "  0.24101611229387726,\n",
       "  0.24553025230168588,\n",
       "  0.19414280712717,\n",
       "  0.2548708382692956,\n",
       "  0.2283806946562894,\n",
       "  0.25525300433770187,\n",
       "  0.29369413954129525,\n",
       "  0.24076179539337483,\n",
       "  0.24835416681591713,\n",
       "  0.22357032844145347,\n",
       "  0.2576940205491827,\n",
       "  0.28138863120404056,\n",
       "  0.2700648415497321,\n",
       "  0.21083677372628828,\n",
       "  0.23397987940948856,\n",
       "  0.21348808926168375,\n",
       "  0.2945784357935399,\n",
       "  0.2223925733663348,\n",
       "  0.20376788208254362,\n",
       "  0.22728520777385797,\n",
       "  0.2239040012060263,\n",
       "  0.28507266630050704,\n",
       "  0.34421007865964365,\n",
       "  0.23430924621592727,\n",
       "  0.20951611171335946,\n",
       "  0.31149388942138495,\n",
       "  0.21706165372596328,\n",
       "  0.26029378704378847,\n",
       "  0.2576183488675358,\n",
       "  0.26083521441985336,\n",
       "  0.22532370205208876,\n",
       "  0.20399599391245848,\n",
       "  0.274268121403222,\n",
       "  0.22490324528962333,\n",
       "  0.24529108941315292,\n",
       "  0.23705434249651042,\n",
       "  0.23047638299901257,\n",
       "  0.18692784945240568,\n",
       "  0.24339035287476515,\n",
       "  0.22453167235548016,\n",
       "  0.24785324814709866,\n",
       "  0.24730889092033262,\n",
       "  0.27907216725654865,\n",
       "  0.24202019221526652,\n",
       "  0.24165609520685655,\n",
       "  0.27999824553820596,\n",
       "  0.2603453602629006,\n",
       "  0.24247727288774013,\n",
       "  0.229395786842754,\n",
       "  0.34028593896879605,\n",
       "  0.27570225753208794,\n",
       "  0.23186080155882183,\n",
       "  0.22275076260926138,\n",
       "  0.22745737835523658,\n",
       "  0.24283480320853795,\n",
       "  0.24140628152296492,\n",
       "  0.2769057882029583,\n",
       "  0.23763986111646632,\n",
       "  0.2677675627359716,\n",
       "  0.29080265539372585,\n",
       "  0.25960087301279866,\n",
       "  0.2695772417591609,\n",
       "  0.2337966586208257,\n",
       "  0.23727150266919408,\n",
       "  0.22604448933998966,\n",
       "  0.20876689436555176,\n",
       "  0.26017111699860507,\n",
       "  0.30084359746239925,\n",
       "  0.22473517407903418,\n",
       "  0.2273177458145527,\n",
       "  0.2368532593556265,\n",
       "  0.24206026869601846,\n",
       "  0.3034027721067624,\n",
       "  0.2551056031512434,\n",
       "  0.2163879081300384,\n",
       "  0.2278205198140777,\n",
       "  0.23596160280835096,\n",
       "  0.24865571359265673,\n",
       "  0.21827227386953463,\n",
       "  0.26818797998883587,\n",
       "  0.24658542660430954,\n",
       "  0.1998695736977933,\n",
       "  0.22973672292036407,\n",
       "  0.24468277248865947,\n",
       "  0.20369095422553646,\n",
       "  0.22772271265052108,\n",
       "  0.22426185368555276,\n",
       "  0.26105300601595993,\n",
       "  0.19078767100344968,\n",
       "  0.2647334084258177,\n",
       "  0.3595689981389527,\n",
       "  0.25654821192701055,\n",
       "  0.3037397477214711,\n",
       "  0.2335031543655135,\n",
       "  0.26969480251234135,\n",
       "  0.22021564454726503,\n",
       "  0.2776830629075733,\n",
       "  0.25688338927765614,\n",
       "  0.215170285835615,\n",
       "  0.2525121401704504,\n",
       "  0.23935174142290613,\n",
       "  0.26935620043787245,\n",
       "  0.29077497843277,\n",
       "  0.2913638414103997,\n",
       "  0.26337128003070887,\n",
       "  0.21405951844180146,\n",
       "  0.1956552359543646,\n",
       "  0.2655342194770866,\n",
       "  0.1935659717652943,\n",
       "  0.24238234346687648,\n",
       "  0.316710135673007,\n",
       "  0.23131892052235323,\n",
       "  0.2785428278038821,\n",
       "  0.2480640468921255,\n",
       "  0.29005774405001467,\n",
       "  0.23902128569027603,\n",
       "  0.20421830160437762,\n",
       "  0.26351148772355215,\n",
       "  0.2699383298012717,\n",
       "  0.23363011993149121,\n",
       "  0.2542935221471353,\n",
       "  0.2004856992662846,\n",
       "  0.26531398776830584,\n",
       "  0.19671102878123067,\n",
       "  0.2443291580904464,\n",
       "  0.2519114884407748,\n",
       "  0.234096559085671,\n",
       "  0.2406094903781907,\n",
       "  0.24127115313951025,\n",
       "  0.2215193065209718,\n",
       "  0.19636408552196244,\n",
       "  0.2062636797902452,\n",
       "  0.2257411035779121,\n",
       "  0.2697472542859947,\n",
       "  0.25887864187457704,\n",
       "  0.21347476012623606,\n",
       "  0.21616626259195704,\n",
       "  0.28076778801141405,\n",
       "  0.28653670665174036,\n",
       "  0.2427004310816391,\n",
       "  0.21749357004997327,\n",
       "  0.22711736979449898,\n",
       "  0.28018708065084996,\n",
       "  0.22532913282782835,\n",
       "  0.2434565044330526,\n",
       "  0.23023529329701461,\n",
       "  0.23401300410115464,\n",
       "  0.2555779357567567,\n",
       "  0.2456370564782979,\n",
       "  0.24267705294350722,\n",
       "  0.2697647139003049,\n",
       "  0.24731221076828522,\n",
       "  0.21481798213081493,\n",
       "  0.25597306999797353,\n",
       "  0.2481359119573232,\n",
       "  0.2580959770252039,\n",
       "  0.2686587121350801,\n",
       "  0.2859185113872826,\n",
       "  0.2892315692632106,\n",
       "  0.2812235505234993,\n",
       "  0.29244539044428486,\n",
       "  0.3375016673849031,\n",
       "  0.2603046114167963,\n",
       "  0.2733879882487166,\n",
       "  0.2950699305537604,\n",
       "  0.30173217693622606,\n",
       "  0.280834344261385,\n",
       "  0.2988374365491312,\n",
       "  0.2561866675441325,\n",
       "  0.2685443112281494,\n",
       "  0.28507317111971675,\n",
       "  0.2685055233622639,\n",
       "  0.3238136893771495,\n",
       "  0.3337719184570085,\n",
       "  0.31662022944343277,\n",
       "  0.29389707318757213,\n",
       "  0.28737149785050425,\n",
       "  0.3157219982966114,\n",
       "  0.2926940169635309,\n",
       "  0.3434532973110813,\n",
       "  0.27725929595059556,\n",
       "  0.2857217125343104,\n",
       "  0.3012076316701823,\n",
       "  0.3372131821169339,\n",
       "  0.38712482406479964,\n",
       "  0.38868472337705073,\n",
       "  0.2625908381465759,\n",
       "  0.26295615361729024,\n",
       "  0.29169543149266863,\n",
       "  0.32092891921551453,\n",
       "  0.3013663590619607,\n",
       "  0.3082759828472196,\n",
       "  0.2962079746317356,\n",
       "  0.32374586083546847,\n",
       "  0.30419693786814533,\n",
       "  0.3183624566077555,\n",
       "  0.2987653654806141,\n",
       "  0.298925544521718,\n",
       "  0.2898330071351358,\n",
       "  0.33791682702536846,\n",
       "  0.26963453736965437,\n",
       "  0.28706304774600633,\n",
       "  0.2656375495243067,\n",
       "  0.35827545096982155,\n",
       "  0.3543190309842426,\n",
       "  0.2923568729281937,\n",
       "  0.26391572624033327,\n",
       "  0.2876339783106517,\n",
       "  0.2856464751592845,\n",
       "  0.3120154696209463,\n",
       "  0.2879445564041114,\n",
       "  0.3015090419439849,\n",
       "  0.3124887668007911,\n",
       "  0.263783886304117,\n",
       "  0.2941447064003108,\n",
       "  0.33522400000922303,\n",
       "  0.289596303800412,\n",
       "  0.3344002383551954,\n",
       "  0.32143215026657246,\n",
       "  0.294283755335812,\n",
       "  0.3274778235571702,\n",
       "  0.31024762681303436,\n",
       "  0.3259906407508895,\n",
       "  0.3199869290688074,\n",
       "  0.330582369057188,\n",
       "  0.3669410184261915,\n",
       "  0.32172721143239336,\n",
       "  0.2977808638222379,\n",
       "  0.281041091209515,\n",
       "  0.29153510334995636,\n",
       "  0.3190303637538348,\n",
       "  0.2815046979493837,\n",
       "  0.2854489899810111,\n",
       "  0.28216882822329536,\n",
       "  0.3102679527295675,\n",
       "  0.2848513158278314,\n",
       "  0.2637577594224304,\n",
       "  0.27900022248442524,\n",
       "  0.280973236492075,\n",
       "  0.26494643999342277,\n",
       "  0.24875086180475933,\n",
       "  0.2436112965777666,\n",
       "  0.26297257382965283,\n",
       "  0.26698036059314895,\n",
       "  0.2753699969795556,\n",
       "  0.25805808051254575,\n",
       "  0.2786969273351914,\n",
       "  0.28796534157202675,\n",
       "  0.2853450549402953,\n",
       "  0.23787660554303236,\n",
       "  0.31253381131562125,\n",
       "  0.28721894520062297,\n",
       "  0.35275942593555704,\n",
       "  0.3108978026882668,\n",
       "  0.293801463079952,\n",
       "  0.2790068488923204,\n",
       "  0.38773400683933906,\n",
       "  0.3109144841895957,\n",
       "  0.24277146734419125,\n",
       "  0.34273945230929204,\n",
       "  0.26095465259240647,\n",
       "  0.38019335343884414,\n",
       "  0.27802709799113057,\n",
       "  0.304586169939735,\n",
       "  0.376533986505372,\n",
       "  0.31018306590905187,\n",
       "  0.2957300315978178,\n",
       "  0.25265178389025567,\n",
       "  0.3923718967616375,\n",
       "  0.29778424306073054,\n",
       "  0.26143372193971365,\n",
       "  0.3129268827602613,\n",
       "  0.3244324962931107,\n",
       "  0.311274113135893,\n",
       "  0.31129306968955567,\n",
       "  0.2574752231493284,\n",
       "  0.3228744038916772,\n",
       "  0.23901128659203444,\n",
       "  0.3191908611650434,\n",
       "  0.3042687238411452,\n",
       "  0.2890505575467404,\n",
       "  0.26845112120925857,\n",
       "  0.2791686464972775,\n",
       "  0.27915549740353574,\n",
       "  0.2910706343136828,\n",
       "  0.3099738230456478,\n",
       "  0.3333179579554597,\n",
       "  0.284910442723393,\n",
       "  0.3012495365338533,\n",
       "  0.3515780616516749,\n",
       "  0.2709753585029404,\n",
       "  0.3563892228911202,\n",
       "  0.3153276657755285,\n",
       "  0.32805560083553686,\n",
       "  0.3487728419418163,\n",
       "  0.2880590607971682,\n",
       "  0.3534665880076481,\n",
       "  0.29800796630014614,\n",
       "  0.2845270849586258,\n",
       "  0.2818957552548653,\n",
       "  0.2941832421264182,\n",
       "  0.25560360047617353,\n",
       "  0.3216433580766927,\n",
       "  0.2922112365293105,\n",
       "  0.2557951300955636,\n",
       "  0.2631238128517114,\n",
       "  0.3102241982993594,\n",
       "  0.36460177648993747,\n",
       "  0.3453684870327068,\n",
       "  0.31773671710346624,\n",
       "  0.29384899935441655,\n",
       "  0.3800731733142455,\n",
       "  0.297141612760607,\n",
       "  0.28425393509450725,\n",
       "  0.2987078277856062,\n",
       "  0.2648667925336081,\n",
       "  0.24974289984198333,\n",
       "  0.3116129843905039,\n",
       "  0.27877224780783694,\n",
       "  0.2558247384187985,\n",
       "  0.24971533919402614,\n",
       "  0.2855536950966158,\n",
       "  0.3088855203449401,\n",
       "  0.23756581109338945,\n",
       "  0.30117110487048665,\n",
       "  0.282153140815995,\n",
       "  0.31580449426314966,\n",
       "  0.2887046701699987,\n",
       "  0.2619035829057532,\n",
       "  0.2842653711613602,\n",
       "  0.28020549443911535,\n",
       "  0.30413304533739643,\n",
       "  0.27156636050152716,\n",
       "  0.2639729261514438,\n",
       "  0.28183538335286185,\n",
       "  0.3015636343972147,\n",
       "  0.2951394096653474,\n",
       "  0.2951965219639856,\n",
       "  0.2975024577029214,\n",
       "  0.27809083232380655,\n",
       "  0.2911166843152632,\n",
       "  0.25787904360460473,\n",
       "  0.2739962426534712,\n",
       "  0.2645858362544123,\n",
       "  0.29412677328484765,\n",
       "  0.2914058924325879,\n",
       "  0.3279078364708644,\n",
       "  0.28640888429040895,\n",
       "  0.30027551064306807,\n",
       "  0.292917707805835,\n",
       "  0.2836933611752348,\n",
       "  0.30088322077062096,\n",
       "  0.30070768116416424,\n",
       "  0.321068277155529,\n",
       "  0.36875197149840255,\n",
       "  0.30506855567465757,\n",
       "  0.3172579068252606,\n",
       "  0.30759488265109497,\n",
       "  0.28136590017859275,\n",
       "  0.30043725804513,\n",
       "  0.28829754241329514,\n",
       "  0.30843066986726253,\n",
       "  0.31552263231349703,\n",
       "  0.3099261639977688,\n",
       "  0.29438541907695276,\n",
       "  0.28440354282579544,\n",
       "  0.34872963886498687,\n",
       "  0.33680428254819295,\n",
       "  0.3162387364484351,\n",
       "  0.3381467523820842,\n",
       "  0.31418834736027595,\n",
       "  0.2903973539119031,\n",
       "  0.32693159355328155,\n",
       "  0.3873587024117215,\n",
       "  0.36201042473250583,\n",
       "  0.31777644514577214,\n",
       "  0.29452440002920127,\n",
       "  0.3141832691464997,\n",
       "  0.3182713530540075,\n",
       "  0.3369727548334705,\n",
       "  0.2698421741327803,\n",
       "  0.3012118642166013,\n",
       "  0.28709561034083064,\n",
       "  0.30027380892223227,\n",
       "  0.3551494935806033,\n",
       "  0.27336538758369455,\n",
       "  0.31618641448791196,\n",
       "  0.305046446945311,\n",
       "  0.29075094725991846,\n",
       "  0.3305418560865132,\n",
       "  0.30418070786165763,\n",
       "  0.32360885060333217,\n",
       "  0.3527232624636221,\n",
       "  0.28633232470622627,\n",
       "  0.26801281092820706,\n",
       "  0.28339184574764104,\n",
       "  0.28003674577156157,\n",
       "  0.2767038841766735,\n",
       "  0.2652765055992377,\n",
       "  0.28794692044722026,\n",
       "  0.30818312207165544,\n",
       "  0.35910401449036006,\n",
       "  0.3326649920075511,\n",
       "  0.2605780816073877,\n",
       "  0.2671916074264531,\n",
       "  0.3336909077811631,\n",
       "  0.2867963750963067,\n",
       "  0.29040817965758325,\n",
       "  0.2940141535193553,\n",
       "  0.28376976929725817,\n",
       "  0.3027440070541488,\n",
       "  0.30785549819409047,\n",
       "  0.335991979298318,\n",
       "  0.29598277375447807,\n",
       "  0.3238823330167838,\n",
       "  0.3864028582103115,\n",
       "  0.30195236521928476,\n",
       "  0.31510665142653016,\n",
       "  0.2907771022767995,\n",
       "  0.28996889863807895,\n",
       "  0.3260354606972615,\n",
       "  0.3403474665175304,\n",
       "  0.3249729427849827,\n",
       "  0.3358830318062131,\n",
       "  0.25679890063417116],\n",
       " 'val_acc': [0.9823874755381604,\n",
       "  0.9823874755381604,\n",
       "  0.9821079116578139,\n",
       "  0.9818283477774672,\n",
       "  0.9823874755381604,\n",
       "  0.9804305283757339,\n",
       "  0.9793122728543472,\n",
       "  0.9793122728543472,\n",
       "  0.9737209952474141,\n",
       "  0.9773553256919206,\n",
       "  0.9742801230081074,\n",
       "  0.9706457925636007,\n",
       "  0.9703662286832542,\n",
       "  0.9765166340508806,\n",
       "  0.961420184512161,\n",
       "  0.9686888454011742,\n",
       "  0.9611406206318144,\n",
       "  0.9684092815208275,\n",
       "  0.9703662286832542,\n",
       "  0.9647749510763209,\n",
       "  0.975398378529494,\n",
       "  0.9650545150733107,\n",
       "  0.9695275370422142,\n",
       "  0.9698071009225608,\n",
       "  0.9628180041471806,\n",
       "  0.9580654179480012,\n",
       "  0.9698071009225608,\n",
       "  0.9661727707113406,\n",
       "  0.9706457925636007,\n",
       "  0.9667318982387475,\n",
       "  0.9717640480849874,\n",
       "  0.9698071009225608,\n",
       "  0.962258876153201,\n",
       "  0.9709253564439474,\n",
       "  0.9633771317912307,\n",
       "  0.9742801230081074,\n",
       "  0.9644953873126174,\n",
       "  0.9619793122728544,\n",
       "  0.9686888454011742,\n",
       "  0.9734414313670674,\n",
       "  0.9653340788370143,\n",
       "  0.9681297176404808,\n",
       "  0.9625384401501909,\n",
       "  0.9370981270386448,\n",
       "  0.9740005591277607,\n",
       "  0.9695275370422142,\n",
       "  0.9608610568681109,\n",
       "  0.9678501537601342,\n",
       "  0.9695275370422142,\n",
       "  0.9507967571756312,\n",
       "  0.9684092815208275,\n",
       "  0.9703662286832542,\n",
       "  0.9622588762698442,\n",
       "  0.9566675985462678,\n",
       "  0.969807101039204,\n",
       "  0.9726027397260274,\n",
       "  0.962258876153201,\n",
       "  0.9756779424098406,\n",
       "  0.9636566955549343,\n",
       "  0.9686888454011742,\n",
       "  0.9712049203242941,\n",
       "  0.9614201848620905,\n",
       "  0.9703662286832542,\n",
       "  0.9689684092815208,\n",
       "  0.9703662286832542,\n",
       "  0.9656136429506472,\n",
       "  0.973161867720007,\n",
       "  0.9577858543009408,\n",
       "  0.9692479733951538,\n",
       "  0.9611406208651007,\n",
       "  0.9703662289165405,\n",
       "  0.9658932068309939,\n",
       "  0.9642158235489139,\n",
       "  0.9686888456344604,\n",
       "  0.9675705901130738,\n",
       "  0.9667318982387475,\n",
       "  0.9681297178737671,\n",
       "  0.9734414316003537,\n",
       "  0.961699748625794,\n",
       "  0.9658932068309939,\n",
       "  0.962538440266834,\n",
       "  0.9644953874292606,\n",
       "  0.9650545151899539,\n",
       "  0.9622588763864873,\n",
       "  0.9717640483182737,\n",
       "  0.9636566957882206,\n",
       "  0.9614201847454473,\n",
       "  0.9630975680275272,\n",
       "  0.9675705901130738,\n",
       "  0.9684092815208275,\n",
       "  0.9714844842046407,\n",
       "  0.9549902156141173,\n",
       "  0.9670114623523804,\n",
       "  0.9653340790703006,\n",
       "  0.9692479733951538,\n",
       "  0.9672910262327271,\n",
       "  0.9577858543009408,\n",
       "  0.9547106516171275,\n",
       "  0.9661727704780543,\n",
       "  0.9675705898797875,\n",
       "  0.9563880348992074,\n",
       "  0.9706457927968871,\n",
       "  0.9647749513096072,\n",
       "  0.9689684092815208,\n",
       "  0.9636566957882206,\n",
       "  0.9667318982387475,\n",
       "  0.9698071009225608,\n",
       "  0.9611406206318144,\n",
       "  0.9670114622357373,\n",
       "  0.9650545149566676,\n",
       "  0.9686888454011742,\n",
       "  0.9670114623523804,\n",
       "  0.974559686888454,\n",
       "  0.9672910262327271,\n",
       "  0.9670114623523804,\n",
       "  0.9636566957882206,\n",
       "  0.9681297176404808,\n",
       "  0.9650545151899539,\n",
       "  0.9642158235489139,\n",
       "  0.9661727704780543,\n",
       "  0.9616997483925077,\n",
       "  0.9675705898797875,\n",
       "  0.9653340788370143,\n",
       "  0.9717640480849874,\n",
       "  0.960861056984754,\n",
       "  0.9681297178737671,\n",
       "  0.9639362596685672,\n",
       "  0.9653340790703006,\n",
       "  0.9672910259994408,\n",
       "  0.9636566957882206,\n",
       "  0.9706457927968871,\n",
       "  0.9633771319078739,\n",
       "  0.9684092815208275,\n",
       "  0.9684092817541138,\n",
       "  0.9569471626599008,\n",
       "  0.9633771319078739,\n",
       "  0.9611406208651007,\n",
       "  0.9653340790703006,\n",
       "  0.9664523343584009,\n",
       "  0.9658932065977076,\n",
       "  0.9698071011558471,\n",
       "  0.9633771319078739,\n",
       "  0.960022365343714,\n",
       "  0.9667318982387475,\n",
       "  0.9661727704780543,\n",
       "  0.9684092815208275,\n",
       "  0.9633771316745876,\n",
       "  0.9644953874292606,\n",
       "  0.9619793122728544,\n",
       "  0.9670114621190942,\n",
       "  0.9692479733951538,\n",
       "  0.9678501539934204,\n",
       "  0.9661727707113406,\n",
       "  0.9658932068309939,\n",
       "  0.9672910259994408,\n",
       "  0.9672910259994408,\n",
       "  0.9700866648029075,\n",
       "  0.9664523343584009,\n",
       "  0.9684092815208275,\n",
       "  0.9700866648029075,\n",
       "  0.9686888454011742,\n",
       "  0.9695275370422142,\n",
       "  0.9644953871959743,\n",
       "  0.9698071009225608,\n",
       "  0.9684092817541138,\n",
       "  0.9524741405743542,\n",
       "  0.9656136427173609,\n",
       "  0.9583449818283478,\n",
       "  0.9695275370422142,\n",
       "  0.9670114621190942,\n",
       "  0.9686888454011742,\n",
       "  0.9686888454011742,\n",
       "  0.9661727704780543,\n",
       "  0.9686888454011742,\n",
       "  0.9658932065977076,\n",
       "  0.9700866648029075,\n",
       "  0.9656136427173609,\n",
       "  0.962258876153201,\n",
       "  0.9619793122728544,\n",
       "  0.9650545149566676,\n",
       "  0.9692479731618675,\n",
       "  0.9728823036063741,\n",
       "  0.9642158233156276,\n",
       "  0.9714844842046407,\n",
       "  0.9672910259994408,\n",
       "  0.9642158233156276,\n",
       "  0.9633771316745876,\n",
       "  0.9614201847454473,\n",
       "  0.9675705898797875,\n",
       "  0.9619793125061407,\n",
       "  0.9650545149566676,\n",
       "  0.9726027397260274,\n",
       "  0.9619793122728544,\n",
       "  0.9611406208651007,\n",
       "  0.9661727707113406,\n",
       "  0.9672910262327271,\n",
       "  0.9709253564439474,\n",
       "  0.9591836734693877,\n",
       "  0.9712049203242941,\n",
       "  0.9684092815208275,\n",
       "  0.9586245457086945,\n",
       "  0.9642158233156276,\n",
       "  0.9658932065977076,\n",
       "  0.9639362594352809,\n",
       "  0.9625384400335477,\n",
       "  0.9695275372755005,\n",
       "  0.9661727707113406,\n",
       "  0.9642158235489139,\n",
       "  0.9628180041471806,\n",
       "  0.9689684095148071,\n",
       "  0.9706457927968871,\n",
       "  0.9664523345916872,\n",
       "  0.962538440266834,\n",
       "  0.9622588763864873,\n",
       "  0.9672910259994408,\n",
       "  0.9723231758456807,\n",
       "  0.9611406208651007,\n",
       "  0.9580654181812874,\n",
       "  0.9689684092815208,\n",
       "  0.9664523343584009,\n",
       "  0.9692479731618675,\n",
       "  0.9667318982387475,\n",
       "  0.9625384400335477,\n",
       "  0.9653340790703006,\n",
       "  0.9658932068309939,\n",
       "  0.964215823665557,\n",
       "  0.9656136430672904,\n",
       "  0.9681297178737671,\n",
       "  0.961699748625794,\n",
       "  0.9642158233156276,\n",
       "  0.9639362596685672,\n",
       "  0.9639362596685672,\n",
       "  0.9653340790703006,\n",
       "  0.9658932068309939,\n",
       "  0.9642158235489139,\n",
       "  0.9619793125061407,\n",
       "  0.954431087853424,\n",
       "  0.9661727707113406,\n",
       "  0.9656136429506472,\n",
       "  0.9611406208651007,\n",
       "  0.960861056984754,\n",
       "  0.9647749513096072,\n",
       "  0.9622588763864873,\n",
       "  0.9712049205575803,\n",
       "  0.9686888456344604,\n",
       "  0.9656136429506472,\n",
       "  0.9661727707113406,\n",
       "  0.9555493433748106,\n",
       "  0.9603019293407038,\n",
       "  0.9616997487424371,\n",
       "  0.9689684092815208,\n",
       "  0.967291026116084,\n",
       "  0.9611406207484575,\n",
       "  0.9658932067143507,\n",
       "  0.9600223654603571,\n",
       "  0.9678501541100636,\n",
       "  0.9656136428340041,\n",
       "  0.9661727708279837,\n",
       "  0.9605814932210505,\n",
       "  0.953592396212384,\n",
       "  0.9538719600927307,\n",
       "  0.9670114622357373,\n",
       "  0.967291026116084,\n",
       "  0.9661727705946974,\n",
       "  0.9614201847454473,\n",
       "  0.9667318984720338,\n",
       "  0.9675705898797875,\n",
       "  0.9681297179904103,\n",
       "  0.9656136428340041,\n",
       "  0.9678501538767773,\n",
       "  0.9628180041471806,\n",
       "  0.9664523343584009,\n",
       "  0.9647749511929641,\n",
       "  0.9684092817541138,\n",
       "  0.9619793126227838,\n",
       "  0.9664523345916872,\n",
       "  0.9650545151899539,\n",
       "  0.9667318984720338,\n",
       "  0.9547106517337707,\n",
       "  0.9561084711355039,\n",
       "  0.9681297178737671,\n",
       "  0.9700866648029075,\n",
       "  0.9664523345916872,\n",
       "  0.9656136427173609,\n",
       "  0.9628180041471806,\n",
       "  0.9667318983553906,\n",
       "  0.9647749511929641,\n",
       "  0.9636566955549343,\n",
       "  0.9692479731618675,\n",
       "  0.9653340788370143,\n",
       "  0.9616997485091509,\n",
       "  0.9656136428340041,\n",
       "  0.9600223651104277,\n",
       "  0.9636566955549343,\n",
       "  0.9639362595519241,\n",
       "  0.9636566956715774,\n",
       "  0.9642158234322707,\n",
       "  0.9650545149566676,\n",
       "  0.9644953871959743,\n",
       "  0.9647749510763209,\n",
       "  0.9600223652270709,\n",
       "  0.9644953871959743,\n",
       "  0.9681297176404808,\n",
       "  0.9670114622357373,\n",
       "  0.9675705898797875,\n",
       "  0.9630975680275272,\n",
       "  0.9675705898797875,\n",
       "  0.9650545151899539,\n",
       "  0.9661727707113406,\n",
       "  0.9633771319078739,\n",
       "  0.9661727707113406,\n",
       "  0.9675705901130738,\n",
       "  0.9639362596685672,\n",
       "  0.964215823665557,\n",
       "  0.9681297179904103,\n",
       "  0.9698071009225608,\n",
       "  0.9670114621190942,\n",
       "  0.9656136427173609,\n",
       "  0.9670114621190942,\n",
       "  0.9639362597852104,\n",
       "  0.9661727704780543,\n",
       "  0.9647749513096072,\n",
       "  0.9639362596685672,\n",
       "  0.9639362596685672,\n",
       "  0.9700866648029075,\n",
       "  0.9583449820616341,\n",
       "  0.9653340790703006,\n",
       "  0.9549902156141173,\n",
       "  0.9605814931044073,\n",
       "  0.960861056984754,\n",
       "  0.9630975680275272,\n",
       "  0.9496785017708876,\n",
       "  0.9580654181812874,\n",
       "  0.9681297178737671,\n",
       "  0.9558289072551572,\n",
       "  0.9653340790703006,\n",
       "  0.9502376296482241,\n",
       "  0.9644953874292606,\n",
       "  0.9633771319078739,\n",
       "  0.9555493433748106,\n",
       "  0.965054515306597,\n",
       "  0.9672910263493703,\n",
       "  0.9712049206742235,\n",
       "  0.9558289072551572,\n",
       "  0.9661727708279837,\n",
       "  0.9695275372755005,\n",
       "  0.9639362596685672,\n",
       "  0.9622588763864873,\n",
       "  0.964215823665557,\n",
       "  0.9653340790703006,\n",
       "  0.9695275372755005,\n",
       "  0.9619793125061407,\n",
       "  0.973161867720007,\n",
       "  0.9639362596685672,\n",
       "  0.9664523345916872,\n",
       "  0.9672910262327271,\n",
       "  0.9686888456344604,\n",
       "  0.9678501539934204,\n",
       "  0.9675705901130738,\n",
       "  0.9672910262327271,\n",
       "  0.9644953874292606,\n",
       "  0.9614201847454473,\n",
       "  0.9678501539934204,\n",
       "  0.9642158235489139,\n",
       "  0.9577858543009408,\n",
       "  0.9692479733951538,\n",
       "  0.9589041098223273,\n",
       "  0.9630975680275272,\n",
       "  0.9611406208651007,\n",
       "  0.9555493432581674,\n",
       "  0.9639362596685672,\n",
       "  0.9561084710188608,\n",
       "  0.9619793125061407,\n",
       "  0.9628180041471806,\n",
       "  0.9650545151899539,\n",
       "  0.9639362596685672,\n",
       "  0.9698071011558471,\n",
       "  0.9603019292240607,\n",
       "  0.9639362596685672,\n",
       "  0.9698071011558471,\n",
       "  0.9670114623523804,\n",
       "  0.9614201847454473,\n",
       "  0.9558289071385141,\n",
       "  0.9619793125061407,\n",
       "  0.9619793125061407,\n",
       "  0.9661727707113406,\n",
       "  0.9569471626599008,\n",
       "  0.9656136429506472,\n",
       "  0.9661727707113406,\n",
       "  0.9661727707113406,\n",
       "  0.9684092815208275,\n",
       "  0.9703662289165405,\n",
       "  0.962538440266834,\n",
       "  0.9661727707113406,\n",
       "  0.9686888456344604,\n",
       "  0.9700866650361938,\n",
       "  0.9653340790703006,\n",
       "  0.9633771319078739,\n",
       "  0.9709253566772337,\n",
       "  0.9650545151899539,\n",
       "  0.9656136429506472,\n",
       "  0.9611406208651007,\n",
       "  0.9656136429506472,\n",
       "  0.9689684095148071,\n",
       "  0.9656136429506472,\n",
       "  0.9650545151899539,\n",
       "  0.9619793125061407,\n",
       "  0.9658932068309939,\n",
       "  0.9664523345916872,\n",
       "  0.9647749513096072,\n",
       "  0.9628180041471806,\n",
       "  0.9639362596685672,\n",
       "  0.9653340790703006,\n",
       "  0.9656136429506472,\n",
       "  0.9667318984720338,\n",
       "  0.9670114623523804,\n",
       "  0.9703662289165405,\n",
       "  0.9706457927968871,\n",
       "  0.9703662289165405,\n",
       "  0.9667318984720338,\n",
       "  0.9670114623523804,\n",
       "  0.9622588763864873,\n",
       "  0.9678501539934204,\n",
       "  0.9647749513096072,\n",
       "  0.9653340790703006,\n",
       "  0.9678501539934204,\n",
       "  0.9636566957882206,\n",
       "  0.9664523345916872,\n",
       "  0.9636566957882206,\n",
       "  0.9594632375830207,\n",
       "  0.9656136429506472,\n",
       "  0.9656136429506472,\n",
       "  0.9664523345916872,\n",
       "  0.9684092817541138,\n",
       "  0.9658932068309939,\n",
       "  0.9661727707113406,\n",
       "  0.9650545151899539,\n",
       "  0.9628180041471806,\n",
       "  0.9670114623523804,\n",
       "  0.9661727707113406,\n",
       "  0.9667318984720338,\n",
       "  0.9597428014633673,\n",
       "  0.9619793125061407,\n",
       "  0.9644953874292606,\n",
       "  0.9636566957882206,\n",
       "  0.9658932068309939,\n",
       "  0.9670114623523804,\n",
       "  0.9647749513096072,\n",
       "  0.959183673702674,\n",
       "  0.9586245459419808,\n",
       "  0.9642158235489139,\n",
       "  0.9658932068309939,\n",
       "  0.9628180041471806,\n",
       "  0.9622588763864873,\n",
       "  0.959183673702674,\n",
       "  0.9656136429506472,\n",
       "  0.9692479733951538,\n",
       "  0.9672910262327271,\n",
       "  0.9661727707113406,\n",
       "  0.959183673702674,\n",
       "  0.9678501539934204,\n",
       "  0.9633771319078739,\n",
       "  0.9656136429506472,\n",
       "  0.9653340790703006,\n",
       "  0.960022365343714,\n",
       "  0.9642158235489139,\n",
       "  0.9628180041471806,\n",
       "  0.9594632375830207,\n",
       "  0.9667318982387475,\n",
       "  0.9670114621190942,\n",
       "  0.9672910262327271,\n",
       "  0.9672910263493703,\n",
       "  0.9681297178737671,\n",
       "  0.9686888454011742,\n",
       "  0.9647749510763209,\n",
       "  0.9619793125061407,\n",
       "  0.9583449820616341,\n",
       "  0.9608610571013971,\n",
       "  0.9684092815208275,\n",
       "  0.9664523347083304,\n",
       "  0.9566675988961972,\n",
       "  0.9670114623523804,\n",
       "  0.9650545151899539,\n",
       "  0.9630975680275272,\n",
       "  0.9653340790703006,\n",
       "  0.9619793125061407,\n",
       "  0.9633771319078739,\n",
       "  0.9563880348992074,\n",
       "  0.9628180041471806,\n",
       "  0.9611406208651007,\n",
       "  0.9535923960957409,\n",
       "  0.9639362596685672,\n",
       "  0.9633771319078739,\n",
       "  0.9656136429506472,\n",
       "  0.9642158235489139,\n",
       "  0.9622588763864873,\n",
       "  0.960022365343714,\n",
       "  0.9611406208651007,\n",
       "  0.9603019293407038,\n",
       "  0.9675705901130738],\n",
       " 'loss': [0.17155887914522708,\n",
       "  0.12684405958126024,\n",
       "  0.12037174616882201,\n",
       "  0.11273544662306732,\n",
       "  0.1069435404839714,\n",
       "  0.09912117305340913,\n",
       "  0.09154185088717441,\n",
       "  0.0828300605298905,\n",
       "  0.07559875381676044,\n",
       "  0.06752863723228862,\n",
       "  0.05902530548267128,\n",
       "  0.05290063249947826,\n",
       "  0.04601823441584692,\n",
       "  0.0426126902456818,\n",
       "  0.03625771810550145,\n",
       "  0.031883905143894435,\n",
       "  0.029430916063313377,\n",
       "  0.02666972019944766,\n",
       "  0.022721953024230652,\n",
       "  0.021166846262320776,\n",
       "  0.02018930337465103,\n",
       "  0.021430654314465357,\n",
       "  0.018857675714927323,\n",
       "  0.018001588174840925,\n",
       "  0.01862615076074465,\n",
       "  0.017141432739004328,\n",
       "  0.015504108179478086,\n",
       "  0.01732733782907685,\n",
       "  0.016810479470078566,\n",
       "  0.015332805246699579,\n",
       "  0.013586967594180254,\n",
       "  0.014476421700204438,\n",
       "  0.01452352235215723,\n",
       "  0.013929076750293742,\n",
       "  0.0126913450718572,\n",
       "  0.012635909501748649,\n",
       "  0.012155420189945946,\n",
       "  0.012766370143509979,\n",
       "  0.011513462430363374,\n",
       "  0.012568550318090305,\n",
       "  0.012283195943016483,\n",
       "  0.01293253350031733,\n",
       "  0.011140352318789421,\n",
       "  0.012062803457070061,\n",
       "  0.012615318791126037,\n",
       "  0.011815787364113926,\n",
       "  0.011885432021296416,\n",
       "  0.0115757346954883,\n",
       "  0.011984458161487787,\n",
       "  0.012149818350844199,\n",
       "  0.009424492288882617,\n",
       "  0.012087913857176705,\n",
       "  0.010958162016883414,\n",
       "  0.010751287829511881,\n",
       "  0.010038515543946748,\n",
       "  0.012399683698383755,\n",
       "  0.010448349872790507,\n",
       "  0.008867082207530463,\n",
       "  0.01041120112035649,\n",
       "  0.011679893706507532,\n",
       "  0.012178033190441629,\n",
       "  0.012209756045982643,\n",
       "  0.009598537470933059,\n",
       "  0.011234384112591733,\n",
       "  0.01170154423201315,\n",
       "  0.010043894921621952,\n",
       "  0.010599691411918062,\n",
       "  0.01016106320712872,\n",
       "  0.008750192456572057,\n",
       "  0.011940218564649848,\n",
       "  0.012903532709356099,\n",
       "  0.010810538912706714,\n",
       "  0.010475471789931112,\n",
       "  0.009984047603841846,\n",
       "  0.01209224690077077,\n",
       "  0.009828121132308017,\n",
       "  0.009197395618431307,\n",
       "  0.00881574602626747,\n",
       "  0.010502577510882254,\n",
       "  0.010952210123190425,\n",
       "  0.009640686786340572,\n",
       "  0.009543330411299848,\n",
       "  0.01153081952959022,\n",
       "  0.008525273730922218,\n",
       "  0.010242868524902223,\n",
       "  0.011927029960492767,\n",
       "  0.010233604769458405,\n",
       "  0.010083581531172277,\n",
       "  0.009823053741643757,\n",
       "  0.010083647905351488,\n",
       "  0.011050977781454743,\n",
       "  0.01152940993126818,\n",
       "  0.01157015673068274,\n",
       "  0.012249958758397524,\n",
       "  0.008201170185723044,\n",
       "  0.010200344773247386,\n",
       "  0.009361183969388736,\n",
       "  0.009335176145290688,\n",
       "  0.01038472535208049,\n",
       "  0.010879733014661854,\n",
       "  0.010609918370284669,\n",
       "  0.009969792410902704,\n",
       "  0.009174070329061826,\n",
       "  0.00786223319344255,\n",
       "  0.008944948634114365,\n",
       "  0.009682425485682053,\n",
       "  0.010236776031157427,\n",
       "  0.008510486025777888,\n",
       "  0.010579899347798629,\n",
       "  0.011167559423814514,\n",
       "  0.010022952802111481,\n",
       "  0.010757276144318555,\n",
       "  0.007283333629524832,\n",
       "  0.009258485929252283,\n",
       "  0.010100647762955849,\n",
       "  0.009261256165848832,\n",
       "  0.011010324276029155,\n",
       "  0.009387008324701752,\n",
       "  0.011147838323730397,\n",
       "  0.009025883737319324,\n",
       "  0.009092192030734574,\n",
       "  0.009639414624524448,\n",
       "  0.009436368410885366,\n",
       "  0.008860596319899123,\n",
       "  0.007955234998914831,\n",
       "  0.008576916515391239,\n",
       "  0.010466643567382503,\n",
       "  0.01047015303662299,\n",
       "  0.009692797785004884,\n",
       "  0.010013180196785801,\n",
       "  0.008708622640772858,\n",
       "  0.009007724256948816,\n",
       "  0.008807412471441685,\n",
       "  0.010258328009342605,\n",
       "  0.010079990756319842,\n",
       "  0.009571797415045115,\n",
       "  0.009508789678721164,\n",
       "  0.010249723538513316,\n",
       "  0.009453953712621642,\n",
       "  0.007950409944828738,\n",
       "  0.008413193093761377,\n",
       "  0.00886351634857261,\n",
       "  0.00951844798305377,\n",
       "  0.009115789016716665,\n",
       "  0.010791353293700354,\n",
       "  0.00878293467009578,\n",
       "  0.00984793241234754,\n",
       "  0.007302650982410358,\n",
       "  0.008701379558779226,\n",
       "  0.008639803095967898,\n",
       "  0.008769892882927273,\n",
       "  0.00959113070993683,\n",
       "  0.008095127343203657,\n",
       "  0.008034559467444323,\n",
       "  0.008185001947537747,\n",
       "  0.009211210754068312,\n",
       "  0.009673787351988752,\n",
       "  0.007455053998240525,\n",
       "  0.00827335619362166,\n",
       "  0.008342408434792748,\n",
       "  0.007763799614369005,\n",
       "  0.008777939519173788,\n",
       "  0.009674373066043698,\n",
       "  0.008602825344423805,\n",
       "  0.00924766575750074,\n",
       "  0.009251347473981021,\n",
       "  0.010258078491175458,\n",
       "  0.007799318498920644,\n",
       "  0.007953692749326259,\n",
       "  0.00892943657190991,\n",
       "  0.009494329056109703,\n",
       "  0.007365304128320999,\n",
       "  0.010237963942747171,\n",
       "  0.008771782901310931,\n",
       "  0.007851108033561073,\n",
       "  0.008616019763292246,\n",
       "  0.008564946296218144,\n",
       "  0.008597063114080622,\n",
       "  0.008587056915757504,\n",
       "  0.007650828377307364,\n",
       "  0.007963346075409777,\n",
       "  0.009803576395220755,\n",
       "  0.008892896880504743,\n",
       "  0.00988123078861244,\n",
       "  0.00909387286936872,\n",
       "  0.008105978794929584,\n",
       "  0.00962741680090328,\n",
       "  0.007923370073789435,\n",
       "  0.007800938473755563,\n",
       "  0.008498647451830975,\n",
       "  0.008726202788144042,\n",
       "  0.00873722031481685,\n",
       "  0.008929483125963084,\n",
       "  0.008515566659096143,\n",
       "  0.0073286741640411004,\n",
       "  0.007325160978453071,\n",
       "  0.008231168717455179,\n",
       "  0.010725104561509373,\n",
       "  0.008571709535281469,\n",
       "  0.008285752598471247,\n",
       "  0.009928373598009358,\n",
       "  0.009124328130375338,\n",
       "  0.007424387586017243,\n",
       "  0.008385762364401588,\n",
       "  0.008871183116571682,\n",
       "  0.009294692714376688,\n",
       "  0.008351167083270905,\n",
       "  0.009165673347050134,\n",
       "  0.008862779420452745,\n",
       "  0.009457490050077389,\n",
       "  0.008989164641793862,\n",
       "  0.009426329546202964,\n",
       "  0.008341600769174,\n",
       "  0.007602884415098686,\n",
       "  0.008427768974806278,\n",
       "  0.0075323108374407335,\n",
       "  0.012123650888561294,\n",
       "  0.009039160968314471,\n",
       "  0.009506924345658458,\n",
       "  0.008933134395616198,\n",
       "  0.010584921928564786,\n",
       "  0.009499887273553042,\n",
       "  0.009814494075297856,\n",
       "  0.0092063170144867,\n",
       "  0.009010942728725043,\n",
       "  0.008651173662704698,\n",
       "  0.010092744722713647,\n",
       "  0.01140277081939311,\n",
       "  0.00997615453604315,\n",
       "  0.009573851120411451,\n",
       "  0.010226893647946489,\n",
       "  0.009016053368268398,\n",
       "  0.00837881158225565,\n",
       "  0.007962174162158336,\n",
       "  0.011200231028719117,\n",
       "  0.009876632183713837,\n",
       "  0.008452197277891475,\n",
       "  0.0092983074394961,\n",
       "  0.008770453349351814,\n",
       "  0.008960861326314486,\n",
       "  0.00828555295496806,\n",
       "  0.008218129195606511,\n",
       "  0.007583787022124892,\n",
       "  0.007458800708863101,\n",
       "  0.008049771220760673,\n",
       "  0.007763625824539043,\n",
       "  0.008354635322652522,\n",
       "  0.008325131209670144,\n",
       "  0.007503823698582401,\n",
       "  0.007727965780763577,\n",
       "  0.007990124853676056,\n",
       "  0.008275778376912674,\n",
       "  0.00800444327685381,\n",
       "  0.00697151073240361,\n",
       "  0.008091667077473153,\n",
       "  0.008108475803635443,\n",
       "  0.0071275717722964135,\n",
       "  0.00896999137363628,\n",
       "  0.007163370234782669,\n",
       "  0.009110367723541868,\n",
       "  0.008047700419172679,\n",
       "  0.010043955515389004,\n",
       "  0.009824898985222178,\n",
       "  0.007962663517103322,\n",
       "  0.008875659506111933,\n",
       "  0.008081615209278562,\n",
       "  0.007396803218378086,\n",
       "  0.007570366051763425,\n",
       "  0.007231048089139306,\n",
       "  0.007591809564782424,\n",
       "  0.008450603382572421,\n",
       "  0.008782950454042081,\n",
       "  0.007692456641834897,\n",
       "  0.008536271483108895,\n",
       "  0.008936095300850032,\n",
       "  0.010306570766203628,\n",
       "  0.009503350765138401,\n",
       "  0.008323905725500456,\n",
       "  0.008270222211101828,\n",
       "  0.009109709529707456,\n",
       "  0.007737411753863488,\n",
       "  0.01078373504938421,\n",
       "  0.008162087278433088,\n",
       "  0.009217823223250871,\n",
       "  0.008325759858909376,\n",
       "  0.007731072597476158,\n",
       "  0.007678251103293914,\n",
       "  0.007197309646915614,\n",
       "  0.007998442320257012,\n",
       "  0.008826003764371938,\n",
       "  0.007692073142689368,\n",
       "  0.008909901301256148,\n",
       "  0.007399644965607724,\n",
       "  0.007827147887043124,\n",
       "  0.00820519667740359,\n",
       "  0.008778285982238333,\n",
       "  0.008265769406453414,\n",
       "  0.007907501529971847,\n",
       "  0.007856068609292487,\n",
       "  0.007281579335622687,\n",
       "  0.008155022703258224,\n",
       "  0.007309908856994312,\n",
       "  0.007370017606003182,\n",
       "  0.008692386259560697,\n",
       "  0.0077386076159315365,\n",
       "  0.007593682366429921,\n",
       "  0.00801131618322034,\n",
       "  0.007552053916558322,\n",
       "  0.007491100440510493,\n",
       "  0.007293011435479528,\n",
       "  0.008063258902728982,\n",
       "  0.009609608096977102,\n",
       "  0.007450401685329652,\n",
       "  0.007966964821732625,\n",
       "  0.008076757499801957,\n",
       "  0.009362732310291616,\n",
       "  0.00874765508085044,\n",
       "  0.00783501814586705,\n",
       "  0.00803661146420092,\n",
       "  0.0087860189901126,\n",
       "  0.008178151691009519,\n",
       "  0.008432580828818596,\n",
       "  0.007776073266225093,\n",
       "  0.009110925879586017,\n",
       "  0.008881340168565373,\n",
       "  0.007905123086758324,\n",
       "  0.00792062905492469,\n",
       "  0.008425212048182832,\n",
       "  0.00838826168420415,\n",
       "  0.008775101597763514,\n",
       "  0.00964373689373976,\n",
       "  0.009431858659878363,\n",
       "  0.009087008944750929,\n",
       "  0.009835207202629941,\n",
       "  0.00840055353058115,\n",
       "  0.009601036245358277,\n",
       "  0.007999734020614366,\n",
       "  0.00874223722168392,\n",
       "  0.008023001875456874,\n",
       "  0.00767047920014129,\n",
       "  0.007520631088006426,\n",
       "  0.00830904535960013,\n",
       "  0.00889211113265053,\n",
       "  0.007704816636103266,\n",
       "  0.008091143762886357,\n",
       "  0.00900481167611466,\n",
       "  0.008785964342611178,\n",
       "  0.008082442641345078,\n",
       "  0.008543271042031467,\n",
       "  0.009066197286111554,\n",
       "  0.008520641278760462,\n",
       "  0.008411692515228199,\n",
       "  0.007569770921610821,\n",
       "  0.008042527274942443,\n",
       "  0.0077598415167854,\n",
       "  0.009306157413525046,\n",
       "  0.00867711177115497,\n",
       "  0.00805435442845134,\n",
       "  0.009125599603116473,\n",
       "  0.008646412383415465,\n",
       "  0.00814722226559379,\n",
       "  0.007992669362474617,\n",
       "  0.008030923631897623,\n",
       "  0.009673656173256726,\n",
       "  0.008188598681191251,\n",
       "  0.007288435196736879,\n",
       "  0.007979333140803289,\n",
       "  0.008721803298826589,\n",
       "  0.00838861533325832,\n",
       "  0.008181208239217991,\n",
       "  0.009905544791389085,\n",
       "  0.009802349565095866,\n",
       "  0.01081363496440066,\n",
       "  0.010252692667139561,\n",
       "  0.009138052995320719,\n",
       "  0.008522184054207968,\n",
       "  0.008895111555275908,\n",
       "  0.008353415547172794,\n",
       "  0.007905211679577992,\n",
       "  0.008423052272180785,\n",
       "  0.008629772300090962,\n",
       "  0.008377597576231228,\n",
       "  0.008091558014001316,\n",
       "  0.008708798751234084,\n",
       "  0.009982049977888355,\n",
       "  0.008579560621651376,\n",
       "  0.008863895627883391,\n",
       "  0.008672590533040076,\n",
       "  0.009810798882984317,\n",
       "  0.00909666217480431,\n",
       "  0.008942703050160794,\n",
       "  0.00871075467242955,\n",
       "  0.010381508474822455,\n",
       "  0.008663414430056816,\n",
       "  0.009313224972838057,\n",
       "  0.00843493478560553,\n",
       "  0.007957149928977858,\n",
       "  0.00878559786193395,\n",
       "  0.009825520725147316,\n",
       "  0.008755629254998495,\n",
       "  0.009084775662831574,\n",
       "  0.009330789100551158,\n",
       "  0.009647800510078245,\n",
       "  0.00893197143437918,\n",
       "  0.00866082493753931,\n",
       "  0.009327868148428156,\n",
       "  0.01005638685124273,\n",
       "  0.01047888455404331,\n",
       "  0.00936795202033905,\n",
       "  0.009260948776808123,\n",
       "  0.009433778691058955,\n",
       "  0.008603649334095593,\n",
       "  0.008055084695543006,\n",
       "  0.0090223982866756,\n",
       "  0.009370073663019689,\n",
       "  0.008333451285801138,\n",
       "  0.009043661156692452,\n",
       "  0.008796304254018252,\n",
       "  0.009024236312291082,\n",
       "  0.00848005864364899,\n",
       "  0.010026216789635823,\n",
       "  0.008994790121850412,\n",
       "  0.0085968234349314,\n",
       "  0.010245802143964635,\n",
       "  0.007942325169921865,\n",
       "  0.008497294190840254,\n",
       "  0.00834171182345073,\n",
       "  0.007662502368002683,\n",
       "  0.009243728743537872,\n",
       "  0.008519134441956426,\n",
       "  0.00836832103923405,\n",
       "  0.008207134441512262,\n",
       "  0.008153488845166723,\n",
       "  0.00863717904353016,\n",
       "  0.009485030856679903,\n",
       "  0.010001093039593885,\n",
       "  0.008484520957120474,\n",
       "  0.009444659482299039,\n",
       "  0.007251692371073285,\n",
       "  0.008824512565242712,\n",
       "  0.008428660271929568,\n",
       "  0.008552672714184867,\n",
       "  0.008719172611504929,\n",
       "  0.008207612893707354,\n",
       "  0.00818356968787829,\n",
       "  0.008558575680539018,\n",
       "  0.009286973578274293,\n",
       "  0.007901637430901125,\n",
       "  0.009975722650621044,\n",
       "  0.010372381011870687,\n",
       "  0.008076560571284764,\n",
       "  0.007957527014869016,\n",
       "  0.008361567960052396,\n",
       "  0.008996120369872521,\n",
       "  0.009168444296668276,\n",
       "  0.00848871823970383,\n",
       "  0.007938895850896766,\n",
       "  0.00921151688205094,\n",
       "  0.009553239035981742,\n",
       "  0.008783474857525498,\n",
       "  0.009681208729641554,\n",
       "  0.009851358845237003,\n",
       "  0.008611622479985483,\n",
       "  0.008602506651302655,\n",
       "  0.008081343796164448,\n",
       "  0.008402558312645848,\n",
       "  0.007965423137800617,\n",
       "  0.008415531891110488,\n",
       "  0.008836267613287926,\n",
       "  0.00870307556712326,\n",
       "  0.008308854572448924,\n",
       "  0.008839923212011313,\n",
       "  0.007940158769472737,\n",
       "  0.007904379071773092,\n",
       "  0.009223881457973076,\n",
       "  0.008935823523940156,\n",
       "  0.008367214346512193,\n",
       "  0.008908581218437175,\n",
       "  0.009398263911886267,\n",
       "  0.008980408531651642,\n",
       "  0.009145011311550696,\n",
       "  0.008633137882975473,\n",
       "  0.008629412843612513,\n",
       "  0.009099529816171839,\n",
       "  0.008627705602065178,\n",
       "  0.009146864451820806,\n",
       "  0.008999970760933696,\n",
       "  0.010396111756910745,\n",
       "  0.008787973377435747,\n",
       "  0.009259235162977322,\n",
       "  0.008349770699668862,\n",
       "  0.008231727734647828,\n",
       "  0.008329416678776656,\n",
       "  0.008636690274448904,\n",
       "  0.009436124833517214,\n",
       "  0.008072618814144571,\n",
       "  0.008979794705200632,\n",
       "  0.00878074025536608,\n",
       "  0.008128750937641793,\n",
       "  0.008121963086950628],\n",
       " 'acc': [0.9624990897837327,\n",
       "  0.9661399548966756,\n",
       "  0.9661399548532731,\n",
       "  0.9662127721242822,\n",
       "  0.966868127793397,\n",
       "  0.9685429258425724,\n",
       "  0.9693439161578713,\n",
       "  0.9731304157867909,\n",
       "  0.9741498579324781,\n",
       "  0.9784460788317226,\n",
       "  0.979247069103619,\n",
       "  0.9832520206801136,\n",
       "  0.9835432898856769,\n",
       "  0.9855093570232287,\n",
       "  0.9881307798732979,\n",
       "  0.9890045875333902,\n",
       "  0.9899512124080682,\n",
       "  0.990606568163988,\n",
       "  0.9922085487945858,\n",
       "  0.9930823564112755,\n",
       "  0.9932279909706546,\n",
       "  0.9927182698305372,\n",
       "  0.9938833466831719,\n",
       "  0.9941017985873444,\n",
       "  0.9936648947789994,\n",
       "  0.9943930678363101,\n",
       "  0.9951940581082065,\n",
       "  0.9943930677929076,\n",
       "  0.9941746159321376,\n",
       "  0.9946843370418733,\n",
       "  0.9953396927109881,\n",
       "  0.9953396927109881,\n",
       "  0.9951212408068157,\n",
       "  0.9950484235054249,\n",
       "  0.9959950484235054,\n",
       "  0.9959222311221146,\n",
       "  0.9954853273137697,\n",
       "  0.9958494138641263,\n",
       "  0.9953396927109881,\n",
       "  0.995776596519333,\n",
       "  0.9959950484235054,\n",
       "  0.995776596519333,\n",
       "  0.9959950484235054,\n",
       "  0.9956309619165514,\n",
       "  0.9958494138207238,\n",
       "  0.9960678657248963,\n",
       "  0.9962135003276779,\n",
       "  0.9962135003276779,\n",
       "  0.9954853273137697,\n",
       "  0.9957037792179422,\n",
       "  0.996941673341586,\n",
       "  0.9963591349304595,\n",
       "  0.9965775868780344,\n",
       "  0.996941673341586,\n",
       "  0.9967232214374135,\n",
       "  0.996140683026287,\n",
       "  0.9966504041794252,\n",
       "  0.9967232214374135,\n",
       "  0.9962135003276779,\n",
       "  0.9962135003710804,\n",
       "  0.9960678657248963,\n",
       "  0.9960678657248963,\n",
       "  0.9970144906429768,\n",
       "  0.9962135003276779,\n",
       "  0.996359134973862,\n",
       "  0.9966504041360227,\n",
       "  0.9960678657248963,\n",
       "  0.996941673341586,\n",
       "  0.9966504041360227,\n",
       "  0.9967232214374135,\n",
       "  0.9959222311221146,\n",
       "  0.996359134973862,\n",
       "  0.9967960387388043,\n",
       "  0.9965047695332411,\n",
       "  0.9966504041360227,\n",
       "  0.996359134973862,\n",
       "  0.996941673341586,\n",
       "  0.9970144906429768,\n",
       "  0.9968688560401952,\n",
       "  0.9965047695332411,\n",
       "  0.9966504041360227,\n",
       "  0.9970144906429768,\n",
       "  0.9967960387388043,\n",
       "  0.9968688560401952,\n",
       "  0.9966504040622385,\n",
       "  0.9966504041360227,\n",
       "  0.9966504041360227,\n",
       "  0.9962863176290687,\n",
       "  0.9970144906429768,\n",
       "  0.9965775868346319,\n",
       "  0.9965047695332411,\n",
       "  0.9965047695332411,\n",
       "  0.9964319522318503,\n",
       "  0.9967232214374135,\n",
       "  0.9971601252457584,\n",
       "  0.9967960387388043,\n",
       "  0.9969416733849884,\n",
       "  0.9970144906429768,\n",
       "  0.996941673341586,\n",
       "  0.996941673341586,\n",
       "  0.9965047695332411,\n",
       "  0.9967960387388043,\n",
       "  0.9968688560401952,\n",
       "  0.9974513944513216,\n",
       "  0.996941673341586,\n",
       "  0.99730575984854,\n",
       "  0.9971601252457584,\n",
       "  0.9972329425471492,\n",
       "  0.9968688560401952,\n",
       "  0.9967232214374135,\n",
       "  0.9965047695332411,\n",
       "  0.9968688560401952,\n",
       "  0.9973785771499308,\n",
       "  0.9968688560401952,\n",
       "  0.996941673341586,\n",
       "  0.9970144906429768,\n",
       "  0.9971601252891609,\n",
       "  0.9972329425471492,\n",
       "  0.9965047695332411,\n",
       "  0.9971601252457584,\n",
       "  0.9967960387388043,\n",
       "  0.9967232214374135,\n",
       "  0.996941673341586,\n",
       "  0.9971601252457584,\n",
       "  0.99730575984854,\n",
       "  0.9971601252457584,\n",
       "  0.9971601252457584,\n",
       "  0.9972329425471492,\n",
       "  0.9971601252891609,\n",
       "  0.9970144906429768,\n",
       "  0.9971601252457584,\n",
       "  0.99730575984854,\n",
       "  0.9972329425471492,\n",
       "  0.9968688560401952,\n",
       "  0.9968688560401952,\n",
       "  0.9972329425471492,\n",
       "  0.9970873079443676,\n",
       "  0.9967960387822068,\n",
       "  0.9975970290541033,\n",
       "  0.9972329425471492,\n",
       "  0.9971601252457584,\n",
       "  0.9970144906429768,\n",
       "  0.9971601252457584,\n",
       "  0.9971601252457584,\n",
       "  0.9966504041360227,\n",
       "  0.99730575984854,\n",
       "  0.996941673341586,\n",
       "  0.9972329425471492,\n",
       "  0.9971601252457584,\n",
       "  0.997524211796115,\n",
       "  0.9974513944513216,\n",
       "  0.9970144906429768,\n",
       "  0.9973785771499308,\n",
       "  0.9973057598919425,\n",
       "  0.9974513944513216,\n",
       "  0.9967232214374135,\n",
       "  0.9968688560835977,\n",
       "  0.9974513944513216,\n",
       "  0.9974513944947241,\n",
       "  0.9972329425471492,\n",
       "  0.9975242117527124,\n",
       "  0.99730575984854,\n",
       "  0.9967960387388043,\n",
       "  0.9974513944513216,\n",
       "  0.9972329425471492,\n",
       "  0.9971601252457584,\n",
       "  0.996941673341586,\n",
       "  0.997669846355494,\n",
       "  0.9975970290541033,\n",
       "  0.9971601252457584,\n",
       "  0.9975242117527124,\n",
       "  0.9975970290541033,\n",
       "  0.9973785771499308,\n",
       "  0.9972329425471492,\n",
       "  0.9973785771499308,\n",
       "  0.9974513944513216,\n",
       "  0.9973785771499308,\n",
       "  0.9974513944513216,\n",
       "  0.9971601252457584,\n",
       "  0.9975970290541033,\n",
       "  0.9977426636568849,\n",
       "  0.9970873079443676,\n",
       "  0.9975242117527124,\n",
       "  0.9971601252457584,\n",
       "  0.9974513944513216,\n",
       "  0.9974513944513216,\n",
       "  0.9972329425905517,\n",
       "  0.9975242117527124,\n",
       "  0.9974513944513216,\n",
       "  0.9975970290541033,\n",
       "  0.99730575984854,\n",
       "  0.9972329425471492,\n",
       "  0.99730575984854,\n",
       "  0.9971601252457584,\n",
       "  0.9972329425471492,\n",
       "  0.9973785771499308,\n",
       "  0.9975970290541033,\n",
       "  0.9970873079443676,\n",
       "  0.99730575984854,\n",
       "  0.9971601252457584,\n",
       "  0.9970873079443676,\n",
       "  0.9971601252457584,\n",
       "  0.9975970290541033,\n",
       "  0.99730575984854,\n",
       "  0.9975242117527124,\n",
       "  0.9972329425471492,\n",
       "  0.9973785771499308,\n",
       "  0.9971601252457584,\n",
       "  0.9971601252891609,\n",
       "  0.9971601252457584,\n",
       "  0.9972329425471492,\n",
       "  0.9970144906429768,\n",
       "  0.9971601252457584,\n",
       "  0.9973785771499308,\n",
       "  0.9975970290541033,\n",
       "  0.9973785771499308,\n",
       "  0.9967232214374135,\n",
       "  0.9972329425471492,\n",
       "  0.9970873079443676,\n",
       "  0.9973785771499308,\n",
       "  0.9970873079443676,\n",
       "  0.9974513944513216,\n",
       "  0.996941673341586,\n",
       "  0.9971601252457584,\n",
       "  0.9970873079443676,\n",
       "  0.99730575984854,\n",
       "  0.9972329425471492,\n",
       "  0.9969416733849884,\n",
       "  0.9970144906429768,\n",
       "  0.9970873079443676,\n",
       "  0.99730575984854,\n",
       "  0.9973785771499308,\n",
       "  0.9973785771499308,\n",
       "  0.9974513944513216,\n",
       "  0.9970144906429768,\n",
       "  0.9970144906429768,\n",
       "  0.9973785771499308,\n",
       "  0.9970144906429768,\n",
       "  0.9974513944513216,\n",
       "  0.9972329425905517,\n",
       "  0.9970873079443676,\n",
       "  0.9974513944513216,\n",
       "  0.9975242117527124,\n",
       "  0.9975242117527124,\n",
       "  0.9975242117527124,\n",
       "  0.9974513944513216,\n",
       "  0.997669846355494,\n",
       "  0.9973785771499308,\n",
       "  0.997524211796115,\n",
       "  0.9977426636568849,\n",
       "  0.9975242117527124,\n",
       "  0.997669846355494,\n",
       "  0.9974513944947241,\n",
       "  0.9977426636568849,\n",
       "  0.997669846355494,\n",
       "  0.9975970290541033,\n",
       "  0.9973785771499308,\n",
       "  0.997669846355494,\n",
       "  0.9975242117527124,\n",
       "  0.9974513944947241,\n",
       "  0.997669846355494,\n",
       "  0.9970873079443676,\n",
       "  0.9973785771499308,\n",
       "  0.9975242117527124,\n",
       "  0.99730575984854,\n",
       "  0.9973785771499308,\n",
       "  0.9974513944513216,\n",
       "  0.99730575984854,\n",
       "  0.9973785771499308,\n",
       "  0.9975970290541033,\n",
       "  0.9975970290541033,\n",
       "  0.9974513944947241,\n",
       "  0.9975970290541033,\n",
       "  0.99730575984854,\n",
       "  0.9973785771499308,\n",
       "  0.9971601252457584,\n",
       "  0.99730575984854,\n",
       "  0.997669846355494,\n",
       "  0.9975242117527124,\n",
       "  0.9975970290975057,\n",
       "  0.9975242117527124,\n",
       "  0.9970873079877701,\n",
       "  0.9975242117527124,\n",
       "  0.99730575984854,\n",
       "  0.9974513944513216,\n",
       "  0.9977426636568849,\n",
       "  0.9975242117527124,\n",
       "  0.9977426636568849,\n",
       "  0.9973785771499308,\n",
       "  0.9974513944513216,\n",
       "  0.9975242117527124,\n",
       "  0.99730575984854,\n",
       "  0.9973785771499308,\n",
       "  0.99730575984854,\n",
       "  0.9973785771499308,\n",
       "  0.9972329425471492,\n",
       "  0.9973785771499308,\n",
       "  0.9977426636568849,\n",
       "  0.9975970290541033,\n",
       "  0.9977426636568849,\n",
       "  0.9974513944513216,\n",
       "  0.9975970290541033,\n",
       "  0.9974513944513216,\n",
       "  0.9973785771499308,\n",
       "  0.997669846355494,\n",
       "  0.9975970290541033,\n",
       "  0.9975242117527124,\n",
       "  0.9974513944513216,\n",
       "  0.9976698463988966,\n",
       "  0.9975242117527124,\n",
       "  0.9974513944513216,\n",
       "  0.9973785771499308,\n",
       "  0.9975242117527124,\n",
       "  0.9978154809582757,\n",
       "  0.9977426636568849,\n",
       "  0.9974513944513216,\n",
       "  0.9973785771499308,\n",
       "  0.9974513944947241,\n",
       "  0.9975970290541033,\n",
       "  0.9975970290541033,\n",
       "  0.9975970290975057,\n",
       "  0.9975970290541033,\n",
       "  0.9973785771499308,\n",
       "  0.9975242117527124,\n",
       "  0.9974513944513216,\n",
       "  0.997669846355494,\n",
       "  0.997669846355494,\n",
       "  0.9975970290541033,\n",
       "  0.9975970290541033,\n",
       "  0.9974513944513216,\n",
       "  0.9972329425471492,\n",
       "  0.9973785771933333,\n",
       "  0.9975242117527124,\n",
       "  0.99730575984854,\n",
       "  0.9975242117527124,\n",
       "  0.9972329425471492,\n",
       "  0.9975970290541033,\n",
       "  0.9975242117527124,\n",
       "  0.9975970290541033,\n",
       "  0.9975242117527124,\n",
       "  0.9975970290541033,\n",
       "  0.9975970290975057,\n",
       "  0.997669846355494,\n",
       "  0.997669846355494,\n",
       "  0.9975970290541033,\n",
       "  0.9975970290541033,\n",
       "  0.997669846355494,\n",
       "  0.9975970290541033,\n",
       "  0.9975970290541033,\n",
       "  0.9975970290541033,\n",
       "  0.9975242117527124,\n",
       "  0.9974513944513216,\n",
       "  0.9975242117527124,\n",
       "  0.997669846355494,\n",
       "  0.9977426636568849,\n",
       "  0.9975970290541033,\n",
       "  0.9975242117527124,\n",
       "  0.9977426636568849,\n",
       "  0.9975242117527124,\n",
       "  0.9975970290541033,\n",
       "  0.997669846355494,\n",
       "  0.9977426636568849,\n",
       "  0.9974513944513216,\n",
       "  0.9975242117527124,\n",
       "  0.9975970290541033,\n",
       "  0.9978154809582757,\n",
       "  0.9977426636568849,\n",
       "  0.9975242117527124,\n",
       "  0.9975970290541033,\n",
       "  0.997669846355494,\n",
       "  0.9975242117527124,\n",
       "  0.9975242117527124,\n",
       "  0.9972329425471492,\n",
       "  0.997524211796115,\n",
       "  0.9974513944513216,\n",
       "  0.9973785771499308,\n",
       "  0.9975970290541033,\n",
       "  0.9975242117527124,\n",
       "  0.9975970290541033,\n",
       "  0.9974513944513216,\n",
       "  0.9975242117527124,\n",
       "  0.9975242117527124,\n",
       "  0.9975970290541033,\n",
       "  0.997669846355494,\n",
       "  0.9973785771499308,\n",
       "  0.9975242117527124,\n",
       "  0.9975242117527124,\n",
       "  0.9975242117527124,\n",
       "  0.9973785771499308,\n",
       "  0.9974513944513216,\n",
       "  0.9973785771499308,\n",
       "  0.9975970290975057,\n",
       "  0.9972329425905517,\n",
       "  0.9975242117527124,\n",
       "  0.9974513944513216,\n",
       "  0.9974513944513216,\n",
       "  0.9977426636568849,\n",
       "  0.997669846355494,\n",
       "  0.9975970290541033,\n",
       "  0.9974513944513216,\n",
       "  0.9974513944513216,\n",
       "  0.9971601252457584,\n",
       "  0.9971601252891609,\n",
       "  0.9974513944513216,\n",
       "  0.9973785771499308,\n",
       "  0.9971601252457584,\n",
       "  0.9972329425471492,\n",
       "  0.9973785771499308,\n",
       "  0.9974513944947241,\n",
       "  0.9975242117527124,\n",
       "  0.9973785771933333,\n",
       "  0.9973785771499308,\n",
       "  0.9975970290975057,\n",
       "  0.9975242117527124,\n",
       "  0.9974513944513216,\n",
       "  0.9975970290541033,\n",
       "  0.9975970290541033,\n",
       "  0.9974513944513216,\n",
       "  0.9975242117527124,\n",
       "  0.9974513944513216,\n",
       "  0.99730575984854,\n",
       "  0.99730575984854,\n",
       "  0.9975242117527124,\n",
       "  0.99730575984854,\n",
       "  0.9974513944513216,\n",
       "  0.9973785771499308,\n",
       "  0.9974513944513216,\n",
       "  0.9977426637002874,\n",
       "  0.9975242117527124,\n",
       "  0.9975970290541033,\n",
       "  0.997669846355494,\n",
       "  0.9975242117527124,\n",
       "  0.9975970290541033,\n",
       "  0.9975970290541033,\n",
       "  0.9973785771499308,\n",
       "  0.9973785771499308,\n",
       "  0.9975242116789282,\n",
       "  0.9974513944513216,\n",
       "  0.9977426636568849,\n",
       "  0.9974513944513216,\n",
       "  0.9975242117527124,\n",
       "  0.997524211796115,\n",
       "  0.9973785771933333,\n",
       "  0.9975970290541033,\n",
       "  0.9977426636568849,\n",
       "  0.9974513944513216,\n",
       "  0.9974513944513216,\n",
       "  0.9978154809582757,\n",
       "  0.9974513944513216,\n",
       "  0.9973785771499308,\n",
       "  0.9975242117527124,\n",
       "  0.9975242117527124,\n",
       "  0.997669846355494,\n",
       "  0.9975242117527124,\n",
       "  0.9973785771499308,\n",
       "  0.997669846355494,\n",
       "  0.9978154809582757,\n",
       "  0.9975242117527124,\n",
       "  0.9975242117527124,\n",
       "  0.997669846355494,\n",
       "  0.9975242117527124,\n",
       "  0.9972329425471492,\n",
       "  0.9975970290541033,\n",
       "  0.9974513944513216,\n",
       "  0.9975970290541033,\n",
       "  0.9975242117527124,\n",
       "  0.997669846355494,\n",
       "  0.9977426636568849,\n",
       "  0.9975970290541033,\n",
       "  0.997669846355494,\n",
       "  0.9975970290541033,\n",
       "  0.9977426636568849,\n",
       "  0.9977426637002874,\n",
       "  0.9977426636568849,\n",
       "  0.9975242117527124,\n",
       "  0.9975242117527124,\n",
       "  0.9975242117527124,\n",
       "  0.9978154809582757,\n",
       "  0.9974513944513216,\n",
       "  0.9973785771499308,\n",
       "  0.9975970290541033,\n",
       "  0.9975970290541033,\n",
       "  0.9977426636568849,\n",
       "  0.99730575984854,\n",
       "  0.9974513944513216,\n",
       "  0.9973785771499308,\n",
       "  0.9974513944513216,\n",
       "  0.9971601252457584,\n",
       "  0.9975970290541033,\n",
       "  0.9974513944513216,\n",
       "  0.9974513944513216,\n",
       "  0.9975242117527124,\n",
       "  0.9978154810016782,\n",
       "  0.997669846355494,\n",
       "  0.9971601252457584,\n",
       "  0.9977426636568849,\n",
       "  0.9972329425471492,\n",
       "  0.9975970290541033,\n",
       "  0.997669846355494,\n",
       "  0.9975970290541033]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_dir = join(\"..\", \"app\", \"services\", \"chat_data_files\", \"models\")\n",
    "model_details = model_helper.load_model_details(\n",
    "    checkpoints_dir=cp_dir, \n",
    "    model_details_filename=\"model_history.json\"\n",
    ")\n",
    "model_details[\"history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# merged_model_loss_values = model_details['history']['main_output_loss']\n",
    "# validation_loss = model_details['history']['val_main_output_loss']\n",
    "merged_model_loss_values = model_details['history']['loss']\n",
    "validation_loss = model_details['history']['val_loss']\n",
    "merged_model_epochs = model_details['epoch']\n",
    "plt.plot(merged_model_epochs, merged_model_loss_values, label='Training Loss')\n",
    "plt.plot(merged_model_epochs, validation_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.31162995,  1.7198551 , -0.26658484, ...,  0.26755935,\n",
       "         -0.25971437,  0.25458497],\n",
       "        [ 0.7395397 , -0.04442636,  0.01110798, ..., -0.33860877,\n",
       "          0.2671296 ,  1.0992235 ],\n",
       "        [ 0.06799658,  0.12074675, -1.2979059 , ...,  0.4600182 ,\n",
       "         -1.2451937 ,  0.1859076 ],\n",
       "        ...,\n",
       "        [-0.6919389 ,  2.9159625 , -1.628642  , ..., -0.8936053 ,\n",
       "          1.2516016 , -0.8342527 ],\n",
       "        [ 0.95484793, -0.20202481, -0.02437298, ..., -0.3512085 ,\n",
       "         -0.79204888, -0.02689505],\n",
       "        [ 0.25896859, -0.6449706 , -0.23375711, ...,  0.15994361,\n",
       "          0.01313286,  0.11158396]]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vectors = np.array(keras_lstm.get_sentence_vectors(\"test\"))\n",
    "# test_vectors = test_vectors.reshape((test_vectors.shape[0], 1, test_vectors.shape[1]))\n",
    "test_vectors = keras_lstm.create_time_steps(np.array(test_vectors), keras_lstm.time_steps)\n",
    "test_vectors[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.43083695, -0.49769064,  0.1643314 , ..., -0.4650444 ,\n",
       "         -0.58196433,  0.02166187],\n",
       "        [ 1.15913795,  0.22453519,  0.33209156, ...,  0.01474681,\n",
       "         -0.91038723,  0.26871113],\n",
       "        [ 0.960371  , -0.21186292,  0.07468702, ..., -1.5586987 ,\n",
       "         -0.596789  , -0.91772795],\n",
       "        ...,\n",
       "        [ 0.53636575, -0.11782386,  0.00602838, ...,  0.22712874,\n",
       "          0.33111453, -0.38606647],\n",
       "        [-0.02151288,  1.2117786 , -0.42026448, ..., -0.66378164,\n",
       "         -0.99202967,  0.5660049 ],\n",
       "        [-0.23964867,  0.1461363 , -0.3329837 , ..., -0.34872812,\n",
       "         -0.11727129, -1.2159463 ]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_vectors = np.array(keras_lstm.get_sentence_vectors(\"validation\"))\n",
    "# validation_vectors = validation_vectors.reshape((validation_vectors.shape[0], 1, validation_vectors.shape[1]))\n",
    "validation_vectors = keras_lstm.create_time_steps(np.array(validation_vectors), keras_lstm.time_steps)\n",
    "validation_vectors[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# test_y_predictions = loaded_merged_model.predict([test_sequences_X, test_features_X])\n",
    "# validation_y_predictions = loaded_merged_model.predict([validation_sequences_X, validation_features_X])\n",
    "\n",
    "test_y_predictions = loaded_merged_model.predict([test_vectors])\n",
    "validation_y_predictions = loaded_merged_model.predict([validation_vectors])\n",
    "\n",
    "test_y_argmax = np.argmax(test_y,axis=1)\n",
    "validation_y_argmax = np.argmax(validation_y,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9990702e-01 9.3017137e-05]\n",
      " [9.9990809e-01 9.1895250e-05]\n",
      " [9.9991012e-01 8.9836569e-05]\n",
      " ...\n",
      " [9.9989915e-01 1.0087567e-04]\n",
      " [9.9990737e-01 9.2634662e-05]\n",
      " [9.9990964e-01 9.0402551e-05]]\n",
      "[9.3017137e-05 9.1895250e-05 8.9836569e-05 ... 1.0087567e-04 9.2634662e-05\n",
      " 9.0402551e-05]\n",
      "(3577,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = .45\n",
    "print(test_y_predictions)\n",
    "print(test_y_predictions[:,1])\n",
    "test_y_predictions_argmax = (test_y_predictions[:,1] > threshold).astype(int)      \n",
    "validation_y_predictions_argmax = (validation_y_predictions[:,1] > threshold).astype(int)\n",
    "\n",
    "# test_y_predictions_argmax = np.argmax(test_y_predictions, axis=1)\n",
    "# validation_y_predictions_argmax = np.argmax(validation_y_predictions, axis=1)\n",
    "\n",
    "print(validation_y_predictions_argmax.shape)\n",
    "print(type(validation_y_predictions_argmax))\n",
    "test_y_predictions_argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99907017e-01, 9.30171373e-05],\n",
       "       [9.99908090e-01, 9.18952501e-05],\n",
       "       [9.99910116e-01, 8.98365688e-05],\n",
       "       [9.99883652e-01, 1.16282681e-04],\n",
       "       [9.99883533e-01, 1.16431365e-04],\n",
       "       [9.99857187e-01, 1.42782956e-04],\n",
       "       [9.99853730e-01, 1.46286417e-04],\n",
       "       [9.99785483e-01, 2.14488959e-04],\n",
       "       [9.99880314e-01, 1.19625147e-04],\n",
       "       [9.99891639e-01, 1.08371190e-04]], dtype=float32)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_predictions_argmax[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_argmax[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97      3282\n",
      "           1       0.19      0.12      0.15       123\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      3405\n",
      "   macro avg       0.58      0.55      0.56      3405\n",
      "weighted avg       0.94      0.95      0.94      3405\n",
      "\n",
      "[[3219   63]\n",
      " [ 108   15]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y_argmax, test_y_predictions_argmax))\n",
    "print(confusion_matrix(test_y_argmax, test_y_predictions_argmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "pycharm": {},
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARIES\n",
      "---------\n",
      " where you planning a single exception raised to the app level or should I make forms handle the various errors at the app level\n",
      "it's because the event system can't keep up with the movement\n",
      "I'm going to make the provider='odbc' and take all the special meaning out of it\n",
      "what's next big ticket in designer?\n",
      "need to clean up the UI system\n",
      "... add drag and drop support for automatically pulling fields/datasources\n",
      "... generalize the designer code so that it can be used w/reports/etc\n",
      "... use BOA (if installed) for code editing\n",
      "insert a duplicate key into table\n",
      "gnuef used to generate pop up dialogs and recover when a dberror was encountered\n",
      "say I query 10 records\n",
      "I have a resultset with 10 recordsets\n",
      "I change the first 3 \n",
      "which causes the _updateFlag's to be set on those records\n",
      "and I insert a new record at the end\n",
      "I go to commit the changes\n",
      "and it performs the updates on the first 3\n",
      "and resets the _updateFlag (since they've been updated)\n",
      "then inserts the last one\n",
      "but it generates an error\n",
      "so\n",
      "PREDICTIONS\n",
      "-----------\n",
      " btw - welcome to the living hell I've been messing with since yesterday\n",
      "well, at least labels work :)\n",
      "I can't find anything consistant\n",
      ":)\n",
      "I honestly don't know how to get entries to work\n",
      "I couldn't trap the keystrokes reliably\n",
      "I think this is a big problem w/wxWindows\n",
      "also\n",
      "it WON'T let go of some events\n",
      "on the labels\n",
      "need to clean up the UI system\n",
      "... add drag and drop support for automatically pulling fields/datasources\n",
      "... generalize the designer code so that it can be used w/reports/etc\n",
      "... use BOA (if installed) for code editing\n",
      "and I switch to win32\n",
      "I have two options here\n",
      "1.  Each recordset, when asked to commit, can save their state and implement a method that can be called that will restore that state, or\n",
      "I was hoping for more than a smiley :)\n",
      "i was thinking it'd hold the records in memory\n",
      "so you could correct the problem and resubmit\n",
      "what's the standard for a dsn?\n",
      "how do they look normally?\n",
      "I used them in VB and IIRC they seperated everything out into specific arguemnts\n",
      "am try\n",
      "ROUGE Scores\n",
      "------------\n",
      "Evaluation with Avg\n",
      "\trouge-1:\tP: 0.533981\tR: 0.523810\tF1: 0.528846\n",
      "\trouge-2:\tP: 0.372549\tR: 0.365385\tF1: 0.368932\n",
      "\trouge-3:\tP: 0.346535\tR: 0.339806\tF1: 0.343137\n",
      "\trouge-4:\tP: 0.340000\tR: 0.333333\tF1: 0.336634\n",
      "\trouge-l:\tP: 0.592841\tR: 0.583415\tF1: 0.588090\n",
      "\trouge-w:\tP: 0.327245\tR: 0.192465\tF1: 0.242378\n"
     ]
    }
   ],
   "source": [
    "from rouge_metrics import *\n",
    "from get_sentences_from_line_numbers import *\n",
    "\n",
    "predicted_chat_logs = [test_chat_logs[index] for index, value in enumerate(test_y_predictions_argmax) if value==1]\n",
    "summaries_chat_logs = [test_chat_logs[index] for index, value in enumerate(test_y_argmax) if value==1]\n",
    "\n",
    "predicted_chat_logs = \"\".join(log for log in predicted_chat_logs)\n",
    "summaries_chat_logs = \"\".join(log for log in summaries_chat_logs)\n",
    "print(\"SUMMARIES\\n---------\\n\", summaries_chat_logs[:1000])\n",
    "print(\"PREDICTIONS\\n-----------\\n\", predicted_chat_logs[:1000])\n",
    "\n",
    "hypotheses = [predicted_chat_logs]\n",
    "references = [summaries_chat_logs]\n",
    "print(\"ROUGE Scores\\n------------\")\n",
    "print_rouge_results(get_rouge_results(hypotheses, references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      3513\n",
      "           1       0.09      0.12      0.11        64\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      3577\n",
      "   macro avg       0.54      0.55      0.54      3577\n",
      "weighted avg       0.97      0.96      0.97      3577\n",
      "\n",
      "[[3434   79]\n",
      " [  56    8]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_y_argmax, validation_y_predictions_argmax))\n",
    "print(confusion_matrix(validation_y_argmax, validation_y_predictions_argmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
