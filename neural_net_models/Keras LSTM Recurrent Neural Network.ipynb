{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras for Word Embedding as well as the LSTM\n",
    "\n",
    "https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470\n",
    "\n",
    "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "https://keras.io/getting-started/functional-api-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from keras.layers import Bidirectional, Dense, Dropout, Input, LSTM, TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_DIR = join(\"..\", \"feature_extraction\", \"feature_outputs\")\n",
    "DATA_FILES_DIR = join(\"..\", \"feature_extraction\", \"data_files\")\n",
    "chat_logs_filename = join(DATA_FILES_DIR, \"gnue_irc_chat_logs_preprocessed.txt\")\n",
    "summarized_chat_date_partitions_filename = join(\n",
    "    DATA_FILES_DIR, \"summarized_chat_date_partitions_cumulative_count.csv\"\n",
    ")\n",
    "summarized_chat_log_ids_filename = join(DATA_FILES_DIR, \"summarized_chat_log_ids.csv\")\n",
    "summarized_chat_features_filename = join(FEATURES_DIR, \"summarized_chats_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_rows = []\n",
    "# def keep(rows): \n",
    "#     return x in rows\n",
    "summarized_chat_date_partitions = pd.read_csv(\n",
    "    summarized_chat_date_partitions_filename, skiprows=lambda x: x in keep_rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_log_id</th>\n",
       "      <th>date_of_log</th>\n",
       "      <th>chat_line_count</th>\n",
       "      <th>cumulative_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85350</td>\n",
       "      <td>2001-11-05</td>\n",
       "      <td>2295</td>\n",
       "      <td>2295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92248</td>\n",
       "      <td>2001-11-02</td>\n",
       "      <td>163</td>\n",
       "      <td>2458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137598</td>\n",
       "      <td>2001-11-03</td>\n",
       "      <td>347</td>\n",
       "      <td>2805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144080</td>\n",
       "      <td>2001-11-04</td>\n",
       "      <td>846</td>\n",
       "      <td>3651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189748</td>\n",
       "      <td>2001-11-17</td>\n",
       "      <td>1259</td>\n",
       "      <td>4910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>196981</td>\n",
       "      <td>2001-11-10</td>\n",
       "      <td>439</td>\n",
       "      <td>5349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>216561</td>\n",
       "      <td>2001-11-11</td>\n",
       "      <td>448</td>\n",
       "      <td>5797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>222782</td>\n",
       "      <td>2001-11-16</td>\n",
       "      <td>1132</td>\n",
       "      <td>6929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>236439</td>\n",
       "      <td>2001-11-18</td>\n",
       "      <td>255</td>\n",
       "      <td>7184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>270720</td>\n",
       "      <td>2001-10-29</td>\n",
       "      <td>481</td>\n",
       "      <td>7665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>274197</td>\n",
       "      <td>2001-10-27</td>\n",
       "      <td>653</td>\n",
       "      <td>8318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>298160</td>\n",
       "      <td>2001-10-26</td>\n",
       "      <td>393</td>\n",
       "      <td>8711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>309914</td>\n",
       "      <td>2001-10-28</td>\n",
       "      <td>191</td>\n",
       "      <td>8902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>346408</td>\n",
       "      <td>2001-10-31</td>\n",
       "      <td>604</td>\n",
       "      <td>9506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>386768</td>\n",
       "      <td>2001-10-30</td>\n",
       "      <td>220</td>\n",
       "      <td>9726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>414983</td>\n",
       "      <td>2001-11-01</td>\n",
       "      <td>893</td>\n",
       "      <td>10619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>419096</td>\n",
       "      <td>2001-11-06</td>\n",
       "      <td>627</td>\n",
       "      <td>11246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>430623</td>\n",
       "      <td>2001-11-08</td>\n",
       "      <td>2043</td>\n",
       "      <td>13289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>449747</td>\n",
       "      <td>2001-11-09</td>\n",
       "      <td>800</td>\n",
       "      <td>14089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>473197</td>\n",
       "      <td>2001-11-07</td>\n",
       "      <td>1990</td>\n",
       "      <td>16079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>495175</td>\n",
       "      <td>2001-11-13</td>\n",
       "      <td>1051</td>\n",
       "      <td>17130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>497259</td>\n",
       "      <td>2001-11-14</td>\n",
       "      <td>536</td>\n",
       "      <td>17666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>526307</td>\n",
       "      <td>2001-11-15</td>\n",
       "      <td>1053</td>\n",
       "      <td>18719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>531627</td>\n",
       "      <td>2001-11-12</td>\n",
       "      <td>1348</td>\n",
       "      <td>20067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>579591</td>\n",
       "      <td>2001-10-24</td>\n",
       "      <td>162</td>\n",
       "      <td>20229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>586206</td>\n",
       "      <td>2001-10-23</td>\n",
       "      <td>165</td>\n",
       "      <td>20394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>623684</td>\n",
       "      <td>2001-10-25</td>\n",
       "      <td>321</td>\n",
       "      <td>20715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    min_log_id date_of_log  chat_line_count  cumulative_count\n",
       "0        85350  2001-11-05             2295              2295\n",
       "1        92248  2001-11-02              163              2458\n",
       "2       137598  2001-11-03              347              2805\n",
       "3       144080  2001-11-04              846              3651\n",
       "4       189748  2001-11-17             1259              4910\n",
       "5       196981  2001-11-10              439              5349\n",
       "6       216561  2001-11-11              448              5797\n",
       "7       222782  2001-11-16             1132              6929\n",
       "8       236439  2001-11-18              255              7184\n",
       "9       270720  2001-10-29              481              7665\n",
       "10      274197  2001-10-27              653              8318\n",
       "11      298160  2001-10-26              393              8711\n",
       "12      309914  2001-10-28              191              8902\n",
       "13      346408  2001-10-31              604              9506\n",
       "14      386768  2001-10-30              220              9726\n",
       "15      414983  2001-11-01              893             10619\n",
       "16      419096  2001-11-06              627             11246\n",
       "17      430623  2001-11-08             2043             13289\n",
       "18      449747  2001-11-09              800             14089\n",
       "19      473197  2001-11-07             1990             16079\n",
       "20      495175  2001-11-13             1051             17130\n",
       "21      497259  2001-11-14              536             17666\n",
       "22      526307  2001-11-15             1053             18719\n",
       "23      531627  2001-11-12             1348             20067\n",
       "24      579591  2001-10-24              162             20229\n",
       "25      586206  2001-10-23              165             20394\n",
       "26      623684  2001-10-25              321             20715"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_chat_date_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16079\n",
      "20067\n"
     ]
    }
   ],
   "source": [
    "index_for_validation_test_split = summarized_chat_date_partitions.tail(4)[\"cumulative_count\"]\n",
    "index_for_validation_test_split = index_for_validation_test_split.values[0]\n",
    "index_for_train_validation_split = summarized_chat_date_partitions.tail(8)[\"cumulative_count\"]\n",
    "index_for_train_validation_split = index_for_train_validation_split.values[0]\n",
    "print(index_for_train_validation_split)\n",
    "print(index_for_validation_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data using pandas\n",
    "unnormalized_summarized_chat_features_df = pd.read_csv(summarized_chat_features_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_id</th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>mean_tf_isf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "      <th>is_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20710</th>\n",
       "      <td>624000</td>\n",
       "      <td>0.987539</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.983457</td>\n",
       "      <td>0.281268</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20711</th>\n",
       "      <td>624001</td>\n",
       "      <td>0.990654</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>0.444846</td>\n",
       "      <td>0.127226</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20712</th>\n",
       "      <td>624002</td>\n",
       "      <td>0.993769</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.008603</td>\n",
       "      <td>0.306102</td>\n",
       "      <td>0.087545</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20713</th>\n",
       "      <td>624003</td>\n",
       "      <td>0.996885</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20714</th>\n",
       "      <td>624004</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.006161</td>\n",
       "      <td>2.205475</td>\n",
       "      <td>0.630764</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       log_id  absolute_sentence_position  sentence_length  \\\n",
       "20710  624000                    0.987539                2   \n",
       "20711  624001                    0.990654                4   \n",
       "20712  624002                    0.993769                7   \n",
       "20713  624003                    0.996885                0   \n",
       "20714  624004                    1.000000                1   \n",
       "\n",
       "       number_of_special_terms  sentiment_score  mean_tf_idf  \\\n",
       "20710                        0           0.0000     0.000432   \n",
       "20711                        0           0.0000     0.000801   \n",
       "20712                        0           0.5267     0.001122   \n",
       "20713                        0           0.4588     0.000000   \n",
       "20714                        0           0.0000     0.000803   \n",
       "\n",
       "       normalized_mean_tf_idf  mean_tf_isf  normalized_mean_tf_isf  is_summary  \n",
       "20710                0.003309     0.983457                0.281268           0  \n",
       "20711                0.006145     0.444846                0.127226           0  \n",
       "20712                0.008603     0.306102                0.087545           0  \n",
       "20713                0.000000     0.000000                0.000000           0  \n",
       "20714                0.006161     2.205475                0.630764           0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unnormalized_summarized_chat_features_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the chat features data\n",
    "\n",
    "We will use Standardization instead\n",
    "52\n",
    "\n",
    "Normalization rescales the values into a range of [0,1]. This might be useful in some cases where all parameters need to have the same positive scale. However, the outliers from the data set are lost.\n",
    "\n",
    "ùëãùëê‚Ñéùëéùëõùëîùëíùëë=ùëã‚àíùëãùëöùëñùëõùëãùëöùëéùë•‚àíùëãùëöùëñùëõ\n",
    "\n",
    "Standardization rescales data to have a mean (ùúá) of 0 and standard deviation (ùúé) of 1 (unit variance).\n",
    "\n",
    "ùëãùëê‚Ñéùëéùëõùëîùëíùëë=ùëã‚àíùúáùúé\n",
    "\n",
    "For most applications standardization is recommended.\n",
    "https://stats.stackexchange.com/questions/10289/whats-the-difference-between-normalization-and-standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9529\n",
      "0.9707\n",
      "0.00043572984749455336\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.04073392898973263\n",
      "3.36078268987328\n",
      "0.3123448778466237\n",
      "0.9611808193277522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3123448778466237"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_data_max_values = unnormalized_summarized_chat_features_df.max()\n",
    "chat_data_min_values = unnormalized_summarized_chat_features_df.min()\n",
    "\n",
    "max_number_of_special_terms = chat_data_max_values.number_of_special_terms\n",
    "max_sentence_length = chat_data_max_values.sentence_length\n",
    "max_sentiment_score = chat_data_max_values.sentiment_score\n",
    "max_absolute_sentence_position = chat_data_max_values.absolute_sentence_position\n",
    "max_mean_tf_idf = chat_data_max_values.mean_tf_idf\n",
    "max_mean_tf_isf = chat_data_max_values.mean_tf_isf\n",
    "max_normalized_mean_tf_idf = chat_data_max_values.normalized_mean_tf_idf\n",
    "max_normalized_mean_tf_isf = chat_data_max_values.normalized_mean_tf_isf\n",
    "\n",
    "min_number_of_special_terms = chat_data_min_values.number_of_special_terms\n",
    "min_sentence_length = chat_data_min_values.sentence_length\n",
    "min_sentiment_score = chat_data_min_values.sentiment_score\n",
    "min_absolute_sentence_position = chat_data_min_values.absolute_sentence_position\n",
    "min_mean_tf_idf = chat_data_min_values.mean_tf_idf\n",
    "min_mean_tf_isf = chat_data_min_values.mean_tf_isf\n",
    "min_normalized_mean_tf_idf = chat_data_min_values.normalized_mean_tf_idf\n",
    "min_normalized_mean_tf_isf = chat_data_min_values.normalized_mean_tf_isf\n",
    "\n",
    "print(min_sentiment_score)\n",
    "print(max_sentiment_score)\n",
    "print(min_absolute_sentence_position)\n",
    "print(max_absolute_sentence_position)\n",
    "print(min_mean_tf_idf)\n",
    "print(min_mean_tf_isf)\n",
    "print(min_normalized_mean_tf_idf)\n",
    "print(min_normalized_mean_tf_isf)\n",
    "print(max_mean_tf_idf)\n",
    "print(max_mean_tf_isf)\n",
    "print(max_normalized_mean_tf_idf)\n",
    "print(max_normalized_mean_tf_isf)\n",
    "max_sentence_length\n",
    "chat_data_max_values[\"normalized_mean_tf_idf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.013699\n",
      "1    0.178082\n",
      "2    0.136986\n",
      "3    0.068493\n",
      "4    0.109589\n",
      "Name: sentence_length, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_id</th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>mean_tf_isf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "      <th>is_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20710</th>\n",
       "      <td>624000</td>\n",
       "      <td>0.987539</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.983457</td>\n",
       "      <td>0.281268</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20711</th>\n",
       "      <td>624001</td>\n",
       "      <td>0.990654</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>0.444846</td>\n",
       "      <td>0.127226</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20712</th>\n",
       "      <td>624002</td>\n",
       "      <td>0.993769</td>\n",
       "      <td>0.095890</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.008603</td>\n",
       "      <td>0.306102</td>\n",
       "      <td>0.087545</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20713</th>\n",
       "      <td>624003</td>\n",
       "      <td>0.996885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20714</th>\n",
       "      <td>624004</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.006161</td>\n",
       "      <td>2.205475</td>\n",
       "      <td>0.630764</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       log_id  absolute_sentence_position  sentence_length  \\\n",
       "20710  624000                    0.987539         0.027397   \n",
       "20711  624001                    0.990654         0.054795   \n",
       "20712  624002                    0.993769         0.095890   \n",
       "20713  624003                    0.996885         0.000000   \n",
       "20714  624004                    1.000000         0.013699   \n",
       "\n",
       "       number_of_special_terms  sentiment_score  mean_tf_idf  \\\n",
       "20710                      0.0           0.0000     0.000432   \n",
       "20711                      0.0           0.0000     0.000801   \n",
       "20712                      0.0           0.5267     0.001122   \n",
       "20713                      0.0           0.4588     0.000000   \n",
       "20714                      0.0           0.0000     0.000803   \n",
       "\n",
       "       normalized_mean_tf_idf  mean_tf_isf  normalized_mean_tf_isf  is_summary  \n",
       "20710                0.003309     0.983457                0.281268           0  \n",
       "20711                0.006145     0.444846                0.127226           0  \n",
       "20712                0.008603     0.306102                0.087545           0  \n",
       "20713                0.000000     0.000000                0.000000           0  \n",
       "20714                0.006161     2.205475                0.630764           0  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_log_df = unnormalized_summarized_chat_features_df.copy()\n",
    "chat_log_df.sentence_length = (\n",
    "    chat_log_df.sentence_length - min_sentence_length) / (\n",
    "    max_sentence_length - min_sentence_length)\n",
    "chat_log_df.number_of_special_terms = (\n",
    "    chat_log_df.number_of_special_terms - min_number_of_special_terms) / (\n",
    "    max_number_of_special_terms - min_number_of_special_terms)\n",
    "print(chat_log_df.sentence_length.head())\n",
    "chat_log_df.iloc[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data_set(data_set_df, features_to_normalize=()):\n",
    "    \"\"\"\n",
    "    Normalize data in the data set\n",
    "    :param data_set_df: A pandas data frame containing the data to be normalized\n",
    "    :param features_to_normalize: The names of the features / columns to normalize in the data frame\n",
    "    :return: new data frame with normalized_values\n",
    "    \"\"\"\n",
    "    data_max_values = data_set_df.max()\n",
    "    data_min_values = data_set_df.min()\n",
    "\n",
    "    if not features_to_normalize:\n",
    "        features_to_normalize = [\n",
    "            \"number_of_special_terms\",\n",
    "            \"sentence_length\",\n",
    "            \"sentiment_score\",\n",
    "            \"absolute_sentence_position\",\n",
    "            \"mean_tf_idf\",\n",
    "            \"mean_tf_isf\",\n",
    "            \"normalized_mean_tf_idf\",\n",
    "            \"normalized_mean_tf_isf\",\n",
    "        ]\n",
    "\n",
    "    new_data_set_df = data_set_df.copy()\n",
    "\n",
    "    for feature in features_to_normalize:\n",
    "        new_data_set_df[feature] = (\n",
    "            new_data_set_df[feature] - data_min_values[feature]) / (\n",
    "            data_max_values[feature] - data_min_values[feature]\n",
    "        )\n",
    "\n",
    "    return new_data_set_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_id</th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>mean_tf_isf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "      <th>is_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20710</th>\n",
       "      <td>624000</td>\n",
       "      <td>0.987534</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495373</td>\n",
       "      <td>0.010594</td>\n",
       "      <td>0.010594</td>\n",
       "      <td>0.292627</td>\n",
       "      <td>0.292627</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20711</th>\n",
       "      <td>624001</td>\n",
       "      <td>0.990650</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495373</td>\n",
       "      <td>0.019673</td>\n",
       "      <td>0.019673</td>\n",
       "      <td>0.132364</td>\n",
       "      <td>0.132364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20712</th>\n",
       "      <td>624002</td>\n",
       "      <td>0.993767</td>\n",
       "      <td>0.095890</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.769183</td>\n",
       "      <td>0.027545</td>\n",
       "      <td>0.027545</td>\n",
       "      <td>0.091081</td>\n",
       "      <td>0.091081</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20713</th>\n",
       "      <td>624003</td>\n",
       "      <td>0.996883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.733884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20714</th>\n",
       "      <td>624004</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495373</td>\n",
       "      <td>0.019725</td>\n",
       "      <td>0.019725</td>\n",
       "      <td>0.656239</td>\n",
       "      <td>0.656239</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       log_id  absolute_sentence_position  sentence_length  \\\n",
       "20710  624000                    0.987534         0.027397   \n",
       "20711  624001                    0.990650         0.054795   \n",
       "20712  624002                    0.993767         0.095890   \n",
       "20713  624003                    0.996883         0.000000   \n",
       "20714  624004                    1.000000         0.013699   \n",
       "\n",
       "       number_of_special_terms  sentiment_score  mean_tf_idf  \\\n",
       "20710                      0.0         0.495373     0.010594   \n",
       "20711                      0.0         0.495373     0.019673   \n",
       "20712                      0.0         0.769183     0.027545   \n",
       "20713                      0.0         0.733884     0.000000   \n",
       "20714                      0.0         0.495373     0.019725   \n",
       "\n",
       "       normalized_mean_tf_idf  mean_tf_isf  normalized_mean_tf_isf  is_summary  \n",
       "20710                0.010594     0.292627                0.292627           0  \n",
       "20711                0.019673     0.132364                0.132364           0  \n",
       "20712                0.027545     0.091081                0.091081           0  \n",
       "20713                0.000000     0.000000                0.000000           0  \n",
       "20714                0.019725     0.656239                0.656239           0  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_data_set(unnormalized_summarized_chat_features_df).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolute_sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>number_of_special_terms</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>normalized_mean_tf_idf</th>\n",
       "      <th>normalized_mean_tf_isf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20710</th>\n",
       "      <td>0.987539</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.281268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20711</th>\n",
       "      <td>0.990654</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>0.127226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20712</th>\n",
       "      <td>0.993769</td>\n",
       "      <td>0.095890</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>0.008603</td>\n",
       "      <td>0.087545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20713</th>\n",
       "      <td>0.996885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20714</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006161</td>\n",
       "      <td>0.630764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       absolute_sentence_position  sentence_length  number_of_special_terms  \\\n",
       "20710                    0.987539         0.027397                      0.0   \n",
       "20711                    0.990654         0.054795                      0.0   \n",
       "20712                    0.993769         0.095890                      0.0   \n",
       "20713                    0.996885         0.000000                      0.0   \n",
       "20714                    1.000000         0.013699                      0.0   \n",
       "\n",
       "       sentiment_score  normalized_mean_tf_idf  normalized_mean_tf_isf  \n",
       "20710           0.0000                0.003309                0.281268  \n",
       "20711           0.0000                0.006145                0.127226  \n",
       "20712           0.5267                0.008603                0.087545  \n",
       "20713           0.4588                0.000000                0.000000  \n",
       "20714           0.0000                0.006161                0.630764  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove unneeded columns. Keep only normalized columns\n",
    "columns_to_drop = [\"log_id\", \"is_summary\", \"mean_tf_idf\", \"mean_tf_isf\"]\n",
    "chat_log_df = chat_log_df.drop(columns=columns_to_drop)\n",
    "chat_log_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(648, 6)\n",
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "[11.   2.5]\n",
      "[11.   2.5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wcyn/anaconda3/envs/gnue-irc/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/wcyn/anaconda3/envs/gnue-irc/lib/python3.6/site-packages/ipykernel_launcher.py:19: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.707220</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.668043</td>\n",
       "      <td>-1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.742270</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.296908</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  1.707220  0.333333\n",
       "1 -0.668043 -1.666667\n",
       "2 -0.742270  1.000000\n",
       "3 -0.296908  0.333333"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_X = chat_log_df.iloc[:index_for_train_validation_split]\n",
    "validation_features_X = chat_log_df.iloc[index_for_train_validation_split:index_for_validation_test_split]\n",
    "test_features_X = chat_log_df.iloc[index_for_validation_test_split:]\n",
    "train_features_X.tail()\n",
    "\n",
    "assert train_features_X.shape[1] == test_features_X.shape[1] \n",
    "assert test_features_X.shape[1] == validation_features_X.shape[1] \n",
    "print(test_features_X.shape)\n",
    "test_features_X.head()\n",
    "\n",
    "pd.DataFrame([1,2,3,4])\n",
    "c = unnormalized_summarized_chat_features_df\n",
    "d = [[34, 3], [2, 0], [1, 4], [7, 3]]\n",
    "ddf = pd.DataFrame(d)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit(ddf))\n",
    "print(scaler.mean_)\n",
    "st = pd.DataFrame(scaler.transform(ddf), columns=ddf.columns.values)\n",
    "print(scaler.mean_)\n",
    "ddf.columns.values\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_chat_log_ids = pd.read_csv(\n",
    "    summarized_chat_log_ids_filename,\n",
    "    names=[\"log_id\", \"is_summary\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_id</th>\n",
       "      <th>is_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85351</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85352</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85353</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85354</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   log_id  is_summary\n",
       "0   85350           0\n",
       "1   85351           0\n",
       "2   85352           0\n",
       "3   85353           0\n",
       "4   85354           0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_chat_log_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summarized_chat_logs(chat_logs_filename, summarized_chat_log_ids):\n",
    "    summarized_chat_log_ids = set(summarized_chat_log_ids)\n",
    "    chats = []\n",
    "    with open(chat_logs_filename) as chat_logs:\n",
    "        line_number = 1\n",
    "        for chat_log in chat_logs:\n",
    "            if line_number in summarized_chat_log_ids:\n",
    "                chats.append(chat_log)\n",
    "            line_number += 1\n",
    "    return chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great\\n',\n",
       " 'i dont recall you ever telling me what you do at your job\\n',\n",
       " \"and you probably can't be persuaded to tell me now.\\n\",\n",
       " 'due to your persistant funkitude\\n',\n",
       " 'umm, he sits around on irc all day ;)\\n']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_logs = get_summarized_chat_logs(chat_logs_filename, summarized_chat_log_ids.log_id.values)\n",
    "chat_logs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Tokenizer Object\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=None,\n",
    "    filters=\"\\n\",\n",
    "    lower = False, \n",
    "    split = ' '\n",
    ")\n",
    "tokenizer.fit_on_texts(chat_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(chat_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3099, 72, 4137, 300, 16, 584, 34, 309, 54]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexes = tokenizer.index_word\n",
    "def get_words_at_line_number(word_indexes, line_number):\n",
    "    return \" \".join(word_indexes[word] for word in sequences[line_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"and you probably can't be persuaded to tell me now.\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_words_at_line_number(word_indexes, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'language'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word[356]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16079,)\n",
      "(648,)\n",
      "[list([547])]\n",
      "648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([list([107, 18365]), list([128, 9, 31, 1456]),\n",
       "       list([18366, 342, 36, 3591, 515, 29, 18367, 14]), list([14]),\n",
       "       list([1454])], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = np.array(sequences[:index_for_train_validation_split])\n",
    "validation_X = np.array(sequences[index_for_train_validation_split:index_for_validation_test_split])\n",
    "test_X = np.array(sequences[index_for_validation_test_split:])\n",
    "print(train_X.shape)\n",
    "print(test_X.shape)\n",
    "print(train_X[:1])\n",
    "print(len(test_X))\n",
    "test_X[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a one hot encoding for the target column\n",
    "chat_log_labels = to_categorical(summarized_chat_log_ids.is_summary)\n",
    "\n",
    "train_y = chat_log_labels[:index_for_train_validation_split]\n",
    "validation_y = chat_log_labels[index_for_train_validation_split:index_for_validation_test_split]\n",
    "test_y = chat_log_labels[index_for_validation_test_split:]\n",
    "\n",
    "s = pd.DataFrame([0, 1, 0, 0 ,0 ,0, 1,1,0])\n",
    "type(s)\n",
    "to_categorical(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[80:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad input sequences\n",
    "max_chat_length = max(len(seq) for seq in sequences)\n",
    "train_X = sequence.pad_sequences(train_X, maxlen=max_chat_length)\n",
    "validation_X = sequence.pad_sequences(validation_X, maxlen=max_chat_length)\n",
    "test_X = sequence.pad_sequences(test_X, maxlen=max_chat_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16079, 73)\n",
      "(648, 73)\n",
      "(3988, 73)\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     4  1575    20   103    15     3 15875   908    12   125  1051\n",
      "    14]\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(test_X.shape)\n",
    "print(validation_X.shape)\n",
    "print(validation_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "top_words = 100000\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_chat_length))\n",
    "model.add(Bidirectional(LSTM(100), input_shape=(max_chat_length, 1)))\n",
    "# model.add((LSTM(100)))\n",
    "# model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns a tensor\n",
    "# inputs = Input(shape=(784,))\n",
    "# # a layer instance is callable on a tensor, and returns a tensor\n",
    "# x = Dense(64, activation='relu')(inputs)\n",
    "# x = Dense(64, activation='relu')(x)\n",
    "# predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# # This creates a model that includes\n",
    "# # the Input layer and three Dense layers\n",
    "# model = Model(inputs=inputs, outputs=predictions)\n",
    "# model.compile(optimizer='rmsprop',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# model.fit(data, labels)  # starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence input: meant to receive sequences of `max_chat_length` integers, between 1 and `top_words`.\n",
    "main_sentence_input = Input(shape=(max_chat_length,), dtype='int32', name='main_sentence_input')\n",
    "\n",
    "# This embedding layer will encode the input sequence\n",
    "# into a sequence of dense 512-dimensional vectors.\n",
    "x = Embedding(output_dim=512, input_dim=top_words, input_length=max_chat_length)(main_sentence_input)\n",
    "\n",
    "# An LSTM will transform the vector sequence into a single vector,\n",
    "# containing information about the entire sequence\n",
    "lstm_out = LSTM(32)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we insert the auxiliary loss, allowing the LSTM and Embedding layer \n",
    "# to be trained smoothly even though the main loss will be much higher in the model.\n",
    "auxiliary_output = Dense(2, activation='sigmoid', name='aux_output')(lstm_out)\n",
    "\n",
    "# At this point, we feed into the model our auxiliary input data by \n",
    "# concatenating it with the LSTM output\n",
    "num_of_feature_columns = test_features_X.shape[1]\n",
    "sentence_features_input = Input(shape=(num_of_feature_columns,), name='sentence_features_input')\n",
    "merged_input_and_output = keras.layers.concatenate([lstm_out, sentence_features_input])\n",
    "\n",
    "# We stack a deep densely-connected network on top\n",
    "x = Dense(64, activation='relu')(merged_input_and_output)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# And finally we add the main logistic regression layer\n",
    "main_output = Dense(2, activation='sigmoid', name='main_output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model with two inputs and two outputs\n",
    "merged_model = Model(\n",
    "    inputs=[main_sentence_input, sentence_features_input], \n",
    "    outputs=[main_output, auxiliary_output]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile the model and assign a weight of 0.2 to the auxiliary loss. To specify different loss_weights or loss for each different output, you can use a list or a dictionary. Here we pass a single loss as the loss argument, so the same loss will be used on all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_sentence_input (InputLayer (None, 73)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 73, 512)      51200000    main_sentence_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                  (None, 32)           69760       embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sentence_features_input (InputL (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 38)           0           lstm_14[0][0]                    \n",
      "                                                                 sentence_features_input[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 64)           2496        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 64)           4160        dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 64)           4160        dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 2)            130         dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "aux_output (Dense)              (None, 2)            66          lstm_14[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 51,280,772\n",
      "Trainable params: 51,280,772\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "merged_model.compile(\n",
    "    optimizer='rmsprop', \n",
    "    #optimizer=\"adam\",\n",
    "    loss='categorical_crossentropy',\n",
    "    loss_weights=[1., 0.2]\n",
    ")\n",
    "merged_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16079 samples, validate on 3988 samples\n",
      "Epoch 1/100\n",
      "16079/16079 [==============================] - 536s 33ms/step - loss: 0.1506 - main_output_loss: 0.1255 - aux_output_loss: 0.1252 - val_loss: 0.1633 - val_main_output_loss: 0.1375 - val_aux_output_loss: 0.1287\n",
      "Epoch 2/100\n",
      "16079/16079 [==============================] - 164s 10ms/step - loss: 0.1430 - main_output_loss: 0.1192 - aux_output_loss: 0.1189 - val_loss: 0.1582 - val_main_output_loss: 0.1318 - val_aux_output_loss: 0.1319\n",
      "Epoch 3/100\n",
      "16079/16079 [==============================] - 169s 11ms/step - loss: 0.1283 - main_output_loss: 0.1067 - aux_output_loss: 0.1083 - val_loss: 0.1743 - val_main_output_loss: 0.1464 - val_aux_output_loss: 0.1393\n",
      "Epoch 4/100\n",
      "16079/16079 [==============================] - 170s 11ms/step - loss: 0.0996 - main_output_loss: 0.0812 - aux_output_loss: 0.0917 - val_loss: 0.2221 - val_main_output_loss: 0.1901 - val_aux_output_loss: 0.1601\n",
      "Epoch 5/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0769 - main_output_loss: 0.0623 - aux_output_loss: 0.0732 - val_loss: 0.2920 - val_main_output_loss: 0.2565 - val_aux_output_loss: 0.1776\n",
      "Epoch 6/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0641 - main_output_loss: 0.0525 - aux_output_loss: 0.0582 - val_loss: 0.3207 - val_main_output_loss: 0.2784 - val_aux_output_loss: 0.2117\n",
      "Epoch 7/100\n",
      "16079/16079 [==============================] - 172s 11ms/step - loss: 0.0583 - main_output_loss: 0.0481 - aux_output_loss: 0.0512 - val_loss: 0.3360 - val_main_output_loss: 0.2907 - val_aux_output_loss: 0.2265\n",
      "Epoch 8/100\n",
      "16079/16079 [==============================] - 173s 11ms/step - loss: 0.0519 - main_output_loss: 0.0430 - aux_output_loss: 0.0444 - val_loss: 0.5666 - val_main_output_loss: 0.5053 - val_aux_output_loss: 0.3062\n",
      "Epoch 9/100\n",
      "16079/16079 [==============================] - 173s 11ms/step - loss: 0.0467 - main_output_loss: 0.0385 - aux_output_loss: 0.0406 - val_loss: 0.4914 - val_main_output_loss: 0.4259 - val_aux_output_loss: 0.3274\n",
      "Epoch 10/100\n",
      "16079/16079 [==============================] - 173s 11ms/step - loss: 0.0432 - main_output_loss: 0.0356 - aux_output_loss: 0.0382 - val_loss: 0.7760 - val_main_output_loss: 0.6936 - val_aux_output_loss: 0.4122\n",
      "Epoch 11/100\n",
      "16079/16079 [==============================] - 173s 11ms/step - loss: 0.0383 - main_output_loss: 0.0314 - aux_output_loss: 0.0344 - val_loss: 0.8541 - val_main_output_loss: 0.7733 - val_aux_output_loss: 0.4041\n",
      "Epoch 12/100\n",
      "16079/16079 [==============================] - 172s 11ms/step - loss: 0.0354 - main_output_loss: 0.0290 - aux_output_loss: 0.0319 - val_loss: 1.4949 - val_main_output_loss: 1.3789 - val_aux_output_loss: 0.5798\n",
      "Epoch 13/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0330 - main_output_loss: 0.0270 - aux_output_loss: 0.0298 - val_loss: 0.6960 - val_main_output_loss: 0.6197 - val_aux_output_loss: 0.3817\n",
      "Epoch 14/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0286 - main_output_loss: 0.0233 - aux_output_loss: 0.0266 - val_loss: 0.6655 - val_main_output_loss: 0.5859 - val_aux_output_loss: 0.3976\n",
      "Epoch 15/100\n",
      "16079/16079 [==============================] - 170s 11ms/step - loss: 0.0261 - main_output_loss: 0.0211 - aux_output_loss: 0.0251 - val_loss: 0.6971 - val_main_output_loss: 0.6172 - val_aux_output_loss: 0.3991\n",
      "Epoch 16/100\n",
      "16079/16079 [==============================] - 169s 11ms/step - loss: 0.0238 - main_output_loss: 0.0193 - aux_output_loss: 0.0226 - val_loss: 0.7996 - val_main_output_loss: 0.7106 - val_aux_output_loss: 0.4450\n",
      "Epoch 17/100\n",
      "16079/16079 [==============================] - 164s 10ms/step - loss: 0.0211 - main_output_loss: 0.0168 - aux_output_loss: 0.0215 - val_loss: 0.7966 - val_main_output_loss: 0.7010 - val_aux_output_loss: 0.4777\n",
      "Epoch 18/100\n",
      "16079/16079 [==============================] - 169s 10ms/step - loss: 0.0190 - main_output_loss: 0.0151 - aux_output_loss: 0.0196 - val_loss: 1.0760 - val_main_output_loss: 0.9656 - val_aux_output_loss: 0.5522\n",
      "Epoch 19/100\n",
      "16079/16079 [==============================] - 169s 10ms/step - loss: 0.0174 - main_output_loss: 0.0138 - aux_output_loss: 0.0177 - val_loss: 1.8558 - val_main_output_loss: 1.7222 - val_aux_output_loss: 0.6680\n",
      "Epoch 20/100\n",
      "16079/16079 [==============================] - 170s 11ms/step - loss: 0.0157 - main_output_loss: 0.0123 - aux_output_loss: 0.0170 - val_loss: 1.2288 - val_main_output_loss: 1.1131 - val_aux_output_loss: 0.5786\n",
      "Epoch 21/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0153 - main_output_loss: 0.0121 - aux_output_loss: 0.0157 - val_loss: 1.0330 - val_main_output_loss: 0.9258 - val_aux_output_loss: 0.5361\n",
      "Epoch 22/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0135 - main_output_loss: 0.0105 - aux_output_loss: 0.0146 - val_loss: 1.0562 - val_main_output_loss: 0.9440 - val_aux_output_loss: 0.5612\n",
      "Epoch 23/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0129 - main_output_loss: 0.0103 - aux_output_loss: 0.0132 - val_loss: 1.5923 - val_main_output_loss: 1.4640 - val_aux_output_loss: 0.6413\n",
      "Epoch 24/100\n",
      "16079/16079 [==============================] - 172s 11ms/step - loss: 0.0131 - main_output_loss: 0.0105 - aux_output_loss: 0.0130 - val_loss: 2.0237 - val_main_output_loss: 1.8874 - val_aux_output_loss: 0.6815\n",
      "Epoch 25/100\n",
      "16079/16079 [==============================] - 172s 11ms/step - loss: 0.0127 - main_output_loss: 0.0103 - aux_output_loss: 0.0121 - val_loss: 2.2515 - val_main_output_loss: 2.1047 - val_aux_output_loss: 0.7337\n",
      "Epoch 26/100\n",
      "16079/16079 [==============================] - 172s 11ms/step - loss: 0.0107 - main_output_loss: 0.0085 - aux_output_loss: 0.0113 - val_loss: 1.8739 - val_main_output_loss: 1.7328 - val_aux_output_loss: 0.7056\n",
      "Epoch 27/100\n",
      "16079/16079 [==============================] - 172s 11ms/step - loss: 0.0102 - main_output_loss: 0.0082 - aux_output_loss: 0.0104 - val_loss: 2.6493 - val_main_output_loss: 2.4866 - val_aux_output_loss: 0.8140\n",
      "Epoch 28/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0103 - main_output_loss: 0.0082 - aux_output_loss: 0.0104 - val_loss: 2.3432 - val_main_output_loss: 2.1781 - val_aux_output_loss: 0.8255\n",
      "Epoch 29/100\n",
      "16079/16079 [==============================] - 172s 11ms/step - loss: 0.0099 - main_output_loss: 0.0079 - aux_output_loss: 0.0102 - val_loss: 3.0934 - val_main_output_loss: 2.9060 - val_aux_output_loss: 0.9370\n",
      "Epoch 30/100\n",
      "16079/16079 [==============================] - 170s 11ms/step - loss: 0.0103 - main_output_loss: 0.0085 - aux_output_loss: 0.0093 - val_loss: 2.4259 - val_main_output_loss: 2.2598 - val_aux_output_loss: 0.8302\n",
      "Epoch 31/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0089 - main_output_loss: 0.0072 - aux_output_loss: 0.0085 - val_loss: 3.7709 - val_main_output_loss: 3.5741 - val_aux_output_loss: 0.9841\n",
      "Epoch 32/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0101 - main_output_loss: 0.0084 - aux_output_loss: 0.0086 - val_loss: 2.9590 - val_main_output_loss: 2.7843 - val_aux_output_loss: 0.8735\n",
      "Epoch 33/100\n",
      "16079/16079 [==============================] - 170s 11ms/step - loss: 0.0084 - main_output_loss: 0.0067 - aux_output_loss: 0.0082 - val_loss: 2.8966 - val_main_output_loss: 2.7165 - val_aux_output_loss: 0.9008\n",
      "Epoch 34/100\n",
      "16079/16079 [==============================] - 169s 11ms/step - loss: 0.0094 - main_output_loss: 0.0078 - aux_output_loss: 0.0077 - val_loss: 2.9673 - val_main_output_loss: 2.7837 - val_aux_output_loss: 0.9184\n",
      "Epoch 35/100\n",
      "16079/16079 [==============================] - 169s 11ms/step - loss: 0.0084 - main_output_loss: 0.0070 - aux_output_loss: 0.0072 - val_loss: 2.8012 - val_main_output_loss: 2.6333 - val_aux_output_loss: 0.8397\n",
      "Epoch 36/100\n",
      "16079/16079 [==============================] - 170s 11ms/step - loss: 0.0085 - main_output_loss: 0.0071 - aux_output_loss: 0.0073 - val_loss: 2.0974 - val_main_output_loss: 1.9228 - val_aux_output_loss: 0.8731\n",
      "Epoch 37/100\n",
      "16079/16079 [==============================] - 170s 11ms/step - loss: 0.0079 - main_output_loss: 0.0065 - aux_output_loss: 0.0068 - val_loss: 2.7733 - val_main_output_loss: 2.5955 - val_aux_output_loss: 0.8894\n",
      "Epoch 38/100\n",
      "16079/16079 [==============================] - 170s 11ms/step - loss: 0.0074 - main_output_loss: 0.0061 - aux_output_loss: 0.0067 - val_loss: 2.8884 - val_main_output_loss: 2.6959 - val_aux_output_loss: 0.9626\n",
      "Epoch 39/100\n",
      "16079/16079 [==============================] - 169s 11ms/step - loss: 0.0092 - main_output_loss: 0.0079 - aux_output_loss: 0.0067 - val_loss: 2.1354 - val_main_output_loss: 1.9673 - val_aux_output_loss: 0.8404\n",
      "Epoch 40/100\n",
      "16079/16079 [==============================] - 170s 11ms/step - loss: 0.0079 - main_output_loss: 0.0067 - aux_output_loss: 0.0061 - val_loss: 1.8675 - val_main_output_loss: 1.7020 - val_aux_output_loss: 0.8273\n",
      "Epoch 41/100\n",
      "16079/16079 [==============================] - 169s 11ms/step - loss: 0.0082 - main_output_loss: 0.0069 - aux_output_loss: 0.0066 - val_loss: 3.0758 - val_main_output_loss: 2.8736 - val_aux_output_loss: 1.0108\n",
      "Epoch 42/100\n",
      "16079/16079 [==============================] - 170s 11ms/step - loss: 0.0077 - main_output_loss: 0.0064 - aux_output_loss: 0.0064 - val_loss: 2.6138 - val_main_output_loss: 2.4279 - val_aux_output_loss: 0.9294\n",
      "Epoch 43/100\n",
      "16079/16079 [==============================] - 169s 11ms/step - loss: 0.0078 - main_output_loss: 0.0066 - aux_output_loss: 0.0063 - val_loss: 3.2890 - val_main_output_loss: 3.0889 - val_aux_output_loss: 1.0003\n",
      "Epoch 44/100\n",
      "16079/16079 [==============================] - 170s 11ms/step - loss: 0.0104 - main_output_loss: 0.0091 - aux_output_loss: 0.0064 - val_loss: 2.3579 - val_main_output_loss: 2.1875 - val_aux_output_loss: 0.8520\n",
      "Epoch 45/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0078 - main_output_loss: 0.0064 - aux_output_loss: 0.0067 - val_loss: 3.4222 - val_main_output_loss: 3.2232 - val_aux_output_loss: 0.9950\n",
      "Epoch 46/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0076 - main_output_loss: 0.0064 - aux_output_loss: 0.0063 - val_loss: 2.8103 - val_main_output_loss: 2.5946 - val_aux_output_loss: 1.0785\n",
      "Epoch 47/100\n",
      "16079/16079 [==============================] - 170s 11ms/step - loss: 0.0079 - main_output_loss: 0.0066 - aux_output_loss: 0.0067 - val_loss: 3.1020 - val_main_output_loss: 2.8822 - val_aux_output_loss: 1.0994\n",
      "Epoch 48/100\n",
      "16079/16079 [==============================] - 170s 11ms/step - loss: 0.0087 - main_output_loss: 0.0075 - aux_output_loss: 0.0063 - val_loss: 2.6913 - val_main_output_loss: 2.4943 - val_aux_output_loss: 0.9847\n",
      "Epoch 49/100\n",
      "16079/16079 [==============================] - 170s 11ms/step - loss: 0.0076 - main_output_loss: 0.0064 - aux_output_loss: 0.0059 - val_loss: 3.2297 - val_main_output_loss: 3.0149 - val_aux_output_loss: 1.0740\n",
      "Epoch 50/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0080 - main_output_loss: 0.0068 - aux_output_loss: 0.0057 - val_loss: 3.0822 - val_main_output_loss: 2.8543 - val_aux_output_loss: 1.1394\n",
      "Epoch 51/100\n",
      "16079/16079 [==============================] - 169s 11ms/step - loss: 0.0079 - main_output_loss: 0.0067 - aux_output_loss: 0.0060 - val_loss: 3.4440 - val_main_output_loss: 3.2239 - val_aux_output_loss: 1.1000\n",
      "Epoch 52/100\n",
      "16079/16079 [==============================] - 169s 11ms/step - loss: 0.0077 - main_output_loss: 0.0066 - aux_output_loss: 0.0057 - val_loss: 2.7615 - val_main_output_loss: 2.5678 - val_aux_output_loss: 0.9689\n",
      "Epoch 53/100\n",
      "16079/16079 [==============================] - 170s 11ms/step - loss: 0.0067 - main_output_loss: 0.0056 - aux_output_loss: 0.0057 - val_loss: 2.0180 - val_main_output_loss: 1.8378 - val_aux_output_loss: 0.9013\n",
      "Epoch 54/100\n",
      "16079/16079 [==============================] - 169s 11ms/step - loss: 0.0083 - main_output_loss: 0.0071 - aux_output_loss: 0.0063 - val_loss: 2.5436 - val_main_output_loss: 2.3461 - val_aux_output_loss: 0.9877\n",
      "Epoch 55/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0084 - main_output_loss: 0.0071 - aux_output_loss: 0.0064 - val_loss: 2.0989 - val_main_output_loss: 1.9088 - val_aux_output_loss: 0.9507\n",
      "Epoch 56/100\n",
      "16079/16079 [==============================] - 169s 11ms/step - loss: 0.0070 - main_output_loss: 0.0059 - aux_output_loss: 0.0056 - val_loss: 2.6982 - val_main_output_loss: 2.4917 - val_aux_output_loss: 1.0326\n",
      "Epoch 57/100\n",
      "16079/16079 [==============================] - 170s 11ms/step - loss: 0.0079 - main_output_loss: 0.0067 - aux_output_loss: 0.0061 - val_loss: 2.9098 - val_main_output_loss: 2.6841 - val_aux_output_loss: 1.1286\n",
      "Epoch 58/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0074 - main_output_loss: 0.0062 - aux_output_loss: 0.0063 - val_loss: 2.7258 - val_main_output_loss: 2.5105 - val_aux_output_loss: 1.0762\n",
      "Epoch 59/100\n",
      "16079/16079 [==============================] - 165s 10ms/step - loss: 0.0070 - main_output_loss: 0.0057 - aux_output_loss: 0.0062 - val_loss: 2.0979 - val_main_output_loss: 1.9203 - val_aux_output_loss: 0.8877\n",
      "Epoch 60/100\n",
      "16079/16079 [==============================] - 158s 10ms/step - loss: 0.0076 - main_output_loss: 0.0064 - aux_output_loss: 0.0058 - val_loss: 2.5023 - val_main_output_loss: 2.3126 - val_aux_output_loss: 0.9488\n",
      "Epoch 61/100\n",
      "16079/16079 [==============================] - 161s 10ms/step - loss: 0.0117 - main_output_loss: 0.0105 - aux_output_loss: 0.0061 - val_loss: 2.2624 - val_main_output_loss: 2.0531 - val_aux_output_loss: 1.0467\n",
      "Epoch 62/100\n",
      "16079/16079 [==============================] - 163s 10ms/step - loss: 0.0085 - main_output_loss: 0.0074 - aux_output_loss: 0.0054 - val_loss: 2.1708 - val_main_output_loss: 1.9709 - val_aux_output_loss: 0.9992\n",
      "Epoch 63/100\n",
      "16079/16079 [==============================] - 166s 10ms/step - loss: 0.0071 - main_output_loss: 0.0059 - aux_output_loss: 0.0057 - val_loss: 2.8016 - val_main_output_loss: 2.5803 - val_aux_output_loss: 1.1064\n",
      "Epoch 64/100\n",
      "16079/16079 [==============================] - 168s 10ms/step - loss: 0.0070 - main_output_loss: 0.0058 - aux_output_loss: 0.0056 - val_loss: 2.5083 - val_main_output_loss: 2.2971 - val_aux_output_loss: 1.0560\n",
      "Epoch 65/100\n",
      "16079/16079 [==============================] - 168s 10ms/step - loss: 0.0072 - main_output_loss: 0.0061 - aux_output_loss: 0.0057 - val_loss: 2.2027 - val_main_output_loss: 2.0085 - val_aux_output_loss: 0.9710\n",
      "Epoch 66/100\n",
      "16079/16079 [==============================] - 169s 11ms/step - loss: 0.0072 - main_output_loss: 0.0061 - aux_output_loss: 0.0056 - val_loss: 2.2485 - val_main_output_loss: 2.0489 - val_aux_output_loss: 0.9977\n",
      "Epoch 67/100\n",
      "16079/16079 [==============================] - 169s 11ms/step - loss: 0.0083 - main_output_loss: 0.0071 - aux_output_loss: 0.0055 - val_loss: 2.5252 - val_main_output_loss: 2.3005 - val_aux_output_loss: 1.1238\n",
      "Epoch 68/100\n",
      "16079/16079 [==============================] - 169s 11ms/step - loss: 0.0083 - main_output_loss: 0.0070 - aux_output_loss: 0.0064 - val_loss: 2.2228 - val_main_output_loss: 1.9973 - val_aux_output_loss: 1.1275\n",
      "Epoch 69/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0079 - main_output_loss: 0.0068 - aux_output_loss: 0.0055 - val_loss: 1.8794 - val_main_output_loss: 1.6691 - val_aux_output_loss: 1.0514\n",
      "Epoch 70/100\n",
      "16079/16079 [==============================] - 172s 11ms/step - loss: 0.0070 - main_output_loss: 0.0059 - aux_output_loss: 0.0055 - val_loss: 1.9864 - val_main_output_loss: 1.8043 - val_aux_output_loss: 0.9106\n",
      "Epoch 71/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0083 - main_output_loss: 0.0072 - aux_output_loss: 0.0058 - val_loss: 2.0969 - val_main_output_loss: 1.8797 - val_aux_output_loss: 1.0861\n",
      "Epoch 72/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0079 - main_output_loss: 0.0067 - aux_output_loss: 0.0059 - val_loss: 2.9111 - val_main_output_loss: 2.6641 - val_aux_output_loss: 1.2352\n",
      "Epoch 73/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0065 - main_output_loss: 0.0054 - aux_output_loss: 0.0056 - val_loss: 1.7376 - val_main_output_loss: 1.5425 - val_aux_output_loss: 0.9757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100\n",
      "16079/16079 [==============================] - 170s 11ms/step - loss: 0.0069 - main_output_loss: 0.0058 - aux_output_loss: 0.0056 - val_loss: 2.0155 - val_main_output_loss: 1.7773 - val_aux_output_loss: 1.1912\n",
      "Epoch 75/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0096 - main_output_loss: 0.0082 - aux_output_loss: 0.0070 - val_loss: 2.3168 - val_main_output_loss: 2.0729 - val_aux_output_loss: 1.2192\n",
      "Epoch 76/100\n",
      "16079/16079 [==============================] - 172s 11ms/step - loss: 0.0083 - main_output_loss: 0.0072 - aux_output_loss: 0.0054 - val_loss: 2.6030 - val_main_output_loss: 2.3298 - val_aux_output_loss: 1.3658\n",
      "Epoch 77/100\n",
      "16079/16079 [==============================] - 172s 11ms/step - loss: 0.0072 - main_output_loss: 0.0060 - aux_output_loss: 0.0057 - val_loss: 1.8970 - val_main_output_loss: 1.6763 - val_aux_output_loss: 1.1037\n",
      "Epoch 78/100\n",
      "16079/16079 [==============================] - 169s 11ms/step - loss: 0.0086 - main_output_loss: 0.0074 - aux_output_loss: 0.0060 - val_loss: 1.9504 - val_main_output_loss: 1.7390 - val_aux_output_loss: 1.0569\n",
      "Epoch 79/100\n",
      "16079/16079 [==============================] - 172s 11ms/step - loss: 0.0084 - main_output_loss: 0.0072 - aux_output_loss: 0.0061 - val_loss: 2.8601 - val_main_output_loss: 2.5846 - val_aux_output_loss: 1.3774\n",
      "Epoch 80/100\n",
      "16079/16079 [==============================] - 170s 11ms/step - loss: 0.0069 - main_output_loss: 0.0057 - aux_output_loss: 0.0058 - val_loss: 2.1616 - val_main_output_loss: 1.9336 - val_aux_output_loss: 1.1399\n",
      "Epoch 81/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0064 - main_output_loss: 0.0054 - aux_output_loss: 0.0054 - val_loss: 2.1707 - val_main_output_loss: 1.9344 - val_aux_output_loss: 1.1816\n",
      "Epoch 82/100\n",
      "16079/16079 [==============================] - 173s 11ms/step - loss: 0.0070 - main_output_loss: 0.0059 - aux_output_loss: 0.0055 - val_loss: 2.8287 - val_main_output_loss: 2.5897 - val_aux_output_loss: 1.1951\n",
      "Epoch 83/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0072 - main_output_loss: 0.0060 - aux_output_loss: 0.0057 - val_loss: 2.4553 - val_main_output_loss: 2.1944 - val_aux_output_loss: 1.3042\n",
      "Epoch 84/100\n",
      "16079/16079 [==============================] - 172s 11ms/step - loss: 0.0069 - main_output_loss: 0.0058 - aux_output_loss: 0.0053 - val_loss: 2.6984 - val_main_output_loss: 2.4772 - val_aux_output_loss: 1.1060\n",
      "Epoch 85/100\n",
      "16079/16079 [==============================] - 173s 11ms/step - loss: 0.0078 - main_output_loss: 0.0067 - aux_output_loss: 0.0055 - val_loss: 1.9475 - val_main_output_loss: 1.7485 - val_aux_output_loss: 0.9950\n",
      "Epoch 86/100\n",
      "16079/16079 [==============================] - 172s 11ms/step - loss: 0.0076 - main_output_loss: 0.0064 - aux_output_loss: 0.0059 - val_loss: 1.8008 - val_main_output_loss: 1.6025 - val_aux_output_loss: 0.9916\n",
      "Epoch 87/100\n",
      "16079/16079 [==============================] - 173s 11ms/step - loss: 0.0089 - main_output_loss: 0.0077 - aux_output_loss: 0.0058 - val_loss: 2.7286 - val_main_output_loss: 2.4770 - val_aux_output_loss: 1.2579\n",
      "Epoch 88/100\n",
      "16079/16079 [==============================] - 5441s 338ms/step - loss: 0.0074 - main_output_loss: 0.0063 - aux_output_loss: 0.0055 - val_loss: 2.7469 - val_main_output_loss: 2.4849 - val_aux_output_loss: 1.3099\n",
      "Epoch 89/100\n",
      "16079/16079 [==============================] - 173s 11ms/step - loss: 0.0080 - main_output_loss: 0.0068 - aux_output_loss: 0.0057 - val_loss: 2.3832 - val_main_output_loss: 2.1179 - val_aux_output_loss: 1.3265\n",
      "Epoch 90/100\n",
      "16079/16079 [==============================] - 174s 11ms/step - loss: 0.0080 - main_output_loss: 0.0069 - aux_output_loss: 0.0057 - val_loss: 2.4533 - val_main_output_loss: 2.2270 - val_aux_output_loss: 1.1316\n",
      "Epoch 91/100\n",
      "16079/16079 [==============================] - 169s 11ms/step - loss: 0.0082 - main_output_loss: 0.0070 - aux_output_loss: 0.0063 - val_loss: 2.7227 - val_main_output_loss: 2.4701 - val_aux_output_loss: 1.2633\n",
      "Epoch 92/100\n",
      "16079/16079 [==============================] - 171s 11ms/step - loss: 0.0088 - main_output_loss: 0.0076 - aux_output_loss: 0.0060 - val_loss: 3.3352 - val_main_output_loss: 3.0166 - val_aux_output_loss: 1.5929\n",
      "Epoch 93/100\n",
      "16079/16079 [==============================] - 172s 11ms/step - loss: 0.0076 - main_output_loss: 0.0065 - aux_output_loss: 0.0055 - val_loss: 2.9069 - val_main_output_loss: 2.6325 - val_aux_output_loss: 1.3721\n",
      "Epoch 94/100\n",
      "16079/16079 [==============================] - 175s 11ms/step - loss: 0.0079 - main_output_loss: 0.0068 - aux_output_loss: 0.0054 - val_loss: 2.6703 - val_main_output_loss: 2.4148 - val_aux_output_loss: 1.2775\n",
      "Epoch 95/100\n",
      "16079/16079 [==============================] - 174s 11ms/step - loss: 0.0070 - main_output_loss: 0.0059 - aux_output_loss: 0.0055 - val_loss: 2.8939 - val_main_output_loss: 2.6047 - val_aux_output_loss: 1.4460\n",
      "Epoch 96/100\n",
      "16079/16079 [==============================] - 173s 11ms/step - loss: 0.0084 - main_output_loss: 0.0072 - aux_output_loss: 0.0057 - val_loss: 2.6714 - val_main_output_loss: 2.3897 - val_aux_output_loss: 1.4086\n",
      "Epoch 97/100\n",
      "16079/16079 [==============================] - 172s 11ms/step - loss: 0.0081 - main_output_loss: 0.0070 - aux_output_loss: 0.0057 - val_loss: 2.6510 - val_main_output_loss: 2.4010 - val_aux_output_loss: 1.2503\n",
      "Epoch 98/100\n",
      "16079/16079 [==============================] - 173s 11ms/step - loss: 0.0072 - main_output_loss: 0.0061 - aux_output_loss: 0.0057 - val_loss: 2.8466 - val_main_output_loss: 2.5932 - val_aux_output_loss: 1.2669\n",
      "Epoch 99/100\n",
      "16079/16079 [==============================] - 173s 11ms/step - loss: 0.0089 - main_output_loss: 0.0078 - aux_output_loss: 0.0056 - val_loss: 2.9130 - val_main_output_loss: 2.6283 - val_aux_output_loss: 1.4234\n",
      "Epoch 100/100\n",
      "16079/16079 [==============================] - 172s 11ms/step - loss: 0.0096 - main_output_loss: 0.0084 - aux_output_loss: 0.0060 - val_loss: 2.7001 - val_main_output_loss: 2.4126 - val_aux_output_loss: 1.4374\n",
      "22670.262973070145  seconds\n"
     ]
    }
   ],
   "source": [
    "# Train the model by passing it lists of input arrays and target arrays 19960.996418952942  seconds\n",
    "# 25183.798012971878  seconds Second time, with 30 minute break in between\n",
    "start = time.time()\n",
    "merged_model_history = merged_model.fit(\n",
    "    [train_X, train_features_X], \n",
    "    [train_y, train_y],\n",
    "    #validation_split=0.2,\n",
    "    validation_data=[\n",
    "        [validation_X, validation_features_X], \n",
    "        [validation_y, validation_y]\n",
    "    ],\n",
    "    epochs=100, \n",
    "    batch_size=64\n",
    ")\n",
    "end = time.time()\n",
    "print(end - start, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_model_loss_values = merged_model_history.history['loss']\n",
    "merged_model_epochs = range(1, len(merged_model_loss_values)+1)\n",
    "\n",
    "plt.plot(merged_model_epochs, merged_model_loss_values, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model_filename = join(DATA_FILES_DIR, 'merged_hybrid_model.h5')\n",
    "merged_model.save(merged_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_merged_model = load_model(merged_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 73, 32)            3200000   \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 200)               106400    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 3,306,802\n",
      "Trainable params: 3,306,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16079 samples, validate on 3988 samples\n",
      "Epoch 1/100\n",
      "16079/16079 [==============================] - 22s 1ms/step - loss: 0.6377 - acc: 0.9469 - val_loss: 0.5210 - val_acc: 0.9709\n",
      "Epoch 2/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.2847 - acc: 0.9684 - val_loss: 0.1497 - val_acc: 0.9709\n",
      "Epoch 3/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.1524 - acc: 0.9684 - val_loss: 0.1314 - val_acc: 0.9709\n",
      "Epoch 4/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.1427 - acc: 0.9684 - val_loss: 0.1348 - val_acc: 0.9709\n",
      "Epoch 5/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.1405 - acc: 0.9684 - val_loss: 0.1320 - val_acc: 0.9709\n",
      "Epoch 6/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.1409 - acc: 0.9684 - val_loss: 0.1314 - val_acc: 0.9709\n",
      "Epoch 7/100\n",
      "16079/16079 [==============================] - 21s 1ms/step - loss: 0.1396 - acc: 0.9684 - val_loss: 0.1319 - val_acc: 0.9709\n",
      "Epoch 8/100\n",
      "16079/16079 [==============================] - 21s 1ms/step - loss: 0.1396 - acc: 0.9684 - val_loss: 0.1313 - val_acc: 0.9709\n",
      "Epoch 9/100\n",
      "16079/16079 [==============================] - 20s 1ms/step - loss: 0.1392 - acc: 0.9684 - val_loss: 0.1311 - val_acc: 0.9709\n",
      "Epoch 10/100\n",
      "16079/16079 [==============================] - 20s 1ms/step - loss: 0.1386 - acc: 0.9684 - val_loss: 0.1311 - val_acc: 0.9709\n",
      "Epoch 11/100\n",
      "16079/16079 [==============================] - 20s 1ms/step - loss: 0.1379 - acc: 0.9684 - val_loss: 0.1307 - val_acc: 0.9709\n",
      "Epoch 12/100\n",
      "16079/16079 [==============================] - 20s 1ms/step - loss: 0.1365 - acc: 0.9684 - val_loss: 0.1300 - val_acc: 0.9709\n",
      "Epoch 13/100\n",
      "16079/16079 [==============================] - 20s 1ms/step - loss: 0.1342 - acc: 0.9684 - val_loss: 0.1291 - val_acc: 0.9709\n",
      "Epoch 14/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.1308 - acc: 0.9684 - val_loss: 0.1278 - val_acc: 0.9709\n",
      "Epoch 15/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.1258 - acc: 0.9684 - val_loss: 0.1287 - val_acc: 0.9709\n",
      "Epoch 16/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.1221 - acc: 0.9684 - val_loss: 0.1251 - val_acc: 0.9709\n",
      "Epoch 17/100\n",
      "16079/16079 [==============================] - 17s 1ms/step - loss: 0.1137 - acc: 0.9685 - val_loss: 0.1242 - val_acc: 0.9709\n",
      "Epoch 18/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.1037 - acc: 0.9691 - val_loss: 0.1236 - val_acc: 0.9709\n",
      "Epoch 19/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0924 - acc: 0.9719 - val_loss: 0.1257 - val_acc: 0.9702\n",
      "Epoch 20/100\n",
      "16079/16079 [==============================] - 20s 1ms/step - loss: 0.0806 - acc: 0.9757 - val_loss: 0.1286 - val_acc: 0.9694\n",
      "Epoch 21/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0714 - acc: 0.9779 - val_loss: 0.1323 - val_acc: 0.9697\n",
      "Epoch 22/100\n",
      "16079/16079 [==============================] - 20s 1ms/step - loss: 0.0715 - acc: 0.9733 - val_loss: 0.1347 - val_acc: 0.9712\n",
      "Epoch 23/100\n",
      "16079/16079 [==============================] - 23s 1ms/step - loss: 0.0675 - acc: 0.9701 - val_loss: 0.1373 - val_acc: 0.9709\n",
      "Epoch 24/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0618 - acc: 0.9693 - val_loss: 0.1407 - val_acc: 0.9709\n",
      "Epoch 25/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0562 - acc: 0.9691 - val_loss: 0.1463 - val_acc: 0.9709\n",
      "Epoch 26/100\n",
      "16079/16079 [==============================] - 20s 1ms/step - loss: 0.0519 - acc: 0.9690 - val_loss: 0.1520 - val_acc: 0.9709\n",
      "Epoch 27/100\n",
      "16079/16079 [==============================] - 20s 1ms/step - loss: 0.0482 - acc: 0.9691 - val_loss: 0.1561 - val_acc: 0.9709\n",
      "Epoch 28/100\n",
      "16079/16079 [==============================] - 21s 1ms/step - loss: 0.0459 - acc: 0.9708 - val_loss: 0.1698 - val_acc: 0.9709\n",
      "Epoch 29/100\n",
      "16079/16079 [==============================] - 21s 1ms/step - loss: 0.0423 - acc: 0.9729 - val_loss: 0.1708 - val_acc: 0.9697\n",
      "Epoch 30/100\n",
      "16079/16079 [==============================] - 20s 1ms/step - loss: 0.0395 - acc: 0.9778 - val_loss: 0.1779 - val_acc: 0.9666\n",
      "Epoch 31/100\n",
      "16079/16079 [==============================] - 20s 1ms/step - loss: 0.0371 - acc: 0.9827 - val_loss: 0.1883 - val_acc: 0.9669\n",
      "Epoch 32/100\n",
      "16079/16079 [==============================] - 21s 1ms/step - loss: 0.0341 - acc: 0.9869 - val_loss: 0.1927 - val_acc: 0.9646\n",
      "Epoch 33/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0292 - acc: 0.9900 - val_loss: 0.1975 - val_acc: 0.9631\n",
      "Epoch 34/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0248 - acc: 0.9917 - val_loss: 0.2046 - val_acc: 0.9609\n",
      "Epoch 35/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0218 - acc: 0.9924 - val_loss: 0.2234 - val_acc: 0.9566\n",
      "Epoch 36/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0200 - acc: 0.9932 - val_loss: 0.2263 - val_acc: 0.9571\n",
      "Epoch 37/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0190 - acc: 0.9940 - val_loss: 0.2252 - val_acc: 0.9609\n",
      "Epoch 38/100\n",
      "16079/16079 [==============================] - 17s 1ms/step - loss: 0.0172 - acc: 0.9943 - val_loss: 0.2333 - val_acc: 0.9604\n",
      "Epoch 39/100\n",
      "16079/16079 [==============================] - 17s 1ms/step - loss: 0.0152 - acc: 0.9953 - val_loss: 0.2401 - val_acc: 0.9596\n",
      "Epoch 40/100\n",
      "16079/16079 [==============================] - 17s 1ms/step - loss: 0.0140 - acc: 0.9956 - val_loss: 0.2373 - val_acc: 0.9604\n",
      "Epoch 41/100\n",
      "16079/16079 [==============================] - 17s 1ms/step - loss: 0.0133 - acc: 0.9961 - val_loss: 0.2447 - val_acc: 0.9589\n",
      "Epoch 42/100\n",
      "16079/16079 [==============================] - 17s 1ms/step - loss: 0.0122 - acc: 0.9962 - val_loss: 0.2538 - val_acc: 0.9589\n",
      "Epoch 43/100\n",
      "16079/16079 [==============================] - 17s 1ms/step - loss: 0.0114 - acc: 0.9967 - val_loss: 0.2536 - val_acc: 0.9586\n",
      "Epoch 44/100\n",
      "16079/16079 [==============================] - 17s 1ms/step - loss: 0.0115 - acc: 0.9968 - val_loss: 0.2598 - val_acc: 0.9579\n",
      "Epoch 45/100\n",
      "16079/16079 [==============================] - 17s 1ms/step - loss: 0.0104 - acc: 0.9969 - val_loss: 0.2671 - val_acc: 0.9599\n",
      "Epoch 46/100\n",
      "16079/16079 [==============================] - 17s 1ms/step - loss: 0.0108 - acc: 0.9966 - val_loss: 0.2715 - val_acc: 0.9559\n",
      "Epoch 47/100\n",
      "16079/16079 [==============================] - 17s 1ms/step - loss: 0.0101 - acc: 0.9969 - val_loss: 0.2673 - val_acc: 0.9559\n",
      "Epoch 48/100\n",
      "16079/16079 [==============================] - 17s 1ms/step - loss: 0.0122 - acc: 0.9962 - val_loss: 0.2831 - val_acc: 0.9401\n",
      "Epoch 49/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0140 - acc: 0.9950 - val_loss: 0.2653 - val_acc: 0.9604\n",
      "Epoch 50/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0096 - acc: 0.9968 - val_loss: 0.2762 - val_acc: 0.9526\n",
      "Epoch 51/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0089 - acc: 0.9971 - val_loss: 0.2769 - val_acc: 0.9564\n",
      "Epoch 52/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0085 - acc: 0.9970 - val_loss: 0.2848 - val_acc: 0.9541\n",
      "Epoch 53/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0079 - acc: 0.9975 - val_loss: 0.2909 - val_acc: 0.9544\n",
      "Epoch 54/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0075 - acc: 0.9978 - val_loss: 0.2927 - val_acc: 0.9569\n",
      "Epoch 55/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0073 - acc: 0.9973 - val_loss: 0.2965 - val_acc: 0.9574\n",
      "Epoch 56/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0073 - acc: 0.9976 - val_loss: 0.3071 - val_acc: 0.9511\n",
      "Epoch 57/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0068 - acc: 0.9978 - val_loss: 0.3034 - val_acc: 0.9574\n",
      "Epoch 58/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0068 - acc: 0.9976 - val_loss: 0.3108 - val_acc: 0.9559\n",
      "Epoch 59/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0064 - acc: 0.9981 - val_loss: 0.3039 - val_acc: 0.9589\n",
      "Epoch 60/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0087 - acc: 0.9970 - val_loss: 0.3078 - val_acc: 0.9526\n",
      "Epoch 61/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0083 - acc: 0.9970 - val_loss: 0.2985 - val_acc: 0.9584\n",
      "Epoch 62/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0073 - acc: 0.9974 - val_loss: 0.3027 - val_acc: 0.9549\n",
      "Epoch 63/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0067 - acc: 0.9975 - val_loss: 0.3127 - val_acc: 0.9546\n",
      "Epoch 64/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0061 - acc: 0.9978 - val_loss: 0.3165 - val_acc: 0.9574\n",
      "Epoch 65/100\n",
      "16079/16079 [==============================] - 17s 1ms/step - loss: 0.0060 - acc: 0.9978 - val_loss: 0.3185 - val_acc: 0.9616\n",
      "Epoch 66/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0058 - acc: 0.9981 - val_loss: 0.3275 - val_acc: 0.9546\n",
      "Epoch 67/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0057 - acc: 0.9976 - val_loss: 0.3302 - val_acc: 0.9571\n",
      "Epoch 68/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0052 - acc: 0.9981 - val_loss: 0.3426 - val_acc: 0.9501\n",
      "Epoch 69/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0053 - acc: 0.9977 - val_loss: 0.3389 - val_acc: 0.9536\n",
      "Epoch 70/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0053 - acc: 0.9978 - val_loss: 0.3426 - val_acc: 0.9511\n",
      "Epoch 71/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0055 - acc: 0.9978 - val_loss: 0.3433 - val_acc: 0.9574\n",
      "Epoch 72/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0501 - acc: 0.9840 - val_loss: 0.2869 - val_acc: 0.9684\n",
      "Epoch 73/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0667 - acc: 0.9801 - val_loss: 0.2090 - val_acc: 0.9486\n",
      "Epoch 74/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0577 - acc: 0.9789 - val_loss: 0.1757 - val_acc: 0.9666\n",
      "Epoch 75/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0457 - acc: 0.9836 - val_loss: 0.1692 - val_acc: 0.9619\n",
      "Epoch 76/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0366 - acc: 0.9885 - val_loss: 0.1686 - val_acc: 0.9641\n",
      "Epoch 77/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0299 - acc: 0.9897 - val_loss: 0.1747 - val_acc: 0.9631\n",
      "Epoch 78/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0242 - acc: 0.9928 - val_loss: 0.1837 - val_acc: 0.9621\n",
      "Epoch 79/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0206 - acc: 0.9935 - val_loss: 0.1943 - val_acc: 0.9611\n",
      "Epoch 80/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0176 - acc: 0.9945 - val_loss: 0.2028 - val_acc: 0.9616\n",
      "Epoch 81/100\n",
      "16079/16079 [==============================] - 20s 1ms/step - loss: 0.0154 - acc: 0.9953 - val_loss: 0.2121 - val_acc: 0.9581\n",
      "Epoch 82/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0140 - acc: 0.9958 - val_loss: 0.2195 - val_acc: 0.9574\n",
      "Epoch 83/100\n",
      "16079/16079 [==============================] - 20s 1ms/step - loss: 0.0127 - acc: 0.9962 - val_loss: 0.2233 - val_acc: 0.9596\n",
      "Epoch 84/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0117 - acc: 0.9967 - val_loss: 0.2287 - val_acc: 0.9609\n",
      "Epoch 85/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0107 - acc: 0.9972 - val_loss: 0.2366 - val_acc: 0.9561\n",
      "Epoch 86/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0100 - acc: 0.9973 - val_loss: 0.2407 - val_acc: 0.9584\n",
      "Epoch 87/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.2491 - val_acc: 0.9539\n",
      "Epoch 88/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0088 - acc: 0.9978 - val_loss: 0.2513 - val_acc: 0.9561\n",
      "Epoch 89/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0089 - acc: 0.9975 - val_loss: 0.2527 - val_acc: 0.9599\n",
      "Epoch 90/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0095 - acc: 0.9967 - val_loss: 0.2545 - val_acc: 0.9594\n",
      "Epoch 91/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0085 - acc: 0.9977 - val_loss: 0.2583 - val_acc: 0.9564\n",
      "Epoch 92/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0081 - acc: 0.9976 - val_loss: 0.2637 - val_acc: 0.9549\n",
      "Epoch 93/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0081 - acc: 0.9972 - val_loss: 0.2694 - val_acc: 0.9536\n",
      "Epoch 94/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0074 - acc: 0.9981 - val_loss: 0.2686 - val_acc: 0.9589\n",
      "Epoch 95/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0082 - acc: 0.9974 - val_loss: 0.2714 - val_acc: 0.9591\n",
      "Epoch 96/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0077 - acc: 0.9980 - val_loss: 0.2803 - val_acc: 0.9519\n",
      "Epoch 97/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0072 - acc: 0.9981 - val_loss: 0.2814 - val_acc: 0.9529\n",
      "Epoch 98/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0070 - acc: 0.9979 - val_loss: 0.2842 - val_acc: 0.9531\n",
      "Epoch 99/100\n",
      "16079/16079 [==============================] - 19s 1ms/step - loss: 0.0067 - acc: 0.9978 - val_loss: 0.2877 - val_acc: 0.9534\n",
      "Epoch 100/100\n",
      "16079/16079 [==============================] - 18s 1ms/step - loss: 0.0065 - acc: 0.9978 - val_loss: 0.2921 - val_acc: 0.9524\n",
      "1879.963231086731  seconds\n"
     ]
    }
   ],
   "source": [
    "# 3036.66828584671\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "    train_X, \n",
    "    train_y, \n",
    "    validation_data=(validation_X, validation_y), \n",
    "    epochs=100, \n",
    "    batch_size=2048\n",
    ")\n",
    "end = time.time()\n",
    "print(end-start, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_values = history.history['loss']\n",
    "epochs = range(1, len(loss_values)+1)\n",
    "\n",
    "plt.plot(epochs, loss_values, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_predictions = loaded_merged_model.predict([test_X, test_features_X])\n",
    "validation_y_predictions = loaded_merged_model.predict([validation_X, validation_features_X])\n",
    "\n",
    "test_y_argmax = np.argmax(test_y,axis=1)\n",
    "validation_y_argmax = np.argmax(validation_y,axis=1)\n",
    "\n",
    "test_y_predictions_argmax = np.argmax(test_y_predictions[1], axis=1)\n",
    "validation_y_predictions_argmax = np.argmax(validation_y_predictions[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_predictions = model.predict(test_X)\n",
    "validation_y_predictions = model.predict(validation_X)\n",
    "\n",
    "test_y_argmax = np.argmax(test_y,axis=1)\n",
    "validation_y_argmax = np.argmax(validation_y,axis=1)\n",
    "\n",
    "test_y_predictions_argmax = np.argmax(test_y_predictions, axis=1)\n",
    "validation_y_predictions_argmax = np.argmax(validation_y_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.3113022e-06, 9.9559379e-01],\n",
       "        [2.9699147e-02, 5.9604645e-08],\n",
       "        [1.4858201e-01, 8.9406967e-08],\n",
       "        ...,\n",
       "        [0.0000000e+00, 9.9992126e-01],\n",
       "        [1.3409239e-01, 2.9802322e-07],\n",
       "        [2.3387089e-01, 6.2286854e-06]], dtype=float32),\n",
       " array([[9.5286965e-04, 3.1424105e-02],\n",
       "        [9.0528780e-01, 4.0978193e-05],\n",
       "        [7.7358443e-01, 2.4735928e-06],\n",
       "        ...,\n",
       "        [6.9218874e-04, 3.8036972e-02],\n",
       "        [5.5010247e-01, 3.0606985e-05],\n",
       "        [6.7977536e-01, 1.2323260e-04]], dtype=float32)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_predictions_argmax[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_argmax[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.76      0.85       619\n",
      "           1       0.05      0.28      0.09        29\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       648\n",
      "   macro avg       0.50      0.52      0.47       648\n",
      "weighted avg       0.92      0.74      0.81       648\n",
      "\n",
      "[[471 148]\n",
      " [ 21   8]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y_argmax, test_y_predictions_argmax))\n",
    "print(confusion_matrix(test_y_argmax, test_y_predictions_argmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARIES\n",
      "---------\n",
      " btw, why are ou guys calling this abstraction thingy GComm when Dave has a project called GNU Comm (GComm fr short)?\n",
      "as far as I'm concerned, GComm is our internal package name... to the external world, it's GNUe Common\n",
      "but that is a good point\n",
      "pyro is an object system like what gcomm will be\n",
      "by same guys that wrote pygmy\n",
      "the python email client\n",
      "I need a production quality GNUe web shopping cart ;-)\n",
      "anything is possible I know, but ideally we need to get ideas on some sort of php and GEAS interface\n",
      "guess I'll just customize interchange.. hopefully actually have it done in a few days\n",
      "our inventory package isn't completed\n",
      "but if you have an inventory package\n",
      "you should be able to access it via geas\n",
      "I know a web interface for GNUe Forms is in the works, but I'm sure you need something relatively quickly\n",
      "madlocke was working on that, but as you probably know, he's been out of commission (sick) lately\n",
      "a new release (feature wise) is probably about 3 or 4 weeks away since the db upgrade is going to be huge\n",
      "I may make an interim bug fix/small feature release to get some of the email support down\n",
      "but we are seeing a GREAT value in DCL as a communication tool to customers\n",
      "as well as a billing tool\n",
      "and todo tool\n",
      "the todo tool works fine\n",
      "but the communication tool is flawed\n",
      "if the accounts dont have same products\n",
      "it should be fairly easy to implement who can view what\n",
      "by product\n",
      "but then the next step becomes billing (or services)\n",
      "at which point products and services need to be 'separate'\n",
      "but that was because i didnt see dcl as a communications tool\n",
      "your use of services sounds like it could be an action\n",
      "as I said, I am going to be working on this stuff (minus billing) for work, so it will come\n",
      "\n",
      "PREDICTIONS\n",
      "-----------\n",
      " well, I managed to slip past the nice men in the white coats and find a terminal\n",
      "http://www.somethingawful.com/news/10-24-2001/anthrax/index.htm\n",
      "btw, why are ou guys calling this abstraction thingy GComm when Dave has a project called GNU Comm (GComm fr short)?\n",
      "as far as I'm concerned, GComm is our internal package name... to the external world, it's GNUe Common\n",
      "m.digest()\n",
      "or, for simplicity, print md5.new(string).digest()  :)\n",
      "it prints funny stuff though\n",
      "'\\003\\340=W@\\226%\\227\\335\\260c\\026\\254)\\237\\033'\n",
      "try .hexdigest()\n",
      "but .hexdigest() gets errors\n",
      "6e53fcfa862e45bf0167068480d403f6\n",
      "    def _make_passkey(self, user, passwd, random):\n",
      "        m = md5.new(user + '-' + passwd + '-' + random)\n",
      "        return self._hexstr(m.digest())\n",
      "    def _hexstr(self, s):\n",
      "        return r\n",
      "er remove the directory in your local copy\n",
      "it wont 'delete' the stupid dir\n",
      "also we shoudl fix driver code to use hexdigest\n",
      "but when I update or co, it comes back\n",
      "hiya javaguy\n",
      "Hey CW.  Wassup?\n",
      "CW: that sounds vaguely familiar.  Some gnome python app?\n",
      "check out jc's spec\n",
      "for Gcomm\n",
      "either that or I customize interchange\n",
      "anyone done any documentation or work on a php interface?\n",
      "first you'd need php<->corba mappings\n",
      "well at that point, I need to access inventory, etc.. is that possible?\n",
      "I guess anything is possible..\n",
      "our inventory package isn't completed\n",
      "webware.sourceforge.net\n",
      "webware and GNUe might be interesting\n",
      "http://www.cs.virginia.edu/stream/\n",
      "worked straight thru the night till 6 am\n",
      "i dont agree with how they did conversion entirely\n",
      "ah good.  I havn't read it yet, just peeked\n",
      "any postscript experts here?\n",
      "real or imagined\n",
      "but, alas\n",
      "who sings that?\n",
      "hiya can anyone help with a small gcc query?\n",
      "ahh okay r u sure you wanna listen? :)\n",
      "it'smore advice i suppose...\n",
      "okay well RH6.2 compiling openssl with gcc and i get fatal sig 11 errors\n",
      "but i wanted to ask someone if they though reinstalling gcc would solve the problem\n",
      "any suggestions/comments?\n",
      "umm 64mb\n",
      "ah that could be a problem as the machine is at a hosting centre... (damn)\n",
      "looks like I'll have to hassle the tech support for some new ram then !\n",
      "i386\n",
      "Sure\n",
      "I knew compulsively reading freshmeat would payoff someday\n",
      "it's funny i used to do exactly that... when it was on news://\n",
      "doesn't it still have a usenet gateway?\n",
      "hmm memtest is a standalone app... would need a reboot... darn it... (machine is in hosting centre)\n",
      "cool found one -> http://www.qcc.sk.ca/~charlesc/software/memtester/ doesn't need boot binary... perfect\n",
      "havoc pennington\n",
      "well it's running... <gulp>\n",
      "thanks nickr ttfn\n",
      "F U N W I T H C A P S L O C K O H Y E A H.\n",
      "one that deals with free software crack addicts?\n",
      "chillywilly: 'im having crack urges'\n",
      "fishdok: trout slaps chilly\n",
      "sure....\n",
      "ttfn\n",
      "bbiaf\n",
      "sheeet\n",
      "galeon is the cure for the common cold\n",
      "galeon is konquerer's beotch\n",
      "konq's\n",
      "gnome reminds me of arthritis\n",
      "they're gettting close to international selection level for archery :)\n",
      "you cannot for I am the only one who holds the key ti true trout mastery\n",
      "oh boy, gotta love those circular dependencies!\n",
      "sheesh, what's OPN coming to\n",
      "Well, I had to fight off the security guards, but.\n",
      "=) Thanks reinhard\n",
      "no but i check the \"new projects\" list every few days\n",
      "and windows isnt a 'monitorless' operating system\n",
      "it wants to 'convert it' (as access 2000 has entirely new architecture)\n",
      "ko\n",
      "and be much more 'official' than informal help\n",
      "more official?\n",
      "i.e. accountable\n",
      "ra\n",
      "ja rastafari\n",
      "the monopoly lives and breathes baby\n",
      "you can get latest version of internet exploder here <link>\n",
      "a suburb here in memphis is basically declaring war on Apple\n",
      "cygwin MUST be trying to sell CDs... why else would cygwin be so much trouble to download/install\n",
      "versions 4.08-4.82.\n",
      "issues, you cannot currently access .NET Passport using Netscape Navigator\n",
      "6.1. We take security seriously and are working with Netscape to resolve\n",
      "Navigator 6.1. Until that time, please use supported browsing software. We\n",
      "this is really really incredible\n",
      "in midst of all monolopy stuff\n",
      "well, I guess that breaks my msn.com dependency\n",
      "refering to MS...\n",
      "this is like the McCarthy era thing...\n",
      "and visit msn.com\n",
      "the commie committee\n",
      "(or lynx even)\n",
      "ToyMan: ain't it purdy\n",
      "fisher price kinda\n",
      "my record for WinME crashing was this\n",
      "open new computer box\n",
      "a little fat-happy look\n",
      "have it crash while doing initial startup\n",
      "and i notice that after promising not to do the smart tags thing in IE 6...\n",
      "MS: No.  No we're not.  We promise.\n",
      "MS: You misunderstand our intentions.. we are actually pulling something out of your butt.  You should be thanking us.\n",
      "the major hangup is of course.....drumroll....Office\n",
      "or there would be no doubt about going GNU/Linux\n",
      "but, james, now is the ideal time to upgrade to XP\n",
      "what's it running per seat?\n",
      "our MS stations will not go past Win98\n",
      "I'd like to take all my empty space on client system (lots of huge ide drives w/ little on them)\n",
      "i just saw the yahoo article on msn.com\n",
      "they \"suggest\" other browsers\n",
      "because they overcomplicate our lives\n",
      "argh!!!!!\n",
      "I strongly dislike cygwin's setup\n",
      "third time I've had to restart it\n",
      "(after going for 2-3 hrs)\n",
      "dedicated DSL\n",
      "with masta's disease no dount\n",
      "http://www.gnome.org/applist/view.php3?name=Goats&prevpage=listrecent.php3&entrylimit=20\n",
      "about 100 times\n",
      "or get mame working?\n",
      "\"All of the Evolution user-visible strings will be frozen by\n",
      "Monday October 21st 12:00pm UTC\"?\n",
      "I still can see new/modified strings every day...\n",
      "no contacts, appointments, tasks...\n",
      "I finally got cygwin downloaded :)\n",
      "if exists (select 1 from mdean) print 'hello'\n",
      "if (mdeam): raise DerekSurprised\n",
      "BUT we are hitting a wall on something (small wall for now)\n",
      "-wall?\n",
      "we are updating 'accounts' 'contacts' to be much more robust and FLEXIBLE\n",
      "but we are seeing a GREAT value in DCL as a communication tool to customers\n",
      "as well as a billing tool\n",
      "and todo tool\n",
      "but the communication tool is flawed\n",
      "but then the next step becomes billing (or services)\n",
      "foo with products x and y\n",
      "BUT the next item is services\n",
      "network installs\n",
      "programming\n",
      "and bills at different rates for each\n",
      "i.e. the customer would never see dcl\n",
      "or billing type\n",
      "im being smartass\n",
      "drive carefully\n",
      "s/outside of irc//\n",
      "derke sounds like der key or dorky :)\n",
      "\n",
      "ROUGE Scores\n",
      "------------\n",
      "Evaluation with Avg\n",
      "\trouge-1:\tP: 0.380952\tR: 0.470588\tF1: 0.421053\n",
      "\trouge-2:\tP: 0.320000\tR: 0.396040\tF1: 0.353982\n",
      "\trouge-3:\tP: 0.314516\tR: 0.390000\tF1: 0.348214\n",
      "\trouge-4:\tP: 0.308943\tR: 0.383838\tF1: 0.342342\n",
      "\trouge-l:\tP: 0.447430\tR: 0.533582\tF1: 0.486723\n",
      "\trouge-w:\tP: 0.313230\tR: 0.229086\tF1: 0.264630\n"
     ]
    }
   ],
   "source": [
    "from rouge_metrics import *\n",
    "from get_sentences_from_line_numbers import *\n",
    "\n",
    "predicted_line_numbers = [index_for_validation_test_split + index + 1 for index, value in enumerate(test_y_predictions_argmax) if value==1]\n",
    "summaries_line_numbers = [index_for_validation_test_split + index + 1 for index, value in enumerate(test_y_argmax) if value==1]\n",
    "\n",
    "summaries_chat_lines = get_sentences_of_line_numbers(CHAT_LOGS, summaries_line_numbers)\n",
    "predicted_chat_lines = get_sentences_of_line_numbers(CHAT_LOGS, predicted_line_numbers)\n",
    "print(\"SUMMARIES\\n---------\\n\", summaries_chat_lines)\n",
    "print(\"PREDICTIONS\\n-----------\\n\", predicted_chat_lines)\n",
    "\n",
    "hypotheses = [predicted_chat_lines]\n",
    "references = [summaries_chat_lines]\n",
    "print(\"ROUGE Scores\\n------------\")\n",
    "print_rouge_results(get_rouge_results(hypotheses, references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      3872\n",
      "           1       0.05      0.04      0.04       116\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      3988\n",
      "   macro avg       0.51      0.51      0.51      3988\n",
      "weighted avg       0.94      0.95      0.95      3988\n",
      "\n",
      "[[3769  103]\n",
      " [ 111    5]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_y_argmax, validation_y_predictions_argmax))\n",
    "print(confusion_matrix(validation_y_argmax, validation_y_predictions_argmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (0, 1)]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actual_and_prediction = list(zip(test_y_argmax, test_y_predictions_argmax))\n",
    "[y for i, y in enumerate(y_actual_and_prediction)][80:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "25\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[20085, 20105, 20106, 20109, 20136, 20138, 20166, 20202, 20211, 20248]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_ids_for_false_positives = []\n",
    "log_ids_for_false_negatives = []\n",
    "log_ids_for_true_positives = []\n",
    "offset = index_for_validation_test_split\n",
    "for index, value in enumerate(y_actual_and_prediction):\n",
    "    if value == (0, 1):\n",
    "        log_ids_for_false_positives.append(offset + index)\n",
    "    elif value == (1, 0):\n",
    "        log_ids_for_false_negatives.append(offset + index)\n",
    "    elif value == (1,1):\n",
    "        log_ids_for_true_positives.append(offset + index)\n",
    "    \n",
    "log_ids_for_false_positives = [index_for_validation_test_split + index for index, value in enumerate(y_actual_and_prediction) if value==(0, 1)]\n",
    "log_ids_for_false_positives = [index_for_validation_test_split + index for index, value in enumerate(y_actual_and_prediction) if value==(0, 1)]\n",
    "print(len(log_ids_for_false_positives))\n",
    "print(len(log_ids_for_false_negatives))\n",
    "print(len(log_ids_for_true_positives))\n",
    "log_ids_for_false_positives[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or, for simplicity, print md5.new(string).digest() :)\n",
      "BUT from geas/DBdriver.py:\n",
      "def _make_passkey(self, user, passwd, random):\n",
      "def _hexstr(self, s):\n",
      "but when I update or co, it comes back\n",
      "doesn't jamest have access to the cvs server?\n",
      "either that or I customize interchange\n",
      "(basically Python-based web application services\n",
      "webware and GNUe might be interesting\n",
      "real or imagined\n",
      "any suggestions/comments?\n",
      "doesn't it still have a usenet gateway?\n",
      "and windows isnt a 'monitorless' operating system\n",
      "there is an option to open as access as 97\n",
      "i will get 97 up here pretty quick\n",
      "and be much more 'official' than informal help\n",
      "more official?\n",
      "i.e. accountable\n",
      "any non ie 5.0 or greater browser in fact\n",
      "F*** YOU! No MSN for you.\n",
      "this is really really incredible\n",
      "I always wondered if gnuebot and bigbrother secretly fought in the background\n",
      "either with win2k or kde&linux\n",
      "the major hangup is of course.....drumroll....Office\n",
      "what's it running per seat?\n",
      "I strongly dislike cygwin's setup\n",
      "if exists (select 1 from mdean) print 'hello'\n",
      "BUT we are hitting a wall on something (small wall for now)\n",
      "we are updating 'accounts' 'contacts' to be much more robust and FLEXIBLE\n",
      "bar with products a and b\n",
      "i want foo to only see stuff for x and y and bar with a and b\n",
      "i.e. the customer would never see dcl\n"
     ]
    }
   ],
   "source": [
    "for log_id in log_ids_for_false_positives:\n",
    "    print(get_words_at_line_number(word_indexes, log_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyro is an object system like what gcomm will be\n",
      "the python email client\n",
      "you should be able to access it via geas\n",
      "and todo tool\n"
     ]
    }
   ],
   "source": [
    "for log_id in log_ids_for_true_positives:\n",
    "    print(get_words_at_line_number(word_indexes, log_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "btw, why are ou guys calling this abstraction thingy GComm when Dave has a project called GNU Comm (GComm fr short)?\n",
      "as far as I'm concerned, GComm is our internal package name... to the external world, it's GNUe Common\n",
      "but that is a good point\n",
      "by same guys that wrote pygmy\n",
      "I need a production quality GNUe web shopping cart ;-)\n",
      "anything is possible I know, but ideally we need to get ideas on some sort of php and GEAS interface\n",
      "guess I'll just customize interchange.. hopefully actually have it done in a few days\n",
      "our inventory package isn't completed\n",
      "but if you have an inventory package\n",
      "I know a web interface for GNUe Forms is in the works, but I'm sure you need something relatively quickly\n",
      "madlocke was working on that, but as you probably know, he's been out of commission (sick) lately\n",
      "a new release (feature wise) is probably about 3 or 4 weeks away since the db upgrade is going to be huge\n",
      "I may make an interim bug fix/small feature release to get some of the email support down\n",
      "but we are seeing a GREAT value in DCL as a communication tool to customers\n",
      "as well as a billing tool\n",
      "the todo tool works fine\n",
      "but the communication tool is flawed\n",
      "if the accounts dont have same products\n",
      "it should be fairly easy to implement who can view what\n",
      "by product\n",
      "but then the next step becomes billing (or services)\n",
      "at which point products and services need to be 'separate'\n",
      "but that was because i didnt see dcl as a communications tool\n",
      "your use of services sounds like it could be an action\n",
      "as I said, I am going to be working on this stuff (minus billing) for work, so it will come\n"
     ]
    }
   ],
   "source": [
    "for log_id in log_ids_for_false_negatives:\n",
    "    print(get_words_at_line_number(word_indexes, log_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merged Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test_y_predictions = merged_model.predict([test_X, test_features_X])\n",
    "merged_validation_y_predictions = merged_model.predict([validation_X, validation_features_X])\n",
    "\n",
    "merged_test_y_argmax = np.argmax(test_y,axis=1)\n",
    "merged_validation_y_argmax = np.argmax(validation_y,axis=1)\n",
    "\n",
    "merged_test_y_predictions_argmax = np.argmax(merged_test_y_predictions[1], axis=1)\n",
    "merged_validation_y_predictions_argmax = np.argmax(merged_validation_y_predictions[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_test_y_predictions_argmax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5ac4bd18f8f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_test_y_predictions_argmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(merged_test_y_predictions[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_test_y_argmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merged_test_y_predictions_argmax' is not defined"
     ]
    }
   ],
   "source": [
    "print(merged_test_y_predictions_argmax[:100])\n",
    "# print(merged_test_y_predictions[0])\n",
    "print(merged_test_y_argmax[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       619\n",
      "           1       0.12      0.17      0.14        29\n",
      "\n",
      "   micro avg       0.91      0.91      0.91       648\n",
      "   macro avg       0.54      0.56      0.55       648\n",
      "weighted avg       0.92      0.91      0.92       648\n",
      "\n",
      "[[584  35]\n",
      " [ 24   5]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(merged_test_y_argmax, merged_test_y_predictions_argmax))\n",
    "print(confusion_matrix(merged_test_y_argmax, merged_test_y_predictions_argmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.96      3872\n",
      "           1       0.04      0.08      0.05       116\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      3988\n",
      "   macro avg       0.50      0.51      0.50      3988\n",
      "weighted avg       0.94      0.92      0.93      3988\n",
      "\n",
      "[[3642  230]\n",
      " [ 107    9]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(merged_validation_y_argmax, merged_validation_y_predictions_argmax))\n",
    "print(confusion_matrix(merged_validation_y_argmax, merged_validation_y_predictions_argmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
